{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode=pd.read_csv('../data/episode_info.csv')\n",
    "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
    "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
    "monica=pd.read_csv(\"../data/Monica.csv\")\n",
    "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
    "ross=pd.read_csv(\"../data/Ross.csv\")\n",
    "joey=pd.read_csv(\"../data/Joey.csv\")\n",
    "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
    "alls=pd.read_csv(\"../data/All.csv\")\n",
    "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
    "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
    "minors=pd.read_csv(\"../data/minors.csv\")\n",
    "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Love is sweet as summer showers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Love is a wondrous work of art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>But your love, oh your love,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Your love is like a giant pigeon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Crapping on my heart.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>Bum bum bum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>Don't take no for an answer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>Bum bum bum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>Don't let love fly away!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>Bum bum bum bum bum bum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 line\n",
       "0     Love is sweet as summer showers\n",
       "1      Love is a wondrous work of art\n",
       "2        But your love, oh your love,\n",
       "3    Your love is like a giant pigeon\n",
       "4               Crapping on my heart.\n",
       "..                                ...\n",
       "225                       Bum bum bum\n",
       "226      Don't take no for an answer!\n",
       "227                       Bum bum bum\n",
       "228          Don't let love fly away!\n",
       "229        Bum bum bum bum bum bum...\n",
       "\n",
       "[230 rows x 1 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
    "#monica\n",
    "songs_ph\n",
    "# monica[\"line\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aithi/Documents/GitHub/friends2-my/scripts'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()  \n",
    "\n",
    "# songs_ph[\"line\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aithi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt') # one time \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Love is sweet as summer showers', 'Love is a wondrous work of art', 'But your love, oh your love,', 'Your love is like a giant pigeon', 'Crapping on my heart.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = []\n",
    "for s in songs_ph[\"line\"]:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "\n",
    "sentences = [y for x in sentences for y in x] # flatten list\n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (Using Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "#Uncomment if not downloaded already ? I will try to do so in Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aithi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "sweet\n",
      "summer\n",
      "showers\n",
      "love\n",
      "wondrous\n",
      "work\n",
      "art\n",
      "love\n",
      "oh\n",
      "love\n",
      "love\n",
      "like\n",
      "giant\n",
      "pigeon\n",
      "crapping\n",
      "heart\n"
     ]
    }
   ],
   "source": [
    "# let’s create vectors for our sentences. \n",
    "\n",
    "#the constituent words in a sentence and then take mean/average \n",
    "#of those vectors to arrive at a consolidated vector for the sentence\n",
    "for i in clean_sentences[0:5]:\n",
    "    for w in i.split():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        #print('I am here')\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "# print(sentences[0])\n",
    "# print(clean_sentences[0])\n",
    "# print(sentence_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import gensim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/1: import tensorflow\n",
      " 2/2: pip --upgrade tensorflow\n",
      " 2/3: import tensorflow as tf\n",
      " 2/4:\n",
      "import tensorflow as tf\n",
      "mnist= tf.keras.datasets.mnist\n",
      " 2/5:\n",
      "import tensorflow as tf\n",
      "mnist= tf.keras.datasets.mnist \n",
      "(x_train,y_train), (x_test,y_test)= mninst.load_data()\n",
      " 2/6:\n",
      "import tensorflow as tf\n",
      "mnist = tf.keras.datasets.mnist \n",
      "(x_train,y_train), (x_test,y_test)= mninst.load_data()\n",
      " 2/7:\n",
      "import tensorflow as tf\n",
      "mnist = tf.keras.datasets.mnist \n",
      "(x_train,y_train), (x_test,y_test)= mninst.load_data()\n",
      " 2/8: tensorflow -V\n",
      " 2/9: tensorflow --version\n",
      "2/10: tf.version\n",
      "2/11: tf._version\n",
      "2/12: tf._version_\n",
      "2/13: tf._version_\n",
      "2/14: tf._version_\n",
      "2/15: tf.__version__\n",
      "2/16: keras.__version__\n",
      "2/17: import keras\n",
      "2/18: tf.keras\n",
      "2/19: tf.keras.__version__\n",
      "2/20: mnist=tf.keras.datasets.mnist\n",
      "2/21:  (x_train,y_train),(x_test, y_test)=mnist.load_data()\n",
      "2/22: import matplotlib.pyplot as plt\n",
      "2/23:\n",
      "plt.imshow(x_train[0], cmap=plt.c.binary)\n",
      "plt.show()\n",
      "2/24:\n",
      "plt.imshow(x_train[0], cmap=plt.cm.binary)\n",
      "plt.show()\n",
      "2/25:\n",
      "x_train = tf.keras.utils.normalize(X)\n",
      "print(x_train[0])\n",
      "2/26:\n",
      "x_train = tf.keras.utils.normalize(x_train)\n",
      "print(x_train[0])\n",
      "2/27:\n",
      "plt.imshow(x_train[0], cmap=plt.cm.binary)\n",
      "plt.show()\n",
      "2/28:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "print(x_train[0])\n",
      "2/29:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "x_train = tf.keras.utils.normalize(x_test,axis=1)\n",
      "print(x_train[0])\n",
      "2/30:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "x_train = tf.keras.utils.normalize(x_test,axis=1)\n",
      "print(x_train[0]);\n",
      "2/31:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "x_train = tf.keras.utils.normalize(x_test,axis=1)\n",
      "#print(x_train[0]);\n",
      "2/32: model=tf.keras.models.Sequential()\n",
      "2/33: model.add(tf.keras.layers.Flatten())\n",
      "2/34: model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "2/35:\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "2/36: model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
      "2/37:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_catergorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/38: moedel.fit(x_train,y_train,epochs=3)\n",
      "2/39: model.fit(x_train,y_train,epochs=3)\n",
      "2/40:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/41: model.fit(x_train,y_train,epochs=3)\n",
      "2/42: model.fit(x_train,y_train,epochs=3)\n",
      "2/43:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
      "#print(x_train[0]);\n",
      "2/44: model=tf.keras.models.Sequential()\n",
      "2/45: model.add(tf.keras.layers.Flatten())\n",
      "2/46:\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "2/47: model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
      "2/48:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/49: model.fit(x_train,y_train,epochs=3)\n",
      "2/50:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "x_train.shape\n",
      "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
      "#print(x_train[0]);\n",
      "2/51:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "print(x_train.shape)\n",
      "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
      "#print(x_train[0]);\n",
      "2/52:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "print(x_train.shape)\n",
      "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
      "#print(x_train[0]);\n",
      "2/53: model.fit(x_train,y_train,epochs=3)\n",
      "2/54:\n",
      "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
      "print(x_train.shape)\n",
      "\n",
      "#print(x_train[0]);\n",
      "2/55: model=tf.keras.models.Sequential()\n",
      "2/56: model.add(tf.keras.layers.Flatten())\n",
      "2/57:\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "2/58: model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
      "2/59:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/60:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/61:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/62:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      "2/63: model.fit(x_train,y_train,epochs=3)\n",
      "2/64: #model.add(tf.keras.layers.Flatten())\n",
      "2/65:\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
      "2/66: model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
      "2/67:\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy'\n",
      "              ,metrics=['accuracy'])\n",
      " 3/1:\n",
      "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
      "\n",
      "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
      "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
      "\n",
      "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
      "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
      "\n",
      "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
      "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
      "\n",
      "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
      "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
      "              metrics=['accuracy'])  # what to track\n",
      "\n",
      "model.fit(x_train, y_train, epochs=3)  # train the model\n",
      " 3/2:\n",
      "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
      "\n",
      "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
      "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
      "\n",
      "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
      "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
      "\n",
      "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
      "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
      "\n",
      "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
      "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
      "              metrics=['accuracy'])  # what to track\n",
      "\n",
      "model.fit(x_train, y_train, epochs=3)  # train the model\n",
      " 3/3:\n",
      "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
      "\n",
      "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
      "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
      "\n",
      "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
      "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
      "\n",
      "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
      "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
      "\n",
      "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
      "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
      "              metrics=['accuracy'])  # what to track\n",
      "\n",
      "model.fit(x_train, y_train, epochs=3)  # train the model\n",
      " 3/4:\n",
      "import tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\n",
      "\n",
      "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
      "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
      "\n",
      "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
      "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
      "\n",
      "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
      "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
      "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
      "\n",
      "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
      "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
      "              metrics=['accuracy'])  # what to track\n",
      "\n",
      "model.fit(x_train, y_train, epochs=3)  # train the model\n",
      " 3/5:\n",
      "val_loss,val_acc=model.evaluate(x_test,y_test)\n",
      "print(val_loss,val_acc)\n",
      " 3/6: model.save('epic_num_reader.model')\n",
      " 3/7: new_model=tf.keras.models.loadf_model('epic_num_reader.model')\n",
      " 3/8: new_model=tf.keras.models.load_model('epic_num_reader.model')\n",
      " 3/9: predictions=new_model.predict([x_test])\n",
      "3/10: predictions=new_model.predict([x_test])\n",
      "3/11: new_model=tf.keras.models.load_model('epic_num_reader')\n",
      "3/12: new_model=tf.keras.models.load_model('epic_num_reader')\n",
      "3/13: new_model=tf.keras.models.load_model('epic_num_reader.model')\n",
      "3/14: new_model=tf.keras.models.load_model('epic_num_reader')\n",
      "3/15: new_model=tf.keras.models.load_model('epic_num_reader.model')\n",
      "3/16: predictions=new_model.predict(x_test)\n",
      "3/17: print(predictions)\n",
      "3/18:\n",
      "import numpy as np\n",
      "print(np.argmax(predictions[0]))\n",
      "3/19: plt.imshow(x_test[0])\n",
      "3/20:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.imshow(x_test[0])\n",
      "3/21: plt.show\n",
      "3/22: plt.show()\n",
      "3/23: plt.show()\n",
      "3/24:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.imshow(x_test[0])\n",
      " 4/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      " 4/2:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      " 4/3:\n",
      "DATADIR=\"/Datasets/PetImages\"\n",
      "CATEGORIS=[\"Dog\", \"Cat\"]\n",
      " 4/4:\n",
      "DATADIR=\":/Datasets/PetImages\"\n",
      "CATEGORIS=[\"Dog\", \"Cat\"]\n",
      " 4/5:\n",
      "DATADIR=\":/Datasets/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "pwd\n",
      " 4/6:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "pwd()\n",
      " 4/7:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      " 4/8:\n",
      "DATADIR=\":/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      " 4/9:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path\n",
      "4/10:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "4/11:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdr(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/12:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/13:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/14:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/15:\n",
      "DATADIR=\"/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "4/16:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/17: pwd()\n",
      "4/18:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "4/19:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "4/20: plt.imshow(img_array,cmap=\"gray\")\n",
      "4/21:\n",
      "plt.imshow(img_array,cmap=\"gray\");\n",
      "plt.show()\n",
      "break\n",
      "4/22:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.l istdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "4/23:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "4/24:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "4/25: print(img_array)\n",
      "4/26:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      "4/27:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    class_num=CATEGORIES.index(category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "        training_data.append([new_array, class_num])\n",
      "4/28:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "            new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "            training_data.append([new_array, class_num])\n",
      "4/29:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "4/30: print(len(training_data))\n",
      "4/31:\n",
      "import random\n",
      "random.shuffle(training_data/)\n",
      "4/32:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "4/33:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "4/34:\n",
      "X=[]# Feature set\n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "    X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/35:\n",
      "X=[]# Feature set\n",
      "y=[]#labels\n",
      "4/36:\n",
      "\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "    X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/37:\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "    X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/38:\n",
      "X=[]# Feature set\n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "    X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/39:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/40:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X,pickle_out)\n",
      "pickle_out.cose()\n",
      "\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y,pickle_out)\n",
      "pickle_out.cose()\n",
      "4/41:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "4/42:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "4/43:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "4/44:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "4/45: X=X/255.0\n",
      "4/46:\n",
      "model=sequential()\n",
      "model.add(add(Conv2D(64),(3,3)), input_shape=X.shape[1:])\n",
      "4/47:\n",
      "model=Sequential()\n",
      "model.add(add(Conv2D(64),(3,3)), input_shape=X.shape[1:])\n",
      "4/48:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "4/49: print(X.shape[1:])\n",
      "4/50: print(X.shape)\n",
      "4/51: model.add(MaxPoolng2D(pool_size=(2,2)))\n",
      "4/52: model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/53:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/54:\n",
      "model.add(Conv2D(264), (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/55:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(64), (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/56:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/57:\n",
      "model.add(Conv2D(264, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/58:\n",
      "model.add(Flatten())\n",
      "model.add(Dense(256))\n",
      "4/59:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/60:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/61:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/62:\n",
      "model.add(Flatten())\n",
      "model.add(Dense(256))\n",
      "4/63:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "4/64:\n",
      "model.compile(loss=\"binary_crossentopy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/65:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/66: model.fit(X,y,batch_size=32)\n",
      "4/67: model.fit(X,y,batch_size=32,validation_split=0.1,epochs=3)\n",
      "4/68:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "4/69:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "4/70: X=X/255.0\n",
      "4/71:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/72: print(X.shape[1:])\n",
      "4/73: print(X.shape)\n",
      "4/74:\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/75:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "4/76:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "4/77:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/78: model.fit(X,y,batch_size=32,validation_split=0.1,epochs=3)\n",
      "4/79:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/80:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "4/81:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "4/82: print(img_array)\n",
      "4/83:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      "4/84:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "4/85: print(len(training_data))\n",
      "4/86:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "4/87:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "4/88:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/89:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/90:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "4/91:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "4/92:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "4/93:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "4/94: X=X/255.0\n",
      "4/95:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/96:\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/97:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "4/98:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/99: model.fit(X,y,batch_size=32,validation_split=0.1,epochs=3)\n",
      "4/100:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/101: model.fit(X,y,batch_size=32,validation_split=0.1,epochs=3)\n",
      "4/102: model.fit(X,y,batch_size=32,validation_split=0.3,epochs=3)\n",
      "4/103:\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/104:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "4/105:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "4/106:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "4/107:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "4/108:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "4/109:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "4/110:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "4/111:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "4/112:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "4/113:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "4/114: X=X/255.0\n",
      "4/115:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/116: print(X.shape[1:])\n",
      "4/117: print(X.shape)\n",
      "4/118: print(X.shape)\n",
      "4/119:\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/120:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "4/121:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "4/122:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/123:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/124:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "4/125:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "4/126:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "4/127:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "4/128:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.datasets import cifar10\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/129:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.datasets import cifar10\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/130: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/131:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.datasets import cifar10\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/132:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.datasets import cifar10\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/133: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/134:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      " \n",
      "\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.datasets import cifar10\n",
      "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/135: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/136:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      " \n",
      "\n",
      "import tensorflow as tf\n",
      "from keras.datasets import cifar10\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/137: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/138: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/139:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "set “KERAS_BACKEND=plaidml.keras.backend”\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/140:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "KERAS_BACKEND=plaidml.keras.backend\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/141:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "KERAS_BACKEND=plaidml.keras.backend\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/142: plaidml -V\n",
      "4/143: import plaidml\n",
      "4/144:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import plaidml\n",
      "\n",
      "KERAS_BACKEND=plaidml.keras.backend\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/145:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import plaidml.keras\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      "4/146:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      " \n",
      "\n",
      "import tensorflow as tf\n",
      "from keras.datasets import cifar10\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers import Conv2D, MaxPooling2D\n",
      "\n",
      "import pickle\n",
      "\n",
      "pickle_in = open(\"X.pickle\",\"rb\")\n",
      "X = pickle.load(pickle_in)\n",
      "\n",
      "pickle_in = open(\"y.pickle\",\"rb\")\n",
      "y = pickle.load(pickle_in)\n",
      "\n",
      "X = X/255.0\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Conv2D(256, (3, 3)))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
      "\n",
      "model.add(Dense(64))\n",
      "\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy',\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "4/147: model.fit(X,y,batch_size=32,validation_split=0.1)\n",
      "4/148:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import plaidml.keras\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      " 5/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import plaidml.keras\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
      " 5/2:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      " 5/3:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      " 5/4: print(img_array)\n",
      " 5/5:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      " 5/6:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      " 5/7: print(len(training_data))\n",
      " 5/8:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      " 5/9:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "5/10:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "5/11:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "5/12:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "5/13:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "5/14:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "5/15: X=X/255.0\n",
      "5/16:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "5/17:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "5/18: X=X/255.0\n",
      "5/19:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "5/20:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "5/21:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "5/22:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "5/23:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "5/24:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      " 6/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      " 6/2:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      " 6/3:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      " 6/4: print(img_array)\n",
      " 6/5:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      " 6/6:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      " 6/7: print(len(training_data))\n",
      " 6/8:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      " 6/9:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "6/10:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/11:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "6/12:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "6/13:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "6/14:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "6/15: X=X/255.0\n",
      "6/16:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "6/17: print(X.shape[1:])\n",
      "6/18: print(X.shape)\n",
      "6/19:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "6/20:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "6/21:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "6/22:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "6/23: model.fit(X,y,batch_size=32,validation_split=0.3,epochs=3)\n",
      "6/24: model.fit(X,y,batch_size=60,validation_split=0.3,epochs=3)\n",
      "6/25: model.fit(X,y,batch_size=100,validation_split=0.3,epochs=3)\n",
      "6/26:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "6/27:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "6/28:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "6/29:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      "6/30:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "6/31: print(len(training_data))\n",
      "6/32:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "6/33:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(0:500,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/34:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape((0:500),IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/35:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(0:500,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/36:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "for features, label in training_data:\n",
      "    X.append(features)\n",
      "    y.append(label)\n",
      "\n",
      "X=np.array(X).reshape(:500,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/37:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==500:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/38:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==500:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "6/39:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "6/40:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "6/41:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "6/42:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "6/43: X=X/255.0\n",
      "6/44:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "6/45: print(X.shape)\n",
      "6/46:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "6/47:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "6/48:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "6/49:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "6/50: model.fit(X,y,batch_size=100,validation_split=0.3,epochs=3)\n",
      " 7/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      " 7/2:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      " 7/3:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      " 7/4: print(img_array)\n",
      " 7/5:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      " 7/6:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      " 7/7: print(len(training_data))\n",
      " 7/8:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      " 7/9:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "7/10:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==500:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "7/11:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "7/12:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "7/13:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "7/14:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "7/15: X=X/255.0\n",
      "7/16:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/17: print(X.shape[1:])\n",
      "7/18: print(X.shape)\n",
      "7/19:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/20:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "7/21:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/22:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "7/23: model.fit(X,y,batch_size=100,validation_split=0.3,epochs=3)\n",
      "7/24: gpu_options =tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
      "7/25:\n",
      "gpu_options =tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
      "sess=tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
      "7/26: model.fit(X,y,batch_size=100,validation_split=0.3,epochs=3)\n",
      "7/27: from tensorflow.keras.callbacks import TensorBoard\n",
      "7/28:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/29:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/30:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/31:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/32:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/33:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/34:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/35:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/36:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "7/37:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "7/38:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "7/39:\n",
      "pickle_in=open(\"X.pickle\",\"rb\")\n",
      "X=pickle.load(pickle_in)\n",
      "7/40:\n",
      "import tensorflow as tf\n",
      "from keras import Sequential\n",
      "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
      "import pickle\n",
      "7/41:\n",
      "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
      "y=pickle.load(open(\"y.pickle\", \"rb\"))\n",
      "7/42: X=X/255.0\n",
      "7/43:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/44:\n",
      "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/45:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(256))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/46:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/47:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "7/48: model.fit(X,y,batch_size=100,validation_split=0.3,epochs=3)\n",
      "7/49: model.fit(X,y,batch_size=100,validation_split=0.1,epochs=3)\n",
      "7/50: model.fit(X,y,batch_size=35,validation_split=0.1,epochs=3)\n",
      "7/51:\n",
      "model=Sequential()\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/52:\n",
      "model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(pool_size=(2,2)))\n",
      "7/53:\n",
      "model.add(Flatten())\n",
      "#Converts 3d feature to 1d feature\n",
      "model.add(Dense(64))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/54:\n",
      "model.add(Dense(1))\n",
      "model.add(Activation('sigmoid'))\n",
      "7/55:\n",
      "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",\n",
      "              metrics=['accuracy'])\n",
      "7/56: model.fit(X,y,batch_size=35,validation_split=0.1,epochs=3)\n",
      " 9/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow\n",
      "10/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow as tf\n",
      "10/2:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow as tf\n",
      "10/3:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "10/4:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow as tf\n",
      "10/5:  ls\n",
      "11/1:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow as tf\n",
      "11/2:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "11/3:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "11/4:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "11/5:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "11/6:\n",
      "import numpy as np \n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import cv2\n",
      "import tensorflow as tf\n",
      "11/7:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/Deep_learn_learn/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "11/8:\n",
      "DATADIR=\"/Users/aithi/Desktop/Summer2019/ML_Siraj_temp/Deep_learn_learn/PetImages\"\n",
      "CATEGORIES=[\"Dog\", \"Cat\"]\n",
      "11/9:\n",
      "for category in CATEGORIES:\n",
      "    path=os.path.join(DATADIR,category)\n",
      "    for img in os.listdir(path):\n",
      "        img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "        plt.imshow(img_array,cmap=\"gray\");\n",
      "        plt.show()\n",
      "        break\n",
      "    break\n",
      "11/10: print(img_array)\n",
      "11/11:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "11/12:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "11/13:\n",
      "for sample in training_data[:10]:\n",
      "    print(sample[1])\n",
      "11/14:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i=10000:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "11/15:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "11/16:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i=10000:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "11/17:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==10000:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "11/18:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==10000:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "11/19:\n",
      "IMG_SIZE=50\n",
      "new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "plt.imshow(new_array,cmap='gray')\n",
      "plt.show()\n",
      "11/20:\n",
      "training_data=[]\n",
      "def create_training_data():\n",
      "    for category in CATEGORIES:\n",
      "        path=os.path.join(DATADIR,category)\n",
      "        class_num=CATEGORIES.index(category)\n",
      "        for img in os.listdir(path):\n",
      "            try:\n",
      "                img_array= cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
      "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
      "                training_data.append([new_array, class_num])\n",
      "            except Exception as e:\n",
      "                pass\n",
      "create_training_data()\n",
      "11/21:\n",
      "import random\n",
      "random.shuffle(training_data)\n",
      "11/22:\n",
      "X=[]# Feature set \n",
      "y=[]#labels\n",
      "i=0;\n",
      "for features, label in training_data:\n",
      "    if i==10000:\n",
      "        break\n",
      "    else:\n",
      "        X.append(features)\n",
      "        y.append(label)\n",
      "        i+=1;\n",
      "\n",
      "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 becomes 3 for color\n",
      "    #Features\n",
      "11/23:\n",
      "import pickle\n",
      "pickle_out = open(\"X.pickle\",\"wb\")\n",
      "pickle.dump(X, pickle_out)\n",
      "pickle_out.close()\n",
      "pickle_out = open(\"y.pickle\",\"wb\")\n",
      "pickle.dump(y, pickle_out)\n",
      "pickle_out.close()\n",
      "16/1:\n",
      "from __future__ import print_function\n",
      "import torch\n",
      "16/2:\n",
      "x = torch.rand(5, 3)\n",
      "print(x)\n",
      "16/3:\n",
      "x = torch.rand(5, 3)\n",
      "#print(x)\n",
      "16/4:\n",
      "#x = torch.rand(5, 3)\n",
      "#print(x)\n",
      "16/5: X = torch.from_numpy(iris.data)\n",
      "16/6:\n",
      "from __future__ import print_function\n",
      "import torch\n",
      "from sklearn.datasets import load_iris\n",
      "16/7: X = torch.from_numpy(iris.data)\n",
      "16/8:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "16/9:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from torchvision.datasets import iris\n",
      "16/10:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "16/11: X = torch.from_numpy(iris.data)\n",
      "16/12:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "16/13:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "16/14:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/1:\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/3: X=scipy.io.loadmat(frey_rawface.matface.mat)\n",
      "17/4: X=scipy.io.loadmat('frey_rawface.matface.mat')\n",
      "17/5: X=scipy.io.loadmat('frey_rawface.mat')\n",
      "17/6: X.shape()\n",
      "17/7: X=np.array(X)\n",
      "17/8: X.shape()\n",
      "17/9: np.shape(X)\n",
      "17/10: np.shape(X)\n",
      "17/11: print(X)\n",
      "17/12: X.shape()\n",
      "17/13: np.shape(X)\n",
      "17/14: X.shape\n",
      "17/15: X.shape\n",
      "17/16: X=np.array(X)\n",
      "17/17: X.shape\n",
      "17/18: X=np.array(X)\n",
      "17/19: print(X)\n",
      "17/20: X=np.array(X_m)\n",
      "17/21:\n",
      "X_m=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "17/22: X=np.array(X_m)\n",
      "17/23: print(X)\n",
      "17/24: X=np.ndarray(X_m)\n",
      "17/25:\n",
      "X_m=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "17/26: X=np.array(X_m)\n",
      "17/27:\n",
      "X=np.array(X_m)\n",
      "np.save('Input_data',X)\n",
      "17/28:\n",
      "np.load('Input_data.npy')\n",
      "print(X)\n",
      "17/29:\n",
      "np.load('Input_data.npy')\n",
      "print(X)\n",
      "17/30:\n",
      "X_m=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "17/31:\n",
      "X=np.array(X_m)\n",
      "np.save('Input_data',X)\n",
      "17/32:\n",
      "np.load('Input_data.npy')\n",
      "print(X)\n",
      "17/33:\n",
      "X=np.array(X_m)\n",
      "np.delete(X,2,0)\n",
      "np.save('Input_data',X)\n",
      "17/34: print(X)\n",
      "17/35:\n",
      "X=np.array(X_m)\n",
      "np.delete(X,1,0)\n",
      "np.save('Input_data',X)\n",
      "17/36:\n",
      "X=np.array(X_m)\n",
      "np.delete(X,1,0)\n",
      "np.save('Input_data',X)\n",
      "17/37: print(X)\n",
      "17/38:\n",
      "\n",
      "np.delete(X,1,0)\n",
      "17/39:\n",
      "\n",
      "X=np.delete(X,1,0)\n",
      "17/40:\n",
      "\n",
      "X=np.delete(X,1,0)\n",
      "17/41: print(X)\n",
      "17/42:\n",
      "\n",
      "X=np.delete(X,2,0)\n",
      "17/43: print(X)\n",
      "17/44:\n",
      "\n",
      "X=np.delete(X,3,0)\n",
      "17/45: print(X)\n",
      "17/46:\n",
      "\n",
      "X=np.delete(X,1,1)\n",
      "17/47: print(X)\n",
      "17/48:\n",
      "X=np.array(X_m)\n",
      "np.save('Input_data',X)\n",
      "17/49:\n",
      "X=np.array(X_m)\n",
      "csvwrite('Input_data.csv',X_m)\n",
      "np.save('Input_data',X)\n",
      "17/50:\n",
      "import numpy as np\n",
      "import csvwrite\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/51:\n",
      "import numpy as np\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/52:\n",
      "X=np.array(X_m)\n",
      "csvwrite('Input_data.csv',X_m)\n",
      "np.save('Input_data',X)\n",
      "17/53:\n",
      "X=np.array(X_m)\n",
      "csvwrite('Input_data.csv',X_m)\n",
      "np.save('Input_data',X)\n",
      "17/54:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/55:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.iteritems()})\n",
      "data.to_csv(\"Input_data.csv\")\n",
      "17/56:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "17/57:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.iteritems()})\n",
      "data.to_csv(\"Input_data.csv\")\n",
      "17/58:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "data.to_csv(\"Input_data.csv\")\n",
      "17/59: X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "17/60:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/61: X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "17/62:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=data.values\n",
      "17/63: print(X)\n",
      "17/64: print(type(X))\n",
      "17/65:\n",
      "# print(type(X))\n",
      "X.shape()\n",
      "17/66:\n",
      "# print(type(X))\n",
      "X.shape?\n",
      "17/67:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/68:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "17/69:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(type(mat))\n",
      "17/70:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "17/71:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=data.values\n",
      "17/72:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/73:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.items()\n",
      "\n",
      "# X=data.values\n",
      "17/74:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/75:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.items()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/76:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.ff()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/77:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.ff()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/78:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.items()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/79:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.keys()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/80:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.values()\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/81:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=mat.values()\n",
      "print(type(X))\n",
      "# X=data.values\n",
      "17/82:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(mat.values())\n",
      "# print(type(X))\n",
      "# X=data.values\n",
      "17/83:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(mat.values())\n",
      " print(type(X))\n",
      "# X=data.values\n",
      "17/84:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(mat.values())\n",
      "print(type(X))\n",
      "# X=data.values\n",
      "17/85:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/86:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/87:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/88:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/89:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/90:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(mat.values())\n",
      "print(type(X))\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/91:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/92:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "print(type(X))\n",
      "print(X)\n",
      "# X=data.values\n",
      "17/93:\n",
      "# print(type(X))\n",
      "X.shape\n",
      "17/94:\n",
      "# print(type(X))\n",
      "np.reshape(X,1965,560)\n",
      "17/95:\n",
      "# print(type(X))\n",
      "X.reshape(X,1965,560)\n",
      "17/96:\n",
      "# print(type(X))\n",
      "X.reshape(1965,560)\n",
      "17/97:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,560)\n",
      "17/98:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X)\n",
      "# X=data.values\n",
      "17/99: np.image(X[0,:])\n",
      "17/100: gray_img = cv2.imread(X, cv2.IMREAD_GRAYSCALE)\n",
      "17/101:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/102:\n",
      "img = Image.fromarray(data, 'Grey')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/103:\n",
      "img = Image.fromarray(data, 'RGB')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/104:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,20,28)\n",
      "17/105:\n",
      "img = Image.fromarray(data, 'Grey')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/106:\n",
      "img = Image.fromarray(X, 'Grey')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/107:\n",
      "X_1=X[0,:]\n",
      "img = Image.fromarray(X_1, 'Grey')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/108:\n",
      "X_1=X[0,:]\n",
      "img = Image.fromarray(X_1, 'Grey')\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/109:\n",
      "X_1=X[0,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/110:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/111:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,28,20)\n",
      "17/112:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "img.show()\n",
      "17/113:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "img.show(cmap='gray')\n",
      "17/114:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.show(img,cmap='gray')\n",
      "17/115:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/116:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,20,28)\n",
      "17/117:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/118:\n",
      "X_1=X[0,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/119:\n",
      "X_1=X[4,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/120:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/121:\n",
      "X_1=X[4,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255)\n",
      "17/122:\n",
      "X_1=X[4,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/123:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,20,28)\n",
      "17/124:\n",
      "X_1=X[4,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/125:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,28,20)\n",
      "17/126:\n",
      "X_1=X[4,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/127:\n",
      "X_1=X[1,:]\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/128:\n",
      "# print(type(X))\n",
      "X=X.reshape(1965,28,20)\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/129:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/130:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X=X.reshape(1965,20,28)\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/131:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/132:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X=X.reshape(1965,28,20)\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/133:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/134:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X=X.reshape(1965,20,28)\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/135:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/136:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X=X.reshape(1965,20,28)\n",
      "X_1=X[1,:]\n",
      "print(X[1,:].shape)\n",
      "17/137:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/138:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X=X.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/139:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/140:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X_1=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/141:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X_1=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/142:\n",
      "# print(type(X))\n",
      "#X=X.reshape(560,1965)\n",
      "X_1=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/143:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X[1,:].shape)\n",
      "17/144:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X_1[1,:].shape)\n",
      "17/145:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "X_1=X[1,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X_1.shape)\n",
      "17/146:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/147:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[1,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X_1.shape)\n",
      "17/148:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[1,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X_1.shape)\n",
      "17/149:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/150:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(20,28)\n",
      "print(X_1.shape)\n",
      "17/151:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray',vmin=0, vmax=255 )\n",
      "17/152:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/153:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/154:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/155:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img)\n",
      "17/156:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "17/157:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()),dtype=uni8)\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/158:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/159:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()),dtype=unit8)\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/160:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()),dtype=unint8)\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/161:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()),dtype=uint8)\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/162:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()),dtype=uint8)\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/163:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/164:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/165:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/166:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/167:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/168:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/169:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "X=data.values\n",
      "17/170:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/171:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/172:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/173:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/174:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/175:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "# X=data.values\n",
      "17/176:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "# X=data.values\n",
      "17/177:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/178:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/179:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "#print(type(mat))\n",
      "17/180:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "# X=data.values\n",
      "17/181:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/182:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "# print(mat)\n",
      "#print(type(mat))\n",
      "17/183:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(mat.values())\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "# X=data.values\n",
      "17/184:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/185:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(mat.values())\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "X=data.values\n",
      "17/186:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/187:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(mat.values())\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "17/188:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.array(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "17/189:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "17/190:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/191:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "17/192: o_faces=torchvision.datasets.ImageNet('./data')\n",
      "17/193:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "# print(mat)\n",
      "#print(type(mat))\n",
      "17/194:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "17/195:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/196:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "17/197:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "17/198: clearall\n",
      "18/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "18/2: clearall\n",
      "18/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "# print(mat)\n",
      "#print(type(mat))\n",
      "18/4:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "# print(X.shape)\n",
      "#X=data.values\n",
      "18/5:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "18/6:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "# X_1=X[0,:]\n",
      "# X_1=X_1.reshape(28,20)\n",
      "# print(X_1.shape)\n",
      "18/7:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "# X_1=X[0,:]\n",
      "# X_1=X_1.reshape(28,20)\n",
      "# print(X_1.shape)\n",
      "18/8:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "# X_1=X[0,:]\n",
      "# X_1=X_1.reshape(28,20)\n",
      "# print(X_1.shape)\n",
      "18/9:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/10:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "18/11:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/12:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "#print(mat)\n",
      "print(type(mat))\n",
      "18/13:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/14:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "18/15:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/16:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/17:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "# X_1=X[0,:]\n",
      "# X_1=X_1.reshape(28,20)\n",
      "# print(X_1.shape)\n",
      "18/18:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/19:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "# X_1=X[0,:]\n",
      "# X_1=X_1.reshape(28,20)\n",
      "# print(X_1.shape)\n",
      "18/20:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "18/21:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/22:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "18/23:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.ff()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/24:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "18/25:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.ff()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/26:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/27:\n",
      "mat = {k:v for v, k in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "18/28:\n",
      "mat = {k:v for v, k in mat.items() if v[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "18/29:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "18/30:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.v()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/31:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "18/32:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "18/33:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/34:\n",
      "X_2=X[10,:]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/35:\n",
      "X_2=X[10,:]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/36:  u, s, vh = np.linalg.svd(X_1, full_matrices=True)\n",
      "18/37: u.shape, s.shape, vh.shape\n",
      "18/38: print(s)\n",
      "18/39: u_rec=np.shape(28,28);s_rec=np.shape(28,20);vh_rec=np.shape(20,20)\n",
      "18/40: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "18/41: u_rec[0:10]=u[0:10]; s_rec[0:10]=s[0:10]; vh_rec[0:10]=vh[0:10]\n",
      "18/42: u_rec[0:10,0:10]=u[0:10,0:10]; s_rec[0:10,0:10]=s[0:10,0:10]; vh_rec[0:10,0:10]=vh[0:10]\n",
      "18/43: u_rec[0:10,0:10]=u[0:10,0:10]; s_rec[0:10,0:10]=s[0:10,0:10]; vh_rec[0:10,0:10]=vh[0:10]\n",
      "18/44:\n",
      "u_rec[0:10,0:10]=u[0:10,0:10]; s_rec[0:10,0:10]=s[0:10,0:10]; \n",
      "vh_rec[0:10,0:10]=vh[0:10]\n",
      "18/45: u_rec[0:10,0:10]=u[0:10,0:10];\n",
      "18/46:\n",
      "s_rec[0:10,0:10]=s[0:10,0:10]; \n",
      "vh_rec[0:10,0:10]=vh[0:10]\n",
      "18/47: s_rec[0:10,0:10]=s[0:10,0:10];\n",
      "18/48: u_rec[0:10,0:10]=u[0:10,0:10];\n",
      "18/49: u_rec[0:10,0:10]=u[0:10,0:10];vh_rec[0:10,0:10]=vh[0:10]\n",
      "18/50: u_rec[0:10,0:10]=u[0:10,0:10]; vh_rec[0:10,0:10]=vh[0:10,0:10]\n",
      "18/51: s_rec[0:10,0:10]=s[0:10,0:10];\n",
      "18/52: s_rec[0:10]=s[0:10];\n",
      "18/53: s_rec[0:10,]=s[0:10,];\n",
      "18/54: s_rec[0:10,0:10]=s[0:10];\n",
      "18/55: X1_rec=np.linalg.multidot(u_rec,s_rec,vh_rec)\n",
      "18/56: X1_rec=np.linalg.multi_dot(u_rec,s_rec,vh_rec)\n",
      "18/57: X1_rec=np.linalg.multi_dot([u_rec,s_rec,vh_rec])\n",
      "18/58:\n",
      "img = Image.fromarray(X1_rec)\n",
      "img.save('ff_rec1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/59: img = Image.fromarray(X1_rec)\n",
      "18/60:\n",
      "img.save('ff_rec1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/61: img.save('ff_rec1.png')\n",
      "18/62:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/63: X1_rec=np.linalg.multi_dot([u_rec,s_rec,vh_rec.transpose()])\n",
      "18/64:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/65:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/66: img.save('ff_rec1.png')\n",
      "18/67: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/68:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/69: u_rec[0:10,:]=u[0:10,:]; vh_rec[0:10,:]=vh[0:10,:]\n",
      "18/70: s_rec[0:10,:]=s[0:10];\n",
      "18/71:\n",
      "for i in range(10):\n",
      "    u_rec[i,i]=s[i]\n",
      "18/72: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/73:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/74:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/75:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    u_rec[i,i]=s[i]\n",
      "18/76: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/77:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/78: u_rec[0:10,:]=u[0:10,:]; vh_rec[0:10,:]=vh[0:10,:]\n",
      "18/79:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    u_rec[i,i]=s[i]\n",
      "18/80: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/81:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/82: u_rec[0:10,0:10]=u[0:10,0:10]; vh_rec[0:10,0:10]=vh[0:10,0:10]\n",
      "18/83:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    u_rec[i,i]=s[i]\n",
      "18/84: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/85:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/86: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "18/87: u_rec[0:10,0:10]=u[0:10,0:10]; vh_rec[0:10,0:10]=vh[0:10,0:10]\n",
      "18/88:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    u_rec[i,i]=s[i]\n",
      "18/89: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/90:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/91:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    u_rec[i,i]=s[i]\n",
      "print(u_rec)\n",
      "18/92:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "print(u_rec)\n",
      "18/93:\n",
      "for i in range(10):\n",
      "    print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "print(s_rec)\n",
      "18/94: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/95:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/96:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/97: X1_rec=u_rec.dot(s_rec).dot(u_rec.transpose())\n",
      "18/98: X1_rec=u_rec.dot(s_rec).dot(u_rec.transpose())\n",
      "18/99: u_rec[0:10,0:10]=u[0:10,0:10]; vh_rec[0:10,0:10]=uh[0:10,0:10]\n",
      "18/100: u_rec[0:10,0:10]=u[0:10,0:10]; vh_rec[0:10,0:10]=u[0:10,0:10]\n",
      "18/101:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "18/102: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/103:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/104: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=v[:,0:10]\n",
      "18/105: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "18/106:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "18/107: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/108:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/109: img.save('ff_rec1.png')\n",
      "18/110:  u, s, vh = np.linalg.svd(X_2, full_matrices=True)\n",
      "18/111: u.shape, s.shape, vh.shape\n",
      "18/112: print(s);\n",
      "18/113: print(s);\n",
      "18/114: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "18/115: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "18/116: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/117:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/118:  u, s, vh = np.linalg.svd(X_2, full_matrices=True)\n",
      "18/119: u.shape, s.shape, vh.shape\n",
      "18/120: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "18/121: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "18/122:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "18/123: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/124: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "18/125:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "18/126: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "18/127:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/128: img.save('ff_rec1.png')\n",
      "18/129: img.save('ff_rec1.png')\n",
      "18/130:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "18/131:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "18/132:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "19/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "19/2:\n",
      "#x = torch.rand(5, 3)\n",
      "# #print(x)\n",
      "# X = torch.from_numpy(iris.data)\n",
      "19/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "19/4:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "19/5:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "19/6:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "19/7:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/8:\n",
      "X_2=X[10,:]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/9:  u, s, vh = np.linalg.svd(X_2, full_matrices=True)\n",
      "19/10: u.shape, s.shape, vh.shape\n",
      "19/11: print(s);\n",
      "19/12: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "19/13: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "19/14:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/15: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "19/16:\n",
      "img = Image.fromarray(X1_rec)\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/17: img.save('ff_rec1.png')\n",
      "19/18:  u, s, vh = np.linalg.svd(X_1, full_matrices=True)\n",
      "19/19: u.shape, s.shape, vh.shape\n",
      "19/20: u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "19/21: u_rec[:,0:10]=u[:,0:10]; vh_rec[:,0:10]=vh[:,0:10]\n",
      "19/22:\n",
      "for i in range(10):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/23: X1_rec=u_rec.dot(s_rec).dot(vh)\n",
      "19/24:\n",
      "img = Image.fromarray(X1_rec)\n",
      "img.save('ff_rec1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/25:\n",
      "img = Image.fromarray(X1_rec)\n",
      "img.save('ff_rec1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/26:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape())\n",
      "19/27:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape\n",
      "19/28:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "19/29:\n",
      "img = Image.fromarray(X1_rec)\n",
      "img.save('ff_rec1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/30:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "img3.save('ff_2.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/31:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "img3.save('ff_rec1.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/32:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/33: u_rec[:,0:k]=u[:,0:k]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "19/34:\n",
      "u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=4;\n",
      "19/35: u_rec[:,0:k]=u[:,0:k]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "19/36:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/37:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "19/38:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/39:\n",
      "u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=2;\n",
      "19/40: u_rec[:,0:k]=u[:,0:k]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "19/41:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/42:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "19/43:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/44:\n",
      "u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=2;\n",
      "19/45:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/46:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "19/47:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/48:  u, s, vh = np.linalg.svd(X_1, full_matrices=True)\n",
      "19/49: u.shape, s.shape, vh.shape\n",
      "19/50: print(s);\n",
      "19/51: print(s) ;\n",
      "19/52:\n",
      "u_rec=np.zeros((28,28));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=2;\n",
      "19/53: u_rec[:,0:k]=u[:,0:k]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "19/54:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "19/55:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "19/56:\n",
      "img3 = Image.fromarray(X1_rec)\n",
      "plt.imshow(img3,cmap='gray')\n",
      "19/57:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[0,:]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "19/58:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,1]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "19/59:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "19/60:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,1]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "19/61:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "19/62:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "20/1:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "20/2:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "20/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "20/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "20/5:\n",
      "#x = torch.rand(5, 3)\n",
      "# #print(x)\n",
      "# X = torch.from_numpy(iris.data)\n",
      "20/6:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "20/7:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "20/8:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "20/9:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "20/10:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "20/11:\n",
      "X_2=X[:,2]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "20/12:\n",
      "X_2=X[:,2]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "20/13:\n",
      "X_2=X[:,20]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "20/14:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "20/15: u.shape, s.shape, vh.shape\n",
      "20/16: print(s) ;\n",
      "20/17: print(s);\n",
      "20/18: # print(s);\n",
      "20/19:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((28,20));vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=2;\n",
      "20/20:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((20,20))\n",
      "\n",
      "k=2;\n",
      "20/21:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=2;\n",
      "20/22:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=10;\n",
      "20/23: u_rec[:,0:k]=u[:,0:k]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "20/24:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "20/25:\n",
      "img3 = Image.fromarray(X1_rec[:,2])\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/26:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/27: u_rec[0:k,:]=u[0:k,:]; vh_rec[:,0:k]=vh[:,0:k]\n",
      "20/28:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "20/29:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "20/30:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "20/31:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/32:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=5;\n",
      "20/33:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "20/34:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "20/35:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "20/36:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/37:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/38:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "20/39:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "20/40:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "20/41: u.shape, s.shape, vh.shape\n",
      "20/42: # print(s);\n",
      "20/43:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=5;\n",
      "20/44:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "20/45:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "20/46:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "21/2:\n",
      "#x = torch.rand(5, 3)\n",
      "# #print(x)\n",
      "# X = torch.from_numpy(iris.data)\n",
      "21/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "21/4:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "21/5:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "21/6:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "21/7:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "21/8:\n",
      "X_2=X[:,20]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "21/9:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "21/10: u.shape, s.shape, vh.shape\n",
      "21/11: # print(s);\n",
      "21/12:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=5;\n",
      "21/13:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/14:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/15:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/16:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/17:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/18:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape))\n",
      "\n",
      "k=10;\n",
      "21/19:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/20:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/21:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/22:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/23:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "u_rec.shape\n",
      "k=10;\n",
      "21/24:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "u_rec.shape\n",
      "k=10;\n",
      "21/25:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape)\n",
      "k=10;\n",
      "21/26:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u.shape, s.shape, vh.shape, )\n",
      "k=10;\n",
      "21/27:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "21/28:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/29:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/30:\n",
      "img3 = Image.fromarray(X1_rec[:,2])\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/31:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/32:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/33:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "print(u_rec)\n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/34:\n",
      "u_rec[0:k,:]=u[0:k,:]; \n",
      "print(u_rec.shape)\n",
      "vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/35:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/36:\n",
      "u_rec[,0:k]=u[,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/37:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "21/38:\n",
      "u_rec[,0:k]=u[,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/39:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/40:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/41:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/42:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/43:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=5;\n",
      "21/44:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/45:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/46:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/47:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save(pca)\n",
      "21/48:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=40;\n",
      "21/49:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "# vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/50:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/51:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh);\n",
      "print(X1_rec.shape)\n",
      "21/52:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save(pca)\n",
      "21/53:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/54:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/55:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/56:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "21/57:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save('pca_k%d'%d)\n",
      "21/58:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "21/59:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/60:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/61:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "21/62:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save('pca_10.png')\n",
      "21/63:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "img.convert('RGB')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save('pca_10.png')\n",
      "21/64:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "img3.convert('RGB')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save('pca_10.png')\n",
      "21/65:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "img3.convert('RGB')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "img3.save('pca_10.png')\n",
      "21/66:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/67:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/68:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=1;\n",
      "21/69:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/70:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/71:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "21/72:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "21/73:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "21/74: u.shape, s.shape, vh.shape\n",
      "21/75: # print(s);\n",
      "21/76:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=1;\n",
      "21/77:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "21/78:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "21/79:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "21/80:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "22/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "22/2:\n",
      "#x = torch.rand(5, 3)\n",
      "# #print(x)\n",
      "# X = torch.from_numpy(iris.data)\n",
      "22/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "22/4:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "22/5:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "22/6:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "22/7:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "22/8:\n",
      "X_2=X[:,20]\n",
      "X_2=X_2.reshape(28,20)\n",
      "img = Image.fromarray(X_2)\n",
      "img.save('ff_2.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "22/9:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "22/10: u.shape, s.shape, vh.shape\n",
      "22/11: # print(s);\n",
      "22/12:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=1;\n",
      "22/13:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "22/14:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "22/15:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "22/16:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "22/17:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=2;\n",
      "22/18:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "22/19:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "22/20:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "22/21:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "22/22:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "23/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "23/2:\n",
      "#x = torch.rand(5, 3)\n",
      "# #print(x)\n",
      "# X = torch.from_numpy(iris.data)\n",
      "23/3:\n",
      "mat=scipy.io.loadmat('frey_rawface.mat')\n",
      "#Returns a dictionary\n",
      "print(mat)\n",
      "print(type(mat))\n",
      "23/4:\n",
      "mat = {k:v for k, v in mat.items() if k[0] != '_'}\n",
      "\n",
      "## Creating values for the dictionary named as mat\n",
      "\n",
      "# data = pd.DataFrame({k: pd.Series(v[0]) for k, v in mat.items()})\n",
      "# data.to_csv(\"Input_data.csv\")\n",
      "###### What if it's not a 2d array\n",
      "23/5:\n",
      "# X = genfromtxt('Input_data.csv', delimiter=',')\n",
      "# New technique but can use panda dataframe\n",
      "print(list(mat.values()))\n",
      "X=np.asarray(list(mat.values()))\n",
      "# print(type(X))\n",
      "print(X.shape)\n",
      "#X=data.values\n",
      "23/6:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,2]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/7:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/8:\n",
      "# X_2=X[:,20]\n",
      "# X_2=X_2.reshape(28,20)\n",
      "# img = Image.fromarray(X_2)\n",
      "# img.save('ff_2.png')\n",
      "# plt.imshow(img,cmap='gray')\n",
      "23/9:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "23/10: u.shape, s.shape, vh.shape\n",
      "23/11: # print(s);\n",
      "23/12:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=2;\n",
      "23/13:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/14:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "23/15:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "23/16:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "23/17:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "23/18:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/19:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "23/20:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "23/21:  u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "23/22:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "23/23:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/24:\n",
      "for i in range(k):\n",
      "    #print(s[i])\n",
      "    s_rec[i,i]=s[i]\n",
      "#print(s_rec)\n",
      "23/25:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "23/26:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "plt.imshow(img3,cmap='gray')\n",
      "23/27:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/28:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/29:\n",
      "img3 = Image.fromarray(X1_rec[:,3].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/30:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/31:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/32:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/33:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/34:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec.transpose());\n",
      "print(X1_rec.shape)\n",
      "23/35:\n",
      "img3 = Image.fromarray(X1_rec[:,20].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/36:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/37:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/38:\n",
      "img3 = Image.fromarray(X1_rec[:,3].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/39:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/40:\n",
      "X1_rec=u_rec.dot(s_rec).dot(vh_rec);\n",
      "print(X1_rec.shape)\n",
      "23/41:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/42:\n",
      "img3 = Image.fromarray(X1_rec[:,2].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/43:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/44:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "\n",
      "s_rec[0:k,0:k]=s[0:k,0:k]\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/45:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/46:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/47:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/48:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/49:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "s_rec[0:k,0:k]=np.diag(s);\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/50: u_rec[:,0:k]=u[:,0:k];\n",
      "23/51:\n",
      "s_rec[0:k,0:k]=np.diag(s);\n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/52: s_rec[0:k,0:k]=np.diag(s);\n",
      "23/53:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[:,0:k]=vh[:,0:k]\n",
      "23/54: s_rec[0:k,0:k]=np.diag(s);\n",
      "23/55:\n",
      " u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "s=np.diag(s)\n",
      "23/56: u.shape, s.shape, vh.shape\n",
      "23/57: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/58:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/59:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/60:\n",
      "img3 = Image.fromarray(X1_rec[:,18].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/61:\n",
      "img3 = Image.fromarray(X1_rec[:,17].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/62:\n",
      "img3 = Image.fromarray(X1_rec[:,9].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/63:\n",
      "img3 = Image.fromarray(X1_rec[:,10].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/64:\n",
      "# print(s);\n",
      "X1_tr=u.dot((s).dot(vh) );\n",
      "23/65:\n",
      " u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "s=np.diag(s).reshape(u.shape[0],v.shape[0])\n",
      "23/66:\n",
      " u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "s=np.diag(s).reshape(u.shape[0],vh.shape[0])\n",
      "23/67:\n",
      " u, s, vh = np.linalg.svd(X, full_matrices=True)\n",
      "s=np.diag(s)\n",
      "23/68:\n",
      "# print(s);\n",
      "s_r=np.zeros((u.shape[0],vh.shape[0]));\n",
      "s_r[0:u.shape[0],0:u.shape[0]]=s[0:u.shape[0],0:u.shape[0]]\n",
      "X_tr=u.dot((s_r).dot(vh) );\n",
      "23/69:\n",
      "# print(s);\n",
      "s_r=np.zeros((u.shape[0],vh.shape[0]));\n",
      "s_r[0:u.shape[0],0:u.shape[0]]=s[0:u.shape[0],0:u.shape[0]]\n",
      "X_tr=u.dot((s_r).dot(vh) );\n",
      "np.allclose(X,X_tr)\n",
      "23/70:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/71: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/72:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/73:\n",
      "img3 = Image.fromarray(X1_rec[:,10].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/74:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/75:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/76:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=2;\n",
      "23/77:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/78:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/79: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/80:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/81:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/82:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=200;\n",
      "23/83:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/84: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/85:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/86:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/87:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=500;\n",
      "23/88:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/89: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/90:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/91:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/92:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=560;\n",
      "23/93:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/94: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/95:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/96:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/97:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/98:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,100]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/99:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/100:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=5;\n",
      "23/101:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/102: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/103:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/104:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/105:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/106:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=560;\n",
      "23/107:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/108: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/109:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/110:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/111:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/112:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/113:\n",
      "img3 = Image.fromarray(X1_rec[:,100].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/114:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,100]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/115:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/116:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,1]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/117:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/118:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/119:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=561;\n",
      "23/120:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/121: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/122:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/123:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/124: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/125:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/126: s_rec[0:k,0:k]=s[(0:k),(0:k)];\n",
      "23/127: s_rec[0:k,0:k]=s[[0:k],[0:k]];\n",
      "23/128: s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/129:\n",
      "print(s[0:k,0:k].shape)\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/130:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k].shape)\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/131:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k-1].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/132:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k-1].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/133:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k-1].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k-1]=s[0:k,0:k];\n",
      "23/134:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/135:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/136:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=560;\n",
      "23/137:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/138:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k-1].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k-1]=s[0:k,0:k];\n",
      "23/139:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/140:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/141:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/142:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k-1]=s[0:k,0:k];\n",
      "23/143:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/144:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/145:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/146:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "23/147:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=1;\n",
      "23/148:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/149:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/150:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/151:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/152:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/153:\n",
      "u_rec=np.zeros((u.shape));s_rec=np.zeros((u.shape[0],vh.shape[0]));\n",
      "vh_rec=np.zeros((vh.shape)) ;\n",
      "print(u_rec.shape, s_rec.shape, vh_rec.shape, )\n",
      "k=10;\n",
      "23/154:\n",
      "u_rec[:,0:k]=u[:,0:k]; \n",
      "# print(u_rec.shape)\n",
      "# vh_rec=vh\n",
      "vh_rec[0:k,:]=vh[0:k,:]\n",
      "## Notice the flip because we are already talking about the transpose\n",
      "## The SVD might return the V instead of the transpose but this is \n",
      "## right here is the transpose or else we need to take the transpose\n",
      "23/155:\n",
      "print(s[0:k,0:k].shape)\n",
      "print(s_rec[0:k,0:k].shape)\n",
      "## Why the heck i need this -1, no idea\n",
      "s_rec[0:k,0:k]=s[0:k,0:k];\n",
      "23/156:\n",
      "# for i in range(k):\n",
      "#     #print(s[i])\n",
      "#     s_rec[i,i]=s[i]\n",
      "# #print(s_rec)\n",
      "23/157:\n",
      "X1_rec=u_rec.dot((s_rec).dot(vh_rec) );\n",
      "print(X1_rec.shape)\n",
      "23/158:\n",
      "img3 = Image.fromarray(X1_rec[:,1].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "23/159:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,100]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/160:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/161:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,50]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/162:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/163:\n",
      "# print(type(X))\n",
      "X=X.reshape(560,1965)\n",
      "#X=X.transpose()\n",
      "print(X.shape)\n",
      "X_1=X[:,500]\n",
      "X_1=X_1.reshape(28,20)\n",
      "print(X_1.shape)\n",
      "23/164:\n",
      "\n",
      "img = Image.fromarray(X_1)\n",
      "img.save('ff_1.png')\n",
      "plt.imshow(img,cmap='gray')\n",
      "23/165:\n",
      "img3 = Image.fromarray(X1_rec[:,500].reshape(28,20))\n",
      "plt.imshow(img3,cmap='gray')\n",
      "if img3.mode != 'RGB':\n",
      "    img3 = img3.convert('RGB')\n",
      "img3.save('pca_10.png')\n",
      "24/1: mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
      "24/2: mnist_trainset = MNIST(root='./data', train=True, download=True, transform=None)\n",
      "24/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "24/4: mnist_trainset = MNIST(root='./data', train=True, download=True, transform=None)\n",
      "24/5:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = datasets.MNIST('./MNIST/', train=True, transform=transform, download=True)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/6:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = MNIST('./data/MNIST/', train=True, transform=transform, download=True)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/7:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = MNIST('./data/MNIST/', train=True, transform=transform, download=True)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/8:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = MNIST('./data', train=True, transform=transform)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/9:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = MNIST('./data', train=True, transform=transform)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/10:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from itertools import iter\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "24/11:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "24/12:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = MNIST('./data', train=True, transform=transform)\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/13:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/14:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/15:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "print(len(train_dataset))\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/16:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/17:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "\n",
      "train_dataset_array = next(iter(train_loader))[0].np()\n",
      "24/18:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = dataloader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/19:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "24/20:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/21:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "24/22:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "\n",
      "train_dataset = mnist_trainset\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/23: train_dataset_array = next(iter(train_loader))[0].np()\n",
      "24/24: train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "24/25:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/26:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/27:\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "24/28:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/29:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/30:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "24/31: print(train_dataset_array.shape)\n",
      "24/32: train_filter = np.where((Y_train == 0 ) | (Y_train == 4))\n",
      "24/33: # train_filter = np.where((Y_train == 0 ) | (Y_train == 4))\n",
      "24/34: print(train_dataset_array)\n",
      "24/35: print(train_dataset_array[1])\n",
      "24/36: print(train_dataset_array[0])\n",
      "24/37: print(train_dataset_array[0,0,0,0])\n",
      "24/38: print(train_dataset_array[0,2,0,0])\n",
      "24/39: print(train_dataset_array[0,0,0,0])\n",
      "24/40: print(train_dataset_array[1,0,0,0])\n",
      "24/41: print(train_dataset_array[4,0,0,0])\n",
      "24/42: print(train_dataset_array[5,0,0,0])\n",
      "24/43: print(train_dataset_array[6,0,0,0])\n",
      "24/44: print(train_dataset_array[200,0,0,0])\n",
      "24/45: print(train_dataset_array[2000,0,0,0])\n",
      "24/46:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "print(train_datalabel_array.shape)\n",
      "24/47:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "24/48:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "24/49: print(Y_train_f)\n",
      "24/50:  train_filter = np.where((Y_train == 0 ) | (Y_train == 4))\n",
      "24/51: train_filter = np.where((Y_train_f == 0 ) | (Y_train_f == 4))\n",
      "24/52: print(train_filter)\n",
      "24/53: print(X_train_f[train_filter].shape)\n",
      "24/54:\n",
      "print(X_train_f[train_filter].shape);\n",
      "### Absolutely cool\n",
      "24/55:\n",
      "print(X_train_f[train_filter].shape);\n",
      "### Absolute fab\n",
      "24/56:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "24/57:\n",
      "# print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "24/58:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "24/59: train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "24/60:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "24/61:\n",
      "#One image\n",
      "X_1=X_train.transpose()\n",
      "24/62:\n",
      "#One image\n",
      "X_1=X_train[3,:].reshape(28,28)\n",
      "24/63: img = Image.fromarray(X_1)\n",
      "24/64: plt.imshow(img,cmap='gray')\n",
      "24/65:\n",
      "#One image\n",
      "print(X_train[3,:].shape)\n",
      "X_1=X_train[3,:].reshape(28,28)\n",
      "24/66: img = Image.fromarray(X_1)\n",
      "24/67: plt.imshow(img,cmap='gray')\n",
      "24/68: plt.imshow(img4,cmap='gray')\n",
      "24/69: plt.imshow(img4,cmap='gray')\n",
      "24/70: img4 = Image.fromarray(X_1)\n",
      "24/71: plt.imshow(img4,cmap='gray')\n",
      "24/72: plt.imshow(img4)\n",
      "24/73: img4 = Image.fromarray(X_1)\n",
      "24/74:\n",
      "#One image\n",
      "print(X_train[3,:].shape)\n",
      "X_1=X_train[3,:].reshape(28,28)\n",
      "print(X_1)\n",
      "24/75:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor()])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "24/76:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/77:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "24/78:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "24/79: train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "24/80: # print(train_filter)\n",
      "24/81:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "24/82: img4 = Image.fromarray(X_1)\n",
      "24/83: plt.imshow(img4,cmap='gray')\n",
      "24/84:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "24/85:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "24/86:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "24/87:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "24/88:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[3,:].reshape(28,28)\n",
      "print(X_1)\n",
      "24/89: img4 = Image.fromarray(X_1)\n",
      "24/90:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "print(X_1)\n",
      "24/91: img4 = Image.fromarray(X_1)\n",
      "24/92: plt.imshow(img4,cmap='gray')\n",
      "26/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "26/2:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "26/3:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "26/4:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/5:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "26/6: train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "26/7: # print(train_filter)\n",
      "26/8:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "### Absolute fab\n",
      "26/9:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "print(X_1)\n",
      "26/10: img4 = Image.fromarray(X_1)\n",
      "26/11: plt.imshow(img4,cmap='gray')\n",
      "26/12: plt.imshow(img4,cmap='gray')\n",
      "26/13:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "X_1.transform()\n",
      "#print(X_1)\n",
      "26/14:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "np.transform(X_!)\n",
      "#print(X_1)\n",
      "26/15:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "np.transform(X_1)\n",
      "#print(X_1)\n",
      "26/16:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "X_1.transpose()\n",
      "#print(X_1)\n",
      "26/17: ?img4 = Image.fromarray(X_1)\n",
      "26/18: img4 = Image.fromarray(X_1)\n",
      "26/19: plt.imshow(img4,cmap='gray')\n",
      "26/20:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "26/21: img4 = Image.fromarray(X_1)\n",
      "26/22: plt.imshow(img4,cmap='gray')\n",
      "26/23:\n",
      "#One image\n",
      "print(X_train[:,1].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "26/24:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "26/25:\n",
      "#One image\n",
      "print(X_train[1,:].shape)\n",
      "X_1=X_train[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray')\n",
      "26/26:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/27:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='RGB')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/28:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='RBG')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/29:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:].reshape(28,28)\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/30:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:][0]\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/31:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=False, transform=transform)\n",
      "26/32:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "26/33:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:][0]\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/34:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "26/35:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "26/36:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1,:][0]\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/37:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1][0]\n",
      "#print(X_1)\n",
      "img4 = Image.fromarray(X_1)\n",
      "plt.imshow(img4,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/38:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "X_1=X_train_f[1][0]\n",
      "#print(X_1)\n",
      "# img4 = Image.fromarray(X_1)\n",
      "plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/39:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[1][0]\n",
      "#print(X_1)\n",
      "# img4 = Image.fromarray(X_1)\n",
      "plt.imshow(X_1,cmap='gray',interpolation='none')plt.imshow(img4,cmap='gray')\n",
      "26/40:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[1][0]\n",
      "#print(X_1)\n",
      "# img4 = Image.fromarray(X_1)\n",
      "plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "26/41:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "26/42:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "#print(X_1)\n",
      "# img4 = Image.fromarray(X_1)\n",
      "plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "26/43: X=X_train.transpose()\n",
      "26/44:\n",
      "X=X_train.transpose();\n",
      "X=X.reshape(784,X.shape(-1))\n",
      "26/45:\n",
      "X=X_train.transpose();\n",
      "X=X.reshape(784,X.shape[-1])\n",
      "26/46:\n",
      "X=X_train.transpose();\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X_shape)\n",
      "26/47:\n",
      "X=X_train.transpose();\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/48: mean_x=np.mean(X)\n",
      "26/49:\n",
      "mean_x=np.mean(X); \n",
      "print(mean_x.shape)\n",
      "26/50:\n",
      "mean_x=np.mean(X); \n",
      "print(mean_x.shape)\n",
      "26/51:\n",
      "mean_x=np.mean(X); \n",
      "print(mean_x.shape)\n",
      "26/52: print(mean_x.shape)\n",
      "26/53:\n",
      "mean_x=np.mean(X); \n",
      "print(mean_x.shape)\n",
      "26/54:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "print(mean_x.shape)\n",
      "26/55: X-mean_x\n",
      "26/56: X-mean_x.dot(np.ones(1,X.shape[-1]))\n",
      "26/57: mean_x.dot(np.ones(1,X.shape[-1]))\n",
      "26/58: X-mean_x.dot(np.ones((1,X.shape[-1])) )\n",
      "26/59: X-mean_x*(np.ones((1,X.shape[-1])) )\n",
      "26/60: X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "26/61:\n",
      "mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "# X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "26/62:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "# print(mean_x.shape)\n",
      "mean_x\n",
      "26/63:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "26/64:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "26/65: u, s, vh = np.linalg.svd(Xb, full_matrices=True)\n",
      "26/66: y1=u[:,1].transpose().dot(X)\n",
      "26/67: y1.shape\n",
      "26/68: y1=u[:,1].transpose().dot(X); y1.shape\n",
      "26/69: plt.imshow(y1)\n",
      "26/70: plt1=plt(y1)\n",
      "26/71: plt1=plt.figure(y1)\n",
      "26/72: plt1=plt.plot(y1)\n",
      "26/73: plt1=plt.scatterplot(y1)\n",
      "26/74: plt1=plt.scatter(y1)\n",
      "26/75: plt1=plt.scatter(range(len(y1)),y1)\n",
      "26/76:\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "np.savefigure(plt1)\n",
      "26/77:\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "np.save(plt1)\n",
      "26/78:\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "np.save(plt1,'fig23.png')\n",
      "26/79:\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "np.save('fig23.png',plt1)\n",
      "26/80:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/81:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "26/82:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/83:\n",
      "X=X_train.transpose();\n",
      "X=X[0:400]\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/84:\n",
      "X=X_train.transpose();\n",
      "X=X[0:400]\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/85:\n",
      "X=X_train.transpose();\n",
      "X=X[0:400]\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/86:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X[0:400]\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/87:\n",
      "X=X[0:400]\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/88:\n",
      "X=X[0:400]\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/89:\n",
      "X=X[0:400]\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/90:\n",
      "\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/91:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "26/92: print(type(train_filter))\n",
      "26/93:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train[train_filter_o2]\n",
      "X_o3=X_train[train_filter_o3]\n",
      "### Absolute fab\n",
      "26/94:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "26/95:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train[train_filter_o2]\n",
      "X_o3=X_train[train_filter_o3]\n",
      "### Absolute fab\n",
      "26/96:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "26/97:\n",
      "X_o2=X_o2.transpose();\n",
      "print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X.shape[-1]);\n",
      "print(X_o2.shape)\n",
      "26/98:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "26/99:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "26/100:\n",
      "X_o2=X_o2.transpose();\n",
      "print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "print(X_o2.shape)\n",
      "26/101:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "26/102:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "26/103:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "\n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "26/104:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "26/105:\n",
      "u, s, vh = np.linalg.svd(Xb, full_matrices=True)\n",
      "u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "26/106:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "\n",
      "y_o2=u_o2[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "y_o3=u_o3[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "26/107:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/108:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/109:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "\n",
      "y_o2=u_o2[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "y_o3=u_o3[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "26/110:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/111:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/112:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "y_o2=u_o2[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "y_o3=u_o3[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "26/113:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/114:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "#plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_03)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "26/115:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "#plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "27/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "27/2:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor()])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "27/3:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "27/4:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "27/5:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "27/6:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "27/7: print(type(train_filter))\n",
      "27/8:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "27/9:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "27/10:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "27/11:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "27/12:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "\n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "27/13:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "27/14:\n",
      "u, s, vh = np.linalg.svd(Xb, full_matrices=True)\n",
      "u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "27/15:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "y_o2=u_o2[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "y_o3=u_o3[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "27/16:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "#plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "27/17:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[40][0]\n",
      "X_3=X_o3[40][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_2,cmap='gray',interpolation='none')\n",
      "27/18:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "print(X_o2.shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "27/19:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "27/20:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "print(X_1=X_train[4][0])\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "27/21:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "print(X_train[4][0].shape)\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "27/22:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[40][0]\n",
      "X_3=X_o3[40][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "27/23:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[40][0]\n",
      "X_3=X_o3[40][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_o3,cmap='gray',interpolation='none')\n",
      "27/24:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[40][0]\n",
      "X_3=X_o3[4][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_o3,cmap='gray',interpolation='none')\n",
      "27/25:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[4][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_o3,cmap='gray',interpolation='none')\n",
      "27/26:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[4][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "27/27:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[4][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_2,cmap='gray',interpolation='none')\n",
      "27/28:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[400][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "27/29:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "27/30:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "27/31:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "27/32:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "Xb_o23=X_o2.append(X_o3)\n",
      "27/33:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "Xb_o23=X_o2+X_o3\n",
      "27/34:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "Xb_o23=Xb_o2+Xb_o3\n",
      "27/35:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "Xb_o23=Xb_o2+Xb_o3\n",
      "27/36:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# Xb_o23=Xb_o2+Xb_o3\n",
      "27/37:\n",
      "u, s, vh = np.linalg.svd(Xb, full_matrices=True)\n",
      "# u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "# u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "27/38:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "27/39:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# Xb_o23=Xb_o2+Xb_o3\n",
      "27/40:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "\n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "27/41:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "# Xb_o23=Xb_o2+Xb_o3\n",
      "27/42:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# Xb_o23=Xb_o2+Xb_o3\n",
      "29/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "29/2:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor()])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "29/3:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "29/4:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "29/5:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "29/6:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "29/7: print(type(train_filter))\n",
      "29/8:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "# print(X_train[4][0].shape)\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "29/9:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[400][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "29/10:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "29/11:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "29/12:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "29/13:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "Xb_o23=Xb_o2+Xb_o3\n",
      "30/1:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "30/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "30/3:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor()])\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "30/4:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "30/5:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "30/6:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "30/7:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "30/8: print(type(train_filter))\n",
      "30/9:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "# print(X_train[4][0].shape)\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "30/10:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[400][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "30/11:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "30/12:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "30/13:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "30/14:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "Xb_o23=Xb_o2+Xb_o3\n",
      "30/15:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# Xb_o23=Xb_o2+Xb_o3\n",
      "30/16:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# Xb_o23 = Xb_o2 + Xb_o3\n",
      "30/17:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "# Xb_o23 = Xb_o2 + Xb_o3\n",
      "30/18:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "print(Xb_o2.shape)\n",
      "print(Xb_o3.shape)\n",
      "\n",
      "# Xb_o23 = Xb_o2 + Xb_o3\n",
      "30/19:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "\n",
      "Xb_o23 = np.array(Xb_o2,Xb_o3)\n",
      "print(Xb_o23.shape)\n",
      "30/20:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "\n",
      "Xb_c = np.array(Xb_o2,Xb_o3)\n",
      "print(Xb_c.shape)\n",
      "30/21:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "# Xb_c=np.zeros()\n",
      "Xb_c = np.array(Xb_o2,Xb_o3)\n",
      "# print(Xb_c.shape)\n",
      "30/22:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array(np.array(Xb_o2),np.array(Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/23:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array(np.array(Xb_o2),np.array(Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/24:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array((Xb_o2,Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/25:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array((Xb_o2,Xb_o3),axis=1)\n",
      "print(Xb_c.shape)\n",
      "30/26:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array((Xb_o2,Xb_o3),axis=0)\n",
      "print(Xb_c.shape)\n",
      "30/27:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array((Xb_o2,Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/28:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.zip((Xb_o2,Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/29:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = zip((Xb_o2,Xb_o3))\n",
      "print(Xb_c.shape)\n",
      "30/30:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = np.array(zip((Xb_o2,Xb_o3)))\n",
      "print(Xb_c.shape)\n",
      "30/31:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "Xb_c = zip((Xb_o2,Xb_o3))\n",
      "print(len(Xb_c) )\n",
      "30/32:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack: stack arrays in sequence vertically (row wise).Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack: Stack arrays in sequence horizontally (column wise).Equivalent to np.concatenate(tup, axis=1), \n",
      "'''\n",
      "np.concatenate(Xb_o2,Xb_o3,axis=1) \n",
      "print(len(Xb_c) )\n",
      "30/33:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack: stack arrays in sequence vertically (row wise).Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack: Stack arrays in sequence horizontally (column wise).Equivalent to np.concatenate(tup, axis=1), \n",
      "'''\n",
      "np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(len(Xb_c) )\n",
      "30/34:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(len(Xb_c) )\n",
      "30/35:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(len(Xb_c) )\n",
      "30/36:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "\n",
      "Xb_c=np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(Xb_c.shape)\n",
      "30/37:\n",
      "u, s, vh = np.linalg.svd(Xb_c, full_matrices=True)\n",
      "# u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "# u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "30/38:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "30/39:\n",
      "fig=plt.figure()\n",
      "# plt1=plt.scatter(range(len(y1)),y1);\n",
      "plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/40:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/41:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/42:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/43:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/44:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/45:\n",
      "fig=plt.figure\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/46:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000])),y1[0:5000]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/47:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1[0:5000,])),y1[0:5000,]);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/48: y1_o2=y1[0:5000]\n",
      "30/49:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/50:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/51:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/52:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/53:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "30/54:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/55:\n",
      "fig=plt.figure\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/56:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/57:\n",
      "fig=plt.figure.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/58:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/59:\n",
      "fig=plt.Figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/60:\n",
      "fig=plt.figure()\n",
      "plt1=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/61:\n",
      "fig=plt.figure()\n",
      "plt1=plt.plot(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/62:\n",
      "fig=plt.figure()\n",
      "plt=plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/63:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/64:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/65:\n",
      "plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/66:\n",
      "plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/67:\n",
      "# plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/68:\n",
      "fig=plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/69:\n",
      "fig=plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/70:\n",
      "fig=plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/71:\n",
      "fig=plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/72:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "30/73:\n",
      "fig=plt.figure()\n",
      "# plt= plt.scatter(range(len(y1)),y1);\n",
      "# # plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# # plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "# fig.savefig('2_3_img.png')\n",
      "# #np.save('fig23.png',plt1)\n",
      "30/74:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/75:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/76:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "30/77:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1)),y1);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/78: y1_o2=y1[0:5000]\n",
      "30/79:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/80:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "30/81:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/82: y1_o2=y1[0:5000]\n",
      "30/83:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "30/84:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor()],transforms.Normalize((0.1307,), (0.3081,)) )\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "30/85:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] )\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "31/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/2:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] )\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "31/3:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "31/4:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "31/5:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "31/6:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "31/7: print(type(train_filter))\n",
      "31/8:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "# print(X_train[4][0].shape)\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "31/9:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[400][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "31/10:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "31/11:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "31/12:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "31/13:\n",
      "# mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "\n",
      "Xb_c=np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(Xb_c.shape)\n",
      "31/14:\n",
      "u, s, vh = np.linalg.svd(Xb_c, full_matrices=True)\n",
      "# u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "# u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "31/15:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "31/16: y1_o2=y1[0:5000]\n",
      "31/17:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/18: y1_o2=y1[6000:]\n",
      "31/19:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/20:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/21:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/22: y1_o2=y1[10000:]\n",
      "31/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/24:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/25: y1_o2=y1[:1000]\n",
      "31/26:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/27:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/28:\n",
      "# print(mean_x.dot(np.ones((X.shape[0],X.shape[-1])) ) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "\n",
      "Xb_o2=X_o2-mean_x.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "# Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "\n",
      "Xb_c=np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(Xb_c.shape)\n",
      "31/29:\n",
      "u, s, vh = np.linalg.svd(Xb_c, full_matrices=True)\n",
      "# u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "# u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "31/30:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "31/31: y1_o2=y1[:1000]\n",
      "31/32:\n",
      "fig=plt.figure()\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/33:\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig=plt.figure()\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/34:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/35:\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "fig=plt.figure()\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/36:\n",
      "fig=plt.figure()\n",
      "\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/37:\n",
      "fig=plt.figure()\n",
      "\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/38:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/39:\n",
      "fig=plt.figure()\n",
      "\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/40:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "31/41: y1_o2=y1[:1000]\n",
      "31/42:\n",
      "fig=plt.figure()\n",
      "\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/43:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt= plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/44:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/45:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/46:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/47:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "31/48:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "31/49:\n",
      "y1_o2=y1[10000:]\n",
      "print(y1_o2.shape)\n",
      "31/50:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "\n",
      "X_c=np.concatenate((X_o2,X_o3),axis=1)\n",
      "31/51:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "\n",
      "X_c=np.concatenate((X_o2,X_o3),axis=1)\n",
      "32/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import csv\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader\n",
      "import torch.utils.data.dataloader as dataloader\n",
      "####### Apparently the above two have a difference\n",
      "####### To figure out later\n",
      "import torch.optim as optim\n",
      "from numpy import genfromtxt\n",
      "from torch.utils.data import TensorDataset\n",
      "from torch.autograd import Variable\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import MNIST\n",
      "from sklearn.datasets import fetch_olivetti_faces\n",
      "32/2:\n",
      "##transforms.Normalize((0.1307,), (0.3081,))\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] )\n",
      "## The transform actually builds tensors out of the PIL imnages\n",
      "# train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
      "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
      "32/3:\n",
      "\n",
      "# test_dataset = datasets.MNIST('./MNIST/', train=False, transform=transform, download=True)\n",
      "\n",
      "# print(len(train_dataset))\n",
      "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
      "# test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
      "32/4:\n",
      "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
      "X_train_f=train_dataset_array\n",
      "## next(iter(train_loader))[1].numpy() to get the labels\n",
      "## Note the change from [0] to [1]\n",
      "32/5:\n",
      "#print(train_dataset_array.shape)\n",
      "train_datalabel_array = next(iter(train_loader))[1].numpy()\n",
      "Y_train_f=train_datalabel_array\n",
      "print(train_datalabel_array.shape)\n",
      "32/6:\n",
      "train_filter = np.where((Y_train_f == 2 ) | (Y_train_f == 3))\n",
      "train_filter_o2 = np.where((Y_train_f == 2 ) )\n",
      "train_filter_o3 = np.where((Y_train_f == 3 ) )\n",
      "# print(train_filter)\n",
      "32/7: print(type(train_filter))\n",
      "32/8:\n",
      "print(X_train_f[train_filter].shape);\n",
      "X_train=X_train_f[train_filter]\n",
      "Y_train=Y_train_f[train_filter]\n",
      "\n",
      "X_o2=X_train_f[train_filter_o2]\n",
      "# print(X_train[4][0].shape)\n",
      "print(X_o2[40][0].shape)\n",
      "X_o3=X_train_f[train_filter_o3]\n",
      "### Absolute fab\n",
      "32/9:\n",
      "#One image\n",
      "# print(X_train[1,:].shape)\n",
      "X_1=X_train[4][0]\n",
      "X_2=X_o2[4][0]\n",
      "X_3=X_o3[400][0]\n",
      "\n",
      "# plt.imshow(X_1,cmap='gray',interpolation='none')\n",
      "plt.imshow(X_3,cmap='gray',interpolation='none')\n",
      "32/10:\n",
      "X=X_train.transpose();\n",
      "print(X.shape)\n",
      "X=X.reshape(784,X.shape[-1]);\n",
      "print(X.shape)\n",
      "32/11:\n",
      "X_o2=X_o2.transpose();\n",
      "# print(X_o2.shape)\n",
      "X_o2=X_o2.reshape(784,X_o2.shape[-1]);\n",
      "# print(X_o2.shape)\n",
      "\n",
      "X_o3=X_o3.transpose();\n",
      "print(X_o3.shape)\n",
      "X_o3=X_o3.reshape(784,X_o3.shape[-1]);\n",
      "print(X_o3.shape)\n",
      "\n",
      "X_c=np.concatenate((X_o2,X_o3),axis=1)\n",
      "32/12:\n",
      "mean_x=np.mean(X,axis=1); \n",
      "mean_x_o2 = np.mean(X_o2,axis=1); \n",
      "mean_x_o3 = np.mean(X_o3,axis=1); \n",
      "# print(mean_x.shape)\n",
      "#mean_x\n",
      "32/13:\n",
      "# print(mean_x.dot(np.ones((X.shape[0],X.shape[-1])) ) )\n",
      "### We are centralizing the data here by removing the mean\n",
      "Xb=X-mean_x.dot(np.ones((X.shape[0],X.shape[-1])) )\n",
      "\n",
      "Xb_o2=X_o2-mean_x.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "Xb_o3=X_o3-mean_x.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# Xb_o2=X_o2-mean_x_o2.dot(np.ones((X_o2.shape[0],X_o2.shape[-1])) )\n",
      "# Xb_o3=X_o3-mean_x_o3.dot(np.ones((X_o3.shape[0],X_o3.shape[-1])) )\n",
      "\n",
      "# print(type(Xb_o2))\n",
      "# print(Xb_o2.shape)\n",
      "# print(Xb_o3.shape)\n",
      "'''\n",
      "numpy.vstack:Equivalent to np.concatenate(tup, axis=0) \n",
      "numpy.hstack:Equivalent to np.concatenate(tup, axis=1)\n",
      "'''\n",
      "\n",
      "Xb_c=np.concatenate((Xb_o2,Xb_o3),axis=1) \n",
      "print(Xb_c.shape)\n",
      "32/14:\n",
      "u, s, vh = np.linalg.svd(Xb_c, full_matrices=True)\n",
      "# u_o2, s_o3, vh_o3 = np.linalg.svd(Xb_o2, full_matrices=True)\n",
      "# u_o3, s_o3, vh_o3 = np.linalg.svd(Xb_o3, full_matrices=True)\n",
      "32/15:\n",
      "y1=u[:,1].transpose().dot(X); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "32/16:\n",
      "y1_o2=y1[10000:]\n",
      "print(y1_o2.shape)\n",
      "32/17:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/18:\n",
      "y1=u[:,1].transpose().dot(X_c); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "32/19:\n",
      "y1=u[:,1].transpose().dot(X_c); y1.shape\n",
      "## y1 denotes the projection of the X along the first \n",
      "## principal component (dirn of max variance)\n",
      "\n",
      "# y_o2=u[:,1].transpose().dot(X_o2); y_o2.shape\n",
      "# y_o3=u[:,1].transpose().dot(X_o3); y_o3.shape\n",
      "32/20:\n",
      "y1_o2=y1[10000:]\n",
      "print(y1_o2.shape)\n",
      "32/21:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/22:\n",
      "y1_o2=y1[:5000]\n",
      "print(y1_o2.shape)\n",
      "32/23:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt.scatter(range(len(y1_o2)),y1_o2);\n",
      "plt.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "# plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/24:\n",
      "y1_o2=y1[:5000]\n",
      "print(y1_o2.shape)\n",
      "y1_o2=y1[7000:]\n",
      "32/25:\n",
      "y1_o2=y1[:5000]\n",
      "print(y1_o2.shape)\n",
      "y1_o3=y1[7000:]\n",
      "32/26:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt1.scatter(range(len(y1_o2)),y1_o2, color='red' );\n",
      "plt1.show()\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "plt3.show()\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/27:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt1.plt.scatter(range(len(y1_o2)),y1_o2, color='red' );\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/28:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt1=plt.scatter(range(len(y1_o2)),y1_o2, color='red' );\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y_o3)),y_o3);\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/29:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt1=plt.scatter(range(len(y1_o2)),y1_o2, color='red' );\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y1_o3)),y1_o3);\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/30:\n",
      "y1_o2=y1[:5000]\n",
      "print(y1_o2.shape)\n",
      "y1_o3=y1[7000:12000]\n",
      "32/31:\n",
      "# fig=plt.figure()\n",
      "\n",
      "plt1=plt.scatter(range(len(y1_o2)),y1_o2, color='red' );\n",
      "# plt2=plt.scatter(range(len(y_o2)),y_o2);\n",
      "plt3=plt.scatter(range(len(y1_o3)),y1_o3);\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# fig.savefig('2_3_img.png')\n",
      "#np.save('fig23.png',plt1)\n",
      "32/32: plot(np.diag(S))\n",
      "32/33: plt.plot(np.diag(S))\n",
      "32/34: plt.plot(np.diag(s))\n",
      "32/35: plt3.plot(np.diag(s))\n",
      "32/36: plt.plot(np.diag(s))\n",
      "32/37: plt.plot(np.diag(s)[:100])\n",
      "32/38: plt.plot(np.diag(s)[:40])\n",
      "32/39: plt.plot(np.diag(s)[:30])\n",
      "32/40: plt.plot(np.diag(s)[:20])\n",
      "32/41: plt.plot(np.diag(s)[:10])\n",
      "32/42: plt.plot(np.diag(s)[:4])\n",
      "32/43: print(np.diag(s)[:10])\n",
      "32/44: #print(np.diag(s)[:10])\n",
      "32/45: #plt.plot(np.diag(s)[:4])\n",
      "32/46: #plt.plot(np.diag(s))\n",
      "36/1:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "37/1:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "37/2:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "37/3:\n",
      "class Child(Base):\n",
      "    def __init__(self):\n",
      "        Base.init(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "37/4:\n",
      "b = Base()\n",
      "print('_'*20)\n",
      "c1 = Child()\n",
      "37/5:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "37/6:\n",
      "class Child(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.init(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "37/7:\n",
      "b = Base()\n",
      "print('_'*20)\n",
      "c1 = Child()\n",
      "37/8:\n",
      "class Child(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "37/9:\n",
      "b = Base()\n",
      "print('_'*20)\n",
      "c1 = Child()\n",
      "37/10:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Base):\n",
      "    def __init__(self):\n",
      "38/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "import numpy as np\n",
      "38/2:\n",
      "def make_data(N, f=0.3, rseed=1):\n",
      "    rand = np.random.RandomState(rseed)\n",
      "    x = rand.randn(N)\n",
      "    x[int(f * N):] += 5\n",
      "    return x\n",
      "\n",
      "x = make_data(1000)\n",
      "38/3: hist = plt.hist(x, bins=30, normed=True)\n",
      "38/4: hist = plt.hist(x, bins=30, density=True)\n",
      "38/5:\n",
      "density, bins, patches = hist\n",
      "widths = bins[1:] - bins[:-1]\n",
      "(density * widths).sum()\n",
      "38/6: x = make_data(20)\n",
      "38/7: bins = np.linspace(-5, 10, 10)\n",
      "38/8:\n",
      "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
      "                       sharex=True, sharey=True,\n",
      "                       subplot_kw={'xlim':(-4, 9),\n",
      "                                   'ylim':(-0.02, 0.3)})\n",
      "fig.subplots_adjust(wspace=0.05)\n",
      "for i, offset in enumerate([0.0, 0.6]):\n",
      "    ax[i].hist(x, bins=bins + offset, normed=True)\n",
      "    ax[i].plot(x, np.full_like(x, -0.01), '|k',\n",
      "               markeredgewidth=1)\n",
      "38/9:\n",
      "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
      "                       sharex=True, sharey=True,\n",
      "                       subplot_kw={'xlim':(-4, 9),\n",
      "                                   'ylim':(-0.02, 0.3)})\n",
      "fig.subplots_adjust(wspace=0.05)\n",
      "for i, offset in enumerate([0.0, 0.6]):\n",
      "    ax[i].hist(x, bins=bins + offset, density=True)\n",
      "    ax[i].plot(x, np.full_like(x, -0.01), '|k',\n",
      "               markeredgewidth=1)\n",
      "38/10:\n",
      "###  For example, if we look at a version of this data with only 20 points, \n",
      "### the choice of how to draw the bins can lead to an entirely different interpretation of the data!\n",
      "38/11:\n",
      "from scipy.stats import norm\n",
      "x_d = np.linspace(-4, 8, 1000)\n",
      "density = sum(norm(xi).pdf(x_d) for xi in x)\n",
      "\n",
      "plt.fill_between(x_d, density, alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n",
      "\n",
      "plt.axis([-4, 8, -0.2, 5]);\n",
      "38/12:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "38/13:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=2560\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "38/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "import numpy as np\n",
      "from keras.layers import Input, Dense\n",
      "from keras.models import Model\n",
      "import csv\n",
      "from keras.datasets import mnist\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from keras import regularizers\n",
      "import pandas as pd\n",
      "from random import choices\n",
      "from keras.utils import to_categorical\n",
      "38/15:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=2560\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "38/16:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=2560\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "38/17:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "38/18:\n",
      "## Visualizing the sample data\n",
      "print(argmax(to_categorical(o_h_d, vocab_len), axis =1 ))\n",
      "38/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "import numpy as np\n",
      "from keras.layers import Input, Dense\n",
      "from keras.models import Model\n",
      "import csv\n",
      "from keras.datasets import mnist\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from keras import regularizers\n",
      "import pandas as pd\n",
      "from random import choices\n",
      "from keras.utils import to_categorical\n",
      "from numpy import argmax\n",
      "38/20:\n",
      "## Visualizing the sample data\n",
      "print(argmax(to_categorical(o_h_d, vocab_len), axis =1 ))\n",
      "38/21:\n",
      "## Visualizing the sample data\n",
      "print(o_h_d)\n",
      "print(argmax(to_categorical(o_h_d, vocab_len), axis =1 ))\n",
      "38/22:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "\n",
      "x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/23:\n",
      "## Visualizing the sample data\n",
      "print(o_h_d)\n",
      "print(argmax(to_categorical(o_h_d, vocab_len*n_qubits), axis =0 ))\n",
      "38/24:\n",
      "## Visualizing the sample data\n",
      "print(x_tr)\n",
      "print(argmax(to_categorical(x_tr, vocab_len*n_qubits), axis =1 ))\n",
      "38/25:\n",
      "## Visualizing the sample data\n",
      "print(x_tr)\n",
      "#print(argmax(to_categorical(x_tr, vocab_len*n_qubits), axis =1 ))\n",
      "38/26:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "#print(argmax(to_categorical(x_tr, vocab_len*n_qubits), axis =1 ))\n",
      "38/27:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/28:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "#print(argmax(to_categorical(x_tr, vocab_len*n_qubits), axis =1 ))\n",
      "38/29: print(np.packbits(b, axis=1))\n",
      "38/30: print(np.packbits(x_tr, axis=1))\n",
      "38/31: print(np.packbits(int(x_tr), axis=1))\n",
      "38/32: print(np.packbits(x_tr, axis=1))\n",
      "38/33: print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/34:\n",
      "print(x_tr.astype(int))\n",
      "print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/35:\n",
      "print(x_tr.astype(int))\n",
      "B = np.asarray(x_tr)\n",
      "\n",
      "C = np.matrix([int(''.join(str(x) for x in column),2) for column in B]).reshape((B.shape[0],1))\n",
      "38/36:\n",
      "print(x_tr.astype(int))\n",
      "print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/37:\n",
      "print(x_tr.astype(int))\n",
      "print(x_tr.shape)\n",
      "print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/38:\n",
      "print(x_tr.astype(int))\n",
      "print(x_tr.shape)\n",
      "c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "#print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/39:\n",
      "print(x_tr.astype(int))\n",
      "print(x_tr.shape)\n",
      "c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "x_tr.dot(c)\n",
      "#print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/40:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/41:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/42:\n",
      "print(x_tr.astype(int))\n",
      "print(x_tr.shape)\n",
      "c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "x_tr.dot(c)\n",
      "#print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/43:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/44:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/45:\n",
      "# print(x_tr.astype(int))\n",
      "# print(x_tr.shape)\n",
      "# c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "# x_tr.dot(c)\n",
      "# #print(np.packbits(x_tr.astype(int), axis=1))\n",
      "38/46:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "print(data)\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/47:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "print(data.astype(int))\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/48:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "print(data.asarray.astype(int))\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/49:\n",
      "## Visualizing the sample data\n",
      "print(x_tr[4])\n",
      "print(np.array(data).astype(int))\n",
      "#print(argmax(to_categorical(x_tr, vocab_len**n_qubits), axis =1 ))\n",
      "38/50:\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "38/51: print(idx_samples)\n",
      "38/52: print(idx_samples.astype(int))\n",
      "38/53:\n",
      "idx_samples=np.array(idx_samples)\n",
      "print(idx_samples.astype(int))\n",
      "38/54:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/55:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "print(idx_samples.astype(int))\n",
      "38/56: hist = plt.hist(x, bins=30, density=True)\n",
      "38/57: hist = plt.hist(x, bins=100, density=True)\n",
      "38/58: hist = plt.hist(idx_samples, bins=30, density=True)\n",
      "38/59: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "38/60:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*400\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/61:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "38/62: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "38/63:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4000\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "38/64:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "38/65: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "38/66:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "38/67: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "38/68:\n",
      "###### With only index\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(idx_f, weights, k=test_size)\n",
      "#################\n",
      "38/69: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "38/70:\n",
      "###### With only index\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "38/71: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "39/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "import numpy as np\n",
      "from keras.layers import Input, Dense\n",
      "from keras.models import Model\n",
      "import csv\n",
      "from keras.datasets import mnist\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from keras import regularizers\n",
      "import pandas as pd\n",
      "from random import choices\n",
      "from keras.utils import to_categorical\n",
      "from numpy import argmax\n",
      "39/2:\n",
      "def make_data(N, f=0.3, rseed=1):\n",
      "    rand = np.random.RandomState(rseed)\n",
      "    x = rand.randn(N)\n",
      "    x[int(f * N):] += 5\n",
      "    return x\n",
      "\n",
      "x = make_data(1000)\n",
      "39/3: hist = plt.hist(x, bins=30, density=True)\n",
      "39/4:\n",
      "density, bins, patches = hist\n",
      "widths = bins[1:] - bins[:-1]\n",
      "(density * widths).sum()\n",
      "39/5: x = make_data(20)\n",
      "39/6: bins = np.linspace(-5, 10, 10)\n",
      "39/7:\n",
      "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
      "                       sharex=True, sharey=True,\n",
      "                       subplot_kw={'xlim':(-4, 9),\n",
      "                                   'ylim':(-0.02, 0.3)})\n",
      "fig.subplots_adjust(wspace=0.05)\n",
      "for i, offset in enumerate([0.0, 0.6]):\n",
      "    ax[i].hist(x, bins=bins + offset, density=True)\n",
      "    ax[i].plot(x, np.full_like(x, -0.01), '|k',\n",
      "               markeredgewidth=1)\n",
      "39/8:\n",
      "###  For example, if we look at a version of this data with only 20 points, \n",
      "### the choice of how to draw the bins can lead to an entirely different interpretation of the data!\n",
      "39/9:\n",
      "from scipy.stats import norm\n",
      "x_d = np.linspace(-4, 8, 1000)\n",
      "density = sum(norm(xi).pdf(x_d) for xi in x)\n",
      "\n",
      "plt.fill_between(x_d, density, alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n",
      "\n",
      "plt.axis([-4, 8, -0.2, 5]);\n",
      "39/10:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "39/11:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*4000\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "# print(x_tr.astype(int))\n",
      "# print(x_tr.shape)\n",
      "# c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "# x_tr.dot(c)\n",
      "# #print(np.packbits(x_tr.astype(int), axis=1))\n",
      "39/12:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "39/13: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "39/14:\n",
      "###### With only index\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "39/15: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "39/16: thist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "39/17:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "# print(x_tr.astype(int))\n",
      "# print(x_tr.shape)\n",
      "# c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "# x_tr.dot(c)\n",
      "# #print(np.packbits(x_tr.astype(int), axis=1))\n",
      "39/18:\n",
      "###### With only index\n",
      "test_size=500\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "39/19:\n",
      "###### With only index\n",
      "test_size=500\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "39/20:\n",
      "###### With only index\n",
      "test_size=500\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "39/21: thist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "39/22: thist = plt.thist(tidx_samples, bins=30, density=True)\n",
      "39/23: thist = plt.thist(tidx_samples, bins=30, density=True)\n",
      "39/24: thist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "39/25:\n",
      "###### With only index\n",
      "test_size=64*20\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "39/26: hist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "39/27: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "40/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "import numpy as np\n",
      "from keras.layers import Input, Dense\n",
      "from keras.models import Model\n",
      "import csv\n",
      "from keras.datasets import mnist\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from keras import regularizers\n",
      "import pandas as pd\n",
      "from random import choices\n",
      "from keras.utils import to_categorical\n",
      "from numpy import argmax\n",
      "40/2:\n",
      "def make_data(N, f=0.3, rseed=1):\n",
      "    rand = np.random.RandomState(rseed)\n",
      "    x = rand.randn(N)\n",
      "    x[int(f * N):] += 5\n",
      "    return x\n",
      "\n",
      "x = make_data(1000)\n",
      "40/3: hist = plt.hist(x, bins=30, density=True)\n",
      "40/4:\n",
      "density, bins, patches = hist\n",
      "widths = bins[1:] - bins[:-1]\n",
      "(density * widths).sum()\n",
      "40/5: x = make_data(20)\n",
      "40/6: bins = np.linspace(-5, 10, 10)\n",
      "40/7:\n",
      "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
      "                       sharex=True, sharey=True,\n",
      "                       subplot_kw={'xlim':(-4, 9),\n",
      "                                   'ylim':(-0.02, 0.3)})\n",
      "fig.subplots_adjust(wspace=0.05)\n",
      "for i, offset in enumerate([0.0, 0.6]):\n",
      "    ax[i].hist(x, bins=bins + offset, density=True)\n",
      "    ax[i].plot(x, np.full_like(x, -0.01), '|k',\n",
      "               markeredgewidth=1)\n",
      "40/8:\n",
      "###  For example, if we look at a version of this data with only 20 points, \n",
      "### the choice of how to draw the bins can lead to an entirely different interpretation of the data!\n",
      "40/9:\n",
      "from scipy.stats import norm\n",
      "x_d = np.linspace(-4, 8, 1000)\n",
      "density = sum(norm(xi).pdf(x_d) for xi in x)\n",
      "\n",
      "plt.fill_between(x_d, density, alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n",
      "\n",
      "plt.axis([-4, 8, -0.2, 5]);\n",
      "40/10:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "40/11:\n",
      "\n",
      "dataset = f'P_cond4_pauli4_8.csv'\n",
      "df = pd.read_csv(dataset, delimiter=',')\n",
      "x_gen = df[['index_c','counter','counter_1','b1','b2','a1','a2','P_ab']].values\n",
      "\n",
      "n_qubits=4;\n",
      "\n",
      "\n",
      "data_f=x_gen[:,[5,6,3,4]].tolist()\n",
      "\n",
      "weights=df['P_ab'].values\n",
      "\n",
      "size=64*40\n",
      "\n",
      "n_batch=256\n",
      "\n",
      "samples=choices(data_f, weights, k=size)\n",
      "\n",
      "data=samples\n",
      "\n",
      "###### With only index\n",
      "idx_f=x_gen[:,[0]].tolist()\n",
      "idx_samples=choices(idx_f, weights, k=size)\n",
      "#################\n",
      "\n",
      "test_size=500\n",
      "#n_batch=20\n",
      "\n",
      "test_samples=choices(data_f, weights, k=test_size)\n",
      "\n",
      "test_data=test_samples\n",
      "\n",
      "vocab_len=np.shape(data)[-1]\n",
      "# print(vocab_len)\n",
      "# vocab_len=4\n",
      "\n",
      "o_h_d=to_categorical(data)\n",
      "\n",
      "test_o_h_d=to_categorical(test_data)\n",
      "\n",
      "x_tr=np.zeros( (np.shape(o_h_d)[0],np.shape(o_h_d)[1], np.shape(o_h_d)[2] ) )\n",
      "x_tr[:,:,:]=o_h_d[:,:,:]\n",
      "#x_tr = x_tr.reshape((len(x_tr), np.prod(x_tr.shape[1:])))\n",
      "# print(x_tr.astype(int))\n",
      "# print(x_tr.shape)\n",
      "# c = 2**np.arange(x_tr.shape[1])[::-1] \n",
      "# x_tr.dot(c)\n",
      "# #print(np.packbits(x_tr.astype(int), axis=1))\n",
      "40/12:\n",
      "\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "40/13: hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "40/14:\n",
      "###### With only index\n",
      "test_size=64*20\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "#################\n",
      "40/15: hist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "40/16:\n",
      "###### With only index\n",
      "test_size=64*20\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/17: hist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "40/18:\n",
      "###### With only index\n",
      "test_size=64*50\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/19: hist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "40/20:\n",
      "###### With only index\n",
      "test_size=64*100\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/21: hist = plt.hist(tidx_samples, bins=30, density=True)\n",
      "40/22: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/23:\n",
      "###### With only index\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/24: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/25:\n",
      "size=64*400\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "40/26:\n",
      "size=64*400\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "40/27:\n",
      "###### With only index\n",
      "\n",
      "\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/28: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/29:\n",
      "###### With only index\n",
      "\n",
      "\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/30:\n",
      "###### With only index\n",
      "test_size=64*8\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/31: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/32:\n",
      "###### With only index\n",
      "test_size=64*9\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/33: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/34:\n",
      "###### With only index\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/35: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/36:\n",
      "###### With only index\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/37: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/38:\n",
      "size=64*400\n",
      "idx_samples=np.array(idx_samples)\n",
      "#print(idx_samples.astype(int))\n",
      "hist = plt.hist(idx_samples, bins=100, density=True)\n",
      "40/39:\n",
      "###### With only index\n",
      "test_size=64*10\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/40: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/41:\n",
      "###### With only index\n",
      "test_size=64*40\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/42: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/43:\n",
      "###### With only index\n",
      "test_size=64*40\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "40/44: hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/45:\n",
      "###### With only index\n",
      "test_size=64*40\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/46:\n",
      "###### With only index\n",
      "test_size=64*40\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/47:\n",
      "###### With only index\n",
      "test_size=64*40\n",
      "\n",
      "tidx_f=x_gen[:,[0]].tolist()\n",
      "tidx_samples=choices(tidx_f, weights, k=test_size)\n",
      "\n",
      "tidx_samples=np.array(tidx_samples)\n",
      "\n",
      "#################\n",
      "hist = plt.hist(tidx_samples, bins=100, density=True)\n",
      "40/48:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_f, np.full_like(tidx_f, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "40/49:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_f, np.full_like(tidx_f, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/50:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_fx[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(idx_samples[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_f, np.full_like(tidx_f, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/51:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_f[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(idx_samples[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_f, np.full_like(tidx_f, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/52:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(idx_samples[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_f, np.full_like(tidx_f, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/53:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(idx_samples[:, None])\n",
      "\n",
      "plt.fill_between(idx_samples, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/54:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(tidx_samples[:, None])\n",
      "\n",
      "plt.fill_between(tidx_samples, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/55:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(tidx_samples[:, None])\n",
      "\n",
      "plt.fill_between(tidx_samples, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/56:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x_d[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(tidx_samples[:, None])\n",
      "\n",
      "plt.fill_between(tidx_samples, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/57:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x_d[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(tidx_samples[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/58:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(tidx_samples[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/59:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/60:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/61:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, 2*size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/62:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/63:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, test_size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/64:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/65:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/66:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/67:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = tkde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/68:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "print(x.shape)\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "40/69:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "print(x.shape)\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "40/70:\n",
      "from scipy.stats import norm\n",
      "x_d = np.linspace(-4, 8, 1000)\n",
      "density = sum(norm(xi).pdf(x_d) for xi in x)\n",
      "\n",
      "plt.fill_between(x_d, density, alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n",
      "\n",
      "plt.axis([-4, 8, -0.2, 5]);\n",
      "40/71:\n",
      "## This is done using KDE\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "kde.fit(x[:, None])\n",
      "print(x.shape)\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = kde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
      "plt.ylim(-0.02, 0.22)\n",
      "40/72:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = tkde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/73:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "\n",
      "\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = tkde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/74:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = tkde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "40/75:\n",
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "x_d = np.linspace(0, 256, size)\n",
      "# instantiate and fit the KDE model\n",
      "tkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
      "tkde.fit(tidx_samples[:, None])\n",
      "\n",
      "# score_samples returns the log of the probability density\n",
      "logprob = tkde.score_samples(x_d[:, None])\n",
      "\n",
      "plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n",
      "plt.plot(tidx_samples, np.full_like(tidx_samples, -0.01), '|k', markeredgewidth=1)\n",
      "#plt.ylim(-0.02, 0.22)\n",
      "42/1:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/2:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "42/3:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/4:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/5:\n",
      "b = Base()\n",
      "print('_'*20)## Printing a space line for differentiation\n",
      "c1 = Child()\n",
      "42/6:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "42/7:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/8:\n",
      "b = Base()\n",
      "print('_'*20)## Printing a space line for differentiation\n",
      "c1 = Child()\n",
      "42/9:\n",
      "b = Base()\n",
      "print('_'*20)## Printing a space line for differentiation\n",
      "c1 = Child1()\n",
      "42/10:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Base):\n",
      "    def __init__(self):\n",
      "42/11:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Base):\n",
      "    def __init__(self):\n",
      "42/12:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child\n",
      "42/13:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/14:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/15:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/16: c3=child3()\n",
      "42/17: c3=Child3()\n",
      "42/18: c3=Child3()\n",
      "42/19: c3=Child3()\n",
      "42/20:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__()\n",
      "        Child2.__init__()\n",
      "        print('Child3.__init__')\n",
      "42/21: c3=Child3()\n",
      "42/22: c3=Child3()\n",
      "42/23:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/24: c3=Child3()\n",
      "42/25:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "42/26:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child1.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/27:\n",
      "b = Base()\n",
      "print('_'*20)## Printing a space line for differentiation\n",
      "c1 = Child1()\n",
      "42/28:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/29: c3=Child3()\n",
      "42/30: c3=Child2()\n",
      "42/31: c2=Child2()\n",
      "42/32:\n",
      "class Child2(Base):\n",
      "    def __init__(self):\n",
      "        Base.__init(self)\n",
      "        print('Child2.__init__')\n",
      "        \n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/33: c2=Child2()\n",
      "42/34:\n",
      "class Child2(Base):\n",
      "## Inheritence 2 \n",
      "    def __init__(self):\n",
      "        Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child2.__init')\n",
      "        ## Additionally Printing out the child init \n",
      "        \n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        Child1.__init__(self)\n",
      "        Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/35: c2=Child2()\n",
      "42/36: c3=Child3()\n",
      "42/37: # If we use super instead, we are safe\n",
      "42/38:\n",
      "class Base:\n",
      "    def __init__(self):\n",
      "        print('Base.__init')\n",
      "42/39:\n",
      "class Child1(Base):\n",
      "## Inheritence 1 \n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "#         Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child1.__init')\n",
      "        ## Additionally Printing out the child init\n",
      "42/40:\n",
      "b = Base()\n",
      "print('_'*20)## Printing a space line for differentiation\n",
      "c1 = Child1()\n",
      "42/41:\n",
      "class Child2(Base):\n",
      "## Inheritence 2 \n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        #         Base.__init__(self)\n",
      "        ## Directly calling a method from the base class\n",
      "        print('Child2.__init')\n",
      "        ## Additionally Printing out the child init \n",
      "        \n",
      "class Child3(Child1,Child2):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "#         Child1.__init__(self)\n",
      "#         Child2.__init__(self)\n",
      "        print('Child3.__init__')\n",
      "42/42: c3=Child3()\n",
      "43/1: import torch.nn as nn\n",
      "43/2: ## There is a module class in nn which contains anything we build\n",
      "43/3:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "43/4:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "43/5:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method\n",
      "43/6:\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "43/7:\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "43/8:\n",
      "class Network(nn.Module):\n",
      "    def __init__(self)\n",
      "43/9:\n",
      "class Network(nn.Module):\n",
      "    def __init__(self):\n",
      "43/10:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()\n",
      "        self.layer=None\n",
      "        \n",
      "    def forward(self, t):\n",
      "        t=self.layer(t)\n",
      "        return(t)\n",
      "43/11:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "#         self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        t=self.layer(t)\n",
      "        return(t)\n",
      "43/12:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init__()        \n",
      "#         self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linea r(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "43/13:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init__()        \n",
      "#         self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "43/14:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "43/15: network\n",
      "43/16:\n",
      "## Parameter vs Argument:: Parameters are used in function definitions\n",
      "## Parameters are like placeholders(remember tf lol)\n",
      "## Arguments are passed onto the placeholders\n",
      "\n",
      "## self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "## Name are the parameters and the values are arguments - TRIVIAL !!\n",
      "## Data dependent hyperparameter vs Hyperparameters\n",
      "\n",
      "#kernel_size :: filter size\n",
      "#out_channels:: After convolving using one filter gives output channel\n",
      "#If we give out_channels=6 implies there will be 6 different filters \n",
      "#Perhaps each of the kernel size\n",
      "\n",
      "#output layers are features map\n",
      "#If they go into lineaer layer they are called out features\n",
      "\n",
      "\n",
      "# Data dependent hyperparameter: in channel of conv_layer and outfeatures\n",
      "# Dealing with gray scale, in_channels=1\n",
      "# 10 classes of clothing article in fashion MNIST \n",
      "\n",
      "# Swithcing from conv layer to linear layer:: Need to flatten layer\n",
      "####### 12 * 4 * 4 - But 12 represents the previous  # output channels\n",
      "## Guess for 4*4 : 5-1=4 since 2 conv 4*4 ?? Doesn't seem right \n",
      "## Answer \n",
      "\n",
      "## The learnable parameters are the wrights that seats inside our layers\n",
      "43/17:\n",
      "network=Network()\n",
      "#init=initialize\n",
      "#The attributes are initialized with values for objects: Also values \n",
      "#can be objects\n",
      "#\n",
      "\n",
      "print(network\n",
      "     )\n",
      "43/18: # Where is the print:: in the base nn.module class\n",
      "43/19:\n",
      "# When we extend a class, we get all of its functionality, and to complement \n",
      "# this, we can add additional functionality. However, we can override\n",
      "## Example\n",
      "#repr = representation\n",
      "def __repr__(self):\n",
      "    return(\"Custom String\")\n",
      "\n",
      "## Hence this has been done in the base class nn.module already, so when\n",
      "## we call network with the nn module used in definition, we get back \n",
      "## the model we built\n",
      "## If we use only class Network()\n",
      "\n",
      "## we need to use def __repr__(self):\n",
      "##    return(\"Custom String\")\n",
      "43/20: network.fc1\n",
      "43/21: network.conv1\n",
      "43/22: network.conv2\n",
      "43/23: network.fc1\n",
      "43/24: network.fc2\n",
      "43/25: network.out\n",
      "43/26: network.conv1.weight\n",
      "43/27:\n",
      "## Class Parameter(torch.tensor)\n",
      "## This overrides __repr__() and thus we see Parameter containing\n",
      "## Parameter Class actually extend on the tensor class\n",
      "43/28: network.conv1.weight.shape\n",
      "43/29: network.conv2.weight.shape\n",
      "43/30: network.fc1.weight.shape\n",
      "43/31: network.fc2.weight.shape\n",
      "43/32: network.out.weight.shape\n",
      "43/33:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.con2.weight[0].shape\n",
      "43/34:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.conv2.weight[0].shape\n",
      "44/1: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "45/1: import torch.nn as nn\n",
      "45/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "45/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "45/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "45/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "45/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "45/7:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "45/8:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "45/9: import torch.nn as nn\n",
      "45/10:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "45/11:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "45/12: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "45/13: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "45/14: network.fc2.weight.shape\n",
      "46/1: import torch.nn as nn\n",
      "46/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "46/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "46/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "46/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/7:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/8:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "46/9: import torch.nn as nn\n",
      "46/10:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "46/11:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "46/12:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "46/13:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/14:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/15:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/16:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/17:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/18:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/19:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/20:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/21:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/22:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/23:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/24:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/25:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/26:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network= Network()\n",
      "46/27: network\n",
      "46/28:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "46/29: network\n",
      "46/30:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "46/31:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/32:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "46/33: import torch.nn as nn\n",
      "48/1: import torch.nn as nn\n",
      "48/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "48/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "48/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "48/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "48/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "48/7:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "48/8: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "48/9:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "48/10:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "48/11:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "48/12:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "48/13: network\n",
      "48/14:\n",
      "## Parameter vs Argument:: Parameters are used in function definitions\n",
      "## Parameters are like placeholders(remember tf lol)\n",
      "## Arguments are passed onto the placeholders\n",
      "\n",
      "## self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "## Name are the parameters and the values are arguments - TRIVIAL !!\n",
      "## Data dependent hyperparameter vs Hyperparameters\n",
      "\n",
      "#kernel_size :: filter size\n",
      "#out_channels:: After convolving using one filter gives output channel\n",
      "#If we give out_channels=6 implies there will be 6 different filters \n",
      "#Perhaps each of the kernel size\n",
      "\n",
      "#output layers are features map\n",
      "#If they go into lineaer layer they are called out features\n",
      "\n",
      "\n",
      "# Data dependent hyperparameter: in channel of conv_layer and outfeatures\n",
      "# Dealing with gray scale, in_channels=1\n",
      "# 10 classes of clothing article in fashion MNIST \n",
      "\n",
      "# Swithcing from conv layer to linear layer:: Need to flatten layer\n",
      "####### 12 * 4 * 4 - But 12 represents the previous  # output channels\n",
      "## Guess for 4*4 : 5-1=4 since 2 conv 4*4 ?? Doesn't seem right \n",
      "## Answer \n",
      "\n",
      "## The learnable parameters are the wrights that seats inside our layers\n",
      "48/15:\n",
      "network=Network()\n",
      "#init=initialize\n",
      "#The attributes are initialized with values for objects: Also values \n",
      "#can be objects\n",
      "#\n",
      "\n",
      "print(network)\n",
      "48/16: # Where is the print:: in the base nn.module class\n",
      "48/17:\n",
      "# When we extend a class, we get all of its functionality, and to complement \n",
      "# this, we can add additional functionality. However, we can override\n",
      "## Example\n",
      "#repr = representation\n",
      "def __repr__(self):\n",
      "    return(\"Custom String\")\n",
      "\n",
      "## Hence this has been done in the base class nn.module already, so when\n",
      "## we call network with the nn module used in definition, we get back \n",
      "## the model we built\n",
      "## If we use only class Network()\n",
      "\n",
      "## we need to use def __repr__(self):\n",
      "##    return(\"Custom String\")\n",
      "48/18: network.conv1\n",
      "48/19: network.conv2\n",
      "48/20: network.fc1\n",
      "48/21: network.fc2\n",
      "48/22: network.out\n",
      "48/23: network.conv1.weight\n",
      "48/24:\n",
      "## Class Parameter(torch.tensor)\n",
      "## This overrides __repr__() and thus we see Parameter containing\n",
      "## Parameter Class actually extend on the tensor class\n",
      "## We can access the parameter class using \"Super\"\n",
      "48/25:\n",
      "network.conv1.weight.shape\n",
      "## The weights for each of the 6 kernels aren't stored separatelh\n",
      "## it represented by rank 4 tensor lol?\n",
      "\n",
      "##[6,1,5,5] 4 axis\n",
      "48/26:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.conv2.weight[0].shape\n",
      "## [6,5,5] = [depth,height,width]\n",
      "48/27:\n",
      "network.fc1.weight.shape\n",
      "## rank 2 height and width\n",
      "48/28: network.fc2.weight.shape\n",
      "48/29: network.out.weight.shape\n",
      "48/30: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "48/31:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "48/32: in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "48/33:\n",
      "import torch\n",
      "in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "48/34:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "48/35: weight_matrix.matmul(in_features)\n",
      "48/36:\n",
      "for param in network.parameters():\n",
      "    print(param.shape)\n",
      "48/37: for name, param in network.named_parameters():\n",
      "48/38:\n",
      "for name, param in network.named_parameters():\n",
      "    print(name, '\\t\\t', param.shape)\n",
      "48/39: fc=nn.Linear(in_features=4,out_features=3)\n",
      "48/40: fc(in_features)\n",
      "48/41: fc.weight=nn.Parameter(weight_matrix) #This is where we define our W matr\n",
      "48/42: fc(in_features)\n",
      "48/43: fc=nn.Linear(in_features=4,out_features=3,bias=False)\n",
      "48/44: fc=nn.Linear(in_features=4,out_features=3,bias=False)\n",
      "48/45: fc(in_features)\n",
      "48/46: fc.weight=nn.Parameter(weight_matrix) #This is where we define our W matr\n",
      "48/47: fc(in_features)\n",
      "48/48:\n",
      "# fc=nn.Linear(in_features=4,out_features=3,bias=False) \n",
      "## Then we have tensor([30., 40., 50.] answer\n",
      "## y=Ax+b\n",
      "48/49: # Linear layer: y=Ax+b ??\n",
      "48/50:\n",
      "## Forward(input) -> We simply call an object Layer(INPUT) under\n",
      "## the hood this then calls __CALL__[INPUT] and then it then calls\n",
      "## Forward\n",
      "48/51: fc=nn.Linear(in_features=4, out_features=3)\n",
      "50/1: import torch.nn as nn\n",
      "50/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "50/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "50/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "50/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "50/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Transformations are actually defined here\n",
      "#         t=self.layer(t)\n",
      "        return(t)\n",
      "50/7:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "50/8: network\n",
      "50/9:\n",
      "## Parameter vs Argument:: Parameters are used in function definitions\n",
      "## Parameters are like placeholders(remember tf lol)\n",
      "## Arguments are passed onto the placeholders\n",
      "\n",
      "## self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "## Name are the parameters and the values are arguments - TRIVIAL !!\n",
      "## Data dependent hyperparameter vs Hyperparameters\n",
      "\n",
      "#kernel_size :: filter size\n",
      "#out_channels:: After convolving using one filter gives output channel\n",
      "#If we give out_channels=6 implies there will be 6 different filters \n",
      "#Perhaps each of the kernel size\n",
      "\n",
      "#output layers are features map\n",
      "#If they go into lineaer layer they are called out features\n",
      "\n",
      "\n",
      "# Data dependent hyperparameter: in channel of conv_layer and outfeatures\n",
      "# Dealing with gray scale, in_channels=1\n",
      "# 10 classes of clothing article in fashion MNIST \n",
      "\n",
      "# Swithcing from conv layer to linear layer:: Need to flatten layer\n",
      "####### 12 * 4 * 4 - But 12 represents the previous  # output channels\n",
      "## Guess for 4*4 : 5-1=4 since 2 conv 4*4 ?? Doesn't seem right \n",
      "## Answer \n",
      "\n",
      "## The learnable parameters are the wrights that seats inside our layers\n",
      "50/10:\n",
      "network=Network()\n",
      "#init=initialize\n",
      "#The attributes are initialized with values for objects: Also values \n",
      "#can be objects\n",
      "#\n",
      "\n",
      "print(network)\n",
      "50/11: # Where is the print:: in the base nn.module class\n",
      "50/12:\n",
      "# When we extend a class, we get all of its functionality, and to complement \n",
      "# this, we can add additional functionality. However, we can override\n",
      "## Example\n",
      "#repr = representation\n",
      "def __repr__(self):\n",
      "    return(\"Custom String\")\n",
      "\n",
      "## Hence this has been done in the base class nn.module already, so when\n",
      "## we call network with the nn module used in definition, we get back \n",
      "## the model we built\n",
      "## If we use only class Network()\n",
      "\n",
      "## we need to use def __repr__(self):\n",
      "##    return(\"Custom String\")\n",
      "50/13: network.conv1\n",
      "50/14: network.conv2\n",
      "50/15: network.fc1\n",
      "50/16: network.fc2\n",
      "50/17: network.out\n",
      "50/18: network.conv1.weight\n",
      "50/19:\n",
      "## Class Parameter(torch.tensor)\n",
      "## This overrides __repr__() and thus we see Parameter containing\n",
      "## Parameter Class actually extend on the tensor class\n",
      "## We can access the parameter class using \"Super\"\n",
      "50/20:\n",
      "network.conv1.weight.shape\n",
      "## The weights for each of the 6 kernels aren't stored separatelh\n",
      "## it represented by rank 4 tensor lol?\n",
      "\n",
      "##[6,1,5,5] 4 axis\n",
      "50/21:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.conv2.weight[0].shape\n",
      "## [6,5,5] = [depth,height,width]\n",
      "50/22:\n",
      "network.fc1.weight.shape\n",
      "## rank 2 height and width\n",
      "50/23: network.fc2.weight.shape\n",
      "50/24: network.out.weight.shape\n",
      "50/25:\n",
      "import torch\n",
      "in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "50/26:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "50/27: weight_matrix.matmul(in_features)\n",
      "50/28:\n",
      "for param in network.parameters():\n",
      "    print(param.shape)\n",
      "50/29:\n",
      "for name, param in network.named_parameters():\n",
      "    print(name, '\\t\\t', param.shape)\n",
      "50/30: fc=nn.Linear(in_features=4,out_features=3)\n",
      "50/31: fc(in_features)\n",
      "50/32: fc.weight=nn.Parameter(weight_matrix) #This is where we define our W matr\n",
      "50/33: fc(in_features)\n",
      "50/34:\n",
      "# fc=nn.Linear(in_features=4,out_features=3,bias=False) \n",
      "## Then we have tensor([30., 40., 50.] answer\n",
      "## y=Ax+b\n",
      "50/35: # Linear layer: y=Ax+b ??\n",
      "50/36:\n",
      "## Forward(input) -> We simply call an object Layer(INPUT) under\n",
      "## the hood this then calls __CALL__[INPUT] and then it then calls\n",
      "## Forward\n",
      "50/37: fc=nn.Linear(in_features=4, out_features=3)\n",
      "50/38:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "50/39: ## Understanding the forward propagation\n",
      "50/40:\n",
      "inport torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printotions(linewidth=120)\n",
      "50/41:\n",
      "inport torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printotions(linewidth=120)\n",
      "50/42: import torch.nn as nn\n",
      "50/43:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printotions(linewidth=120)\n",
      "50/44:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "50/45: train_set=torchvision.datasets.FashionMNIST(root='/data/FashionMNIST')\n",
      "50/46:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='/data/FashionMNIST', train=True\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transform.ToTensor()]))\n",
      "50/47:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='/data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transform.ToTensor()]))\n",
      "50/48:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='/data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]))\n",
      "50/49:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='/data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/50:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/51:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transforms=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/52:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/53:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "50/54:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=trasforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/55:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "50/56: class Network(nn.Module):\n",
      "50/57:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "50/58:\n",
      "## Stop the gradient flow to reduce storage space yet\n",
      "## storage done as a dynamic Computational graph\n",
      "torch.set_grad_enabled(False)\n",
      "50/59: network = Network()\n",
      "50/60:\n",
      "sample, label = sample\n",
      "image.shape\n",
      "50/61:\n",
      "sample=next(iter(train_set)\n",
      "sample, label = sample\n",
      "image.shape\n",
      "50/62:\n",
      "sample=next(iter(train_set) )\n",
      "sample, label = sample\n",
      "image.shape\n",
      "50/63:\n",
      "sample=next(iter(train_set) )\n",
      "image, label = sample\n",
      "image.shape\n",
      "50/64: ## Need to send in batches\n",
      "50/65:\n",
      "## 4 dim tensor needed as we need a batch\n",
      "## For CNN (batch_size,in_channels,height,width\n",
      "image.unsqueeze(0).shape\n",
      "50/66: pred = network(image.unsqueeze(0))\n",
      "50/67: pred = network(image.unsqueeze(0))\n",
      "50/68: pred.argmax(dim=1)\n",
      "50/69: F.softmax(pred,dim=1)\n",
      "50/70: label\n",
      "50/71: print(torch.__version__)\n",
      "50/72:\n",
      "print(torch.__version__)\n",
      "print(torchvision.__vision__)\n",
      "50/73:\n",
      "print(torch.__version__)\n",
      "print(torchvision.__version__)\n",
      "50/74:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "     def forward(self, t):\n",
      " \n",
      "        # (1)\n",
      "        t=t     \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "     \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "  \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "50/76:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "         def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/77:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/78: data loader = torch.utils.data.DataLoader()\n",
      "50/79: data loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/80: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/81: batch=next(iter(data_loader))\n",
      "50/82: images, labels=batch\n",
      "50/83:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/84: labels.shape\n",
      "50/85: pred=network(images)\n",
      "50/86: pred.shape\n",
      "50/87: pred\n",
      "50/88: preds.argmax(dim=1)\n",
      "50/89: pred.argmax(dim=1)\n",
      "50/90: labels\n",
      "50/91:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/92: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/93: batch=next(iter(data_loader))\n",
      "50/94: images, labels=batch\n",
      "50/95:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/96: labels.shape\n",
      "50/97: pred=network(images)\n",
      "50/98: pred.shape\n",
      "50/99: pred\n",
      "50/100: pred.argmax(dim=1)\n",
      "50/101: labels\n",
      "50/102:\n",
      "## Stop the gradient flow to reduce storage space yet\n",
      "## storage done as a dynamic Computational graph\n",
      "torch.set_grad_enabled(False)\n",
      "50/103: network = Network()\n",
      "50/104:\n",
      "sample=next(iter(train_set) )\n",
      "image, label = sample\n",
      "image.shape\n",
      "50/105: ## Need to send in batches\n",
      "50/106:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/107: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/108: batch=next(iter(data_loader))\n",
      "50/109: images, labels=batch\n",
      "50/110:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/111: labels.shape\n",
      "50/112: pred=network(images)\n",
      "50/113: pred.shape\n",
      "50/114: pred=network(images)\n",
      "50/115:\n",
      "network=Network()\n",
      "pred=network(images)\n",
      "50/116: pred.shape\n",
      "50/117:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/118: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/119: batch=next(iter(data_loader))\n",
      "50/120: images, labels=batch\n",
      "50/121:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/122: labels.shape\n",
      "50/123:\n",
      "network=Network()\n",
      "pred=network(images)\n",
      "50/124:\n",
      "network=Network()\n",
      "preds=network(images)\n",
      "50/125: preds.shape\n",
      "50/126:\n",
      "network=Network()\n",
      "preds=network(images)\n",
      "50/127:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/128: preds.shape\n",
      "50/129: predw\n",
      "50/130: preds\n",
      "50/131: preds.shape\n",
      "50/132:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/133:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/134:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "50/135: preds=network(images)\n",
      "50/136: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/137: batch=next(iter(data_loader))\n",
      "50/138: images, labels=batch\n",
      "50/139:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/140: labels.shape\n",
      "50/141:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/142: preds.shape\n",
      "50/143:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/144: nwtwork=Network()\n",
      "50/145: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "50/146: batch=next(iter(data_loader))\n",
      "50/147: images, labels=batch\n",
      "50/148:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "50/149: labels.shape\n",
      "50/150:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/151:\n",
      "\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/152:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "50/153: labels.shape\n",
      "50/154:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "52/1: import torch.nn as nn\n",
      "52/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "52/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "52/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "52/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "52/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "52/7: ## Understanding the forward propagation\n",
      "52/8:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "52/9: network\n",
      "52/10:\n",
      "## Parameter vs Argument:: Parameters are used in function definitions\n",
      "## Parameters are like placeholders(remember tf lol)\n",
      "## Arguments are passed onto the placeholders\n",
      "\n",
      "## self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "## Name are the parameters and the values are arguments - TRIVIAL !!\n",
      "## Data dependent hyperparameter vs Hyperparameters\n",
      "\n",
      "#kernel_size :: filter size\n",
      "#out_channels:: After convolving using one filter gives output channel\n",
      "#If we give out_channels=6 implies there will be 6 different filters \n",
      "#Perhaps each of the kernel size\n",
      "\n",
      "#output layers are features map\n",
      "#If they go into lineaer layer they are called out features\n",
      "\n",
      "\n",
      "# Data dependent hyperparameter: in channel of conv_layer and outfeatures\n",
      "# Dealing with gray scale, in_channels=1\n",
      "# 10 classes of clothing article in fashion MNIST \n",
      "\n",
      "# Swithcing from conv layer to linear layer:: Need to flatten layer\n",
      "####### 12 * 4 * 4 - But 12 represents the previous  # output channels\n",
      "## Guess for 4*4 : 5-1=4 since 2 conv 4*4 ?? Doesn't seem right \n",
      "## Answer \n",
      "\n",
      "## The learnable parameters are the wrights that seats inside our layers\n",
      "52/11:\n",
      "network=Network()\n",
      "#init=initialize\n",
      "#The attributes are initialized with values for objects: Also values \n",
      "#can be objects\n",
      "#\n",
      "\n",
      "print(network)\n",
      "52/12: # Where is the print:: in the base nn.module class\n",
      "52/13:\n",
      "# When we extend a class, we get all of its functionality, and to complement \n",
      "# this, we can add additional functionality. However, we can override\n",
      "## Example\n",
      "#repr = representation\n",
      "def __repr__(self):\n",
      "    return(\"Custom String\")\n",
      "\n",
      "## Hence this has been done in the base class nn.module already, so when\n",
      "## we call network with the nn module used in definition, we get back \n",
      "## the model we built\n",
      "## If we use only class Network()\n",
      "\n",
      "## we need to use def __repr__(self):\n",
      "##    return(\"Custom String\")\n",
      "52/14: network.conv1\n",
      "52/15: network.conv2\n",
      "52/16: network.fc1\n",
      "52/17: network.fc2\n",
      "52/18: network.out\n",
      "52/19: network.conv1.weight\n",
      "52/20:\n",
      "## Class Parameter(torch.tensor)\n",
      "## This overrides __repr__() and thus we see Parameter containing\n",
      "## Parameter Class actually extend on the tensor class\n",
      "## We can access the parameter class using \"Super\"\n",
      "52/21:\n",
      "network.conv1.weight.shape\n",
      "## The weights for each of the 6 kernels aren't stored separatelh\n",
      "## it represented by rank 4 tensor lol?\n",
      "\n",
      "##[6,1,5,5] 4 axis\n",
      "52/22:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.conv2.weight[0].shape\n",
      "## [6,5,5] = [depth,height,width]\n",
      "52/23:\n",
      "network.fc1.weight.shape\n",
      "## rank 2 height and width\n",
      "52/24: network.fc2.weight.shape\n",
      "52/25: network.out.weight.shape\n",
      "52/26:\n",
      "import torch\n",
      "in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "52/27:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "52/28: weight_matrix.matmul(in_features)\n",
      "52/29:\n",
      "for param in network.parameters():\n",
      "    print(param.shape)\n",
      "52/30:\n",
      "for name, param in network.named_parameters():\n",
      "    print(name, '\\t\\t', param.shape)\n",
      "52/31: fc=nn.Linear(in_features=4,out_features=3)\n",
      "52/32: fc(in_features)\n",
      "52/33: fc.weight=nn.Parameter(weight_matrix) #This is where we define our W matr\n",
      "52/34: fc(in_features)\n",
      "52/35:\n",
      "# fc=nn.Linear(in_features=4,out_features=3,bias=False) \n",
      "## Then we have tensor([30., 40., 50.] answer\n",
      "## y=Ax+b\n",
      "52/36: # Linear layer: y=Ax+b ??\n",
      "52/37:\n",
      "## Forward(input) -> We simply call an object Layer(INPUT) under\n",
      "## the hood this then calls __CALL__[INPUT] and then it then calls\n",
      "## Forward\n",
      "52/38: fc=nn.Linear(in_features=4, out_features=3)\n",
      "52/39:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "52/40:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "52/41:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "52/42:\n",
      "## Stop the gradient flow to reduce storage space yet\n",
      "## storage done as a dynamic Computational graph\n",
      "torch.set_grad_enabled(False)\n",
      "52/43: network = Network()\n",
      "52/44:\n",
      "sample=next(iter(train_set) )\n",
      "image, label = sample\n",
      "image.shape\n",
      "52/45: ## Need to send in batches\n",
      "52/46:\n",
      "## 4 dim tensor needed as we need a batch\n",
      "## For CNN (batch_size,in_channels,height,width)\n",
      "image.unsqueeze(0).shape ## Gives a batch size 1\n",
      "52/47:\n",
      "pred = network(image.unsqueeze(0))\n",
      "## image shape needs to be (batch_size,in_channels,height,width\n",
      "## But we have\n",
      "52/48: pred.argmax(dim=1)\n",
      "52/49: F.softmax(pred,dim=1)\n",
      "52/50: label\n",
      "52/51:\n",
      "print(torch.__version__)\n",
      "print(torchvision.__version__)\n",
      "52/52:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "52/53: nwtwork=Network()\n",
      "52/54: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "52/55: batch=next(iter(data_loader))\n",
      "52/56: images, labels=batch\n",
      "52/57:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "52/58: labels.shape\n",
      "52/59:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "52/60: preds.shape\n",
      "52/61: preds\n",
      "52/62: pred.argmax(dim=1)\n",
      "52/63: labels\n",
      "52/64: pred.argmax(dim=1)\n",
      "52/65: preds\n",
      "52/66: preds.argmax(dim=1)\n",
      "52/67: labels\n",
      "52/68: pred.argmaz(dim=1).eq(labels)\n",
      "52/69: pred.argmax(dim=1).eq(labels)\n",
      "52/70: pred.argmax(dim=1).eq(labels).sum()\n",
      "52/71: preds.argmax(dim=1).eq(labels).sum()\n",
      "52/72:\n",
      "def(get_num_correct(pred,labels)):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "52/73:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "52/74: ## Flow tracking\n",
      "52/75:\n",
      "import torch.otpim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "52/76:\n",
      "import torch.otpim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "52/77:\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "52/78: network=Network()\n",
      "52/79:\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "53/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.otpim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "53/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "53/3:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "53/4:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "53/5:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "53/6: network=Network()\n",
      "53/7: train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "53/8:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "53/9: network=Network()\n",
      "53/10:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "53/11:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "53/12:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "53/13:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "53/14:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "53/15:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "53/16: network=Network()\n",
      "53/17:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "53/18:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "54/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "54/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "54/3:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "54/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "54/5: network=Network()\n",
      "54/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "54/7:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "55/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "55/3:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "55/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "55/5: network=Network()\n",
      "55/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "55/7:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/8:\n",
      " import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "55/9:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "55/10:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "55/11:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "55/12:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "55/13: network=Network()\n",
      "55/14:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "55/15:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/16:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/17:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/18:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "network=Network()\n",
      "55/19:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/20:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "\n",
      "network=Network()\n",
      "55/21:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/22: network = Network()\n",
      "55/23:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "55/24:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "55/25:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "55/26:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "56/1:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "56/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/3:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/5: network = Network()\n",
      "56/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/7:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "56/8:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "56/9:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/10:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "56/11: network = Network()\n",
      "56/12:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/13:\n",
      "##Loss\n",
      "preds=network(images)\n",
      "56/14:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/15:\n",
      "##Loss\n",
      "preds = network()\n",
      "56/16:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch=next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/17:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/18:\n",
      "class Network(nn.Module):\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/19:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/20: network = Network()\n",
      "56/21:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/22:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/23:\n",
      "class Network(nn.Module):\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "        \n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/24:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/25: network = Network()\n",
      "56/26:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/27:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/28:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "56/29:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/30:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/31:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/32:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/33: network = Network()\n",
      "56/34:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/35:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "52/80:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "52/81: nwtwork=Network()\n",
      "52/82: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "52/83: batch=next(iter(data_loader))\n",
      "52/84: images, labels=batch\n",
      "52/85:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "52/86: labels.shape\n",
      "52/87:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "52/88:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "52/89: nwtwork=Network()\n",
      "52/90: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "52/91: batch=next(iter(data_loader))\n",
      "52/92: images, labels=batch\n",
      "52/93:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "52/94: labels.shape\n",
      "52/95:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "52/96: preds.shape\n",
      "52/97:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "57/1: import torch.nn as nn\n",
      "57/2:\n",
      "## There is a module class in nn which contains anything we build\n",
      "## Our layers and neural networks will be functions that we inherit\n",
      "## Base class:: We are going to extend it\n",
      "\n",
      "## ref:deeplizard_CNN@ut\n",
      "57/3:\n",
      "## When we pass a tensor :: It proceeds forward to the output layer, \n",
      "## forward pass- Transformations\n",
      "## Similarly we shall have a backward pass?\n",
      "## def forward(self, input):: from nn.functional package\n",
      "57/4:\n",
      "##1. Extend the nn module\n",
      "##2. Define layers as class attributes like no () we do for method\n",
      "##3. Implement the foward method (things from nn.functional)\n",
      "57/5:\n",
      "## Constructor\n",
      "## Missing rn:: no nn.functional call\n",
      "## Need to use Super \n",
      "\n",
      "class Network:\n",
      "    def __init__(self):\n",
      "        self.layer=None\n",
      "    def forward(self,t):\n",
      "        t = self.layer(t)\n",
      "        return t\n",
      "57/6:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "57/7: ## Understanding the forward propagation\n",
      "57/8:\n",
      "## Get an instance of of the network by calling the constructor\n",
      "network = Network()\n",
      "57/9: network\n",
      "57/10:\n",
      "## Parameter vs Argument:: Parameters are used in function definitions\n",
      "## Parameters are like placeholders(remember tf lol)\n",
      "## Arguments are passed onto the placeholders\n",
      "\n",
      "## self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "## Name are the parameters and the values are arguments - TRIVIAL !!\n",
      "## Data dependent hyperparameter vs Hyperparameters\n",
      "\n",
      "#kernel_size :: filter size\n",
      "#out_channels:: After convolving using one filter gives output channel\n",
      "#If we give out_channels=6 implies there will be 6 different filters \n",
      "#Perhaps each of the kernel size\n",
      "\n",
      "#output layers are features map\n",
      "#If they go into lineaer layer they are called out features\n",
      "\n",
      "\n",
      "# Data dependent hyperparameter: in channel of conv_layer and outfeatures\n",
      "# Dealing with gray scale, in_channels=1\n",
      "# 10 classes of clothing article in fashion MNIST \n",
      "\n",
      "# Swithcing from conv layer to linear layer:: Need to flatten layer\n",
      "####### 12 * 4 * 4 - But 12 represents the previous  # output channels\n",
      "## Guess for 4*4 : 5-1=4 since 2 conv 4*4 ?? Doesn't seem right \n",
      "## Answer \n",
      "\n",
      "## The learnable parameters are the wrights that seats inside our layers\n",
      "57/11:\n",
      "network=Network()\n",
      "#init=initialize\n",
      "#The attributes are initialized with values for objects: Also values \n",
      "#can be objects\n",
      "#\n",
      "\n",
      "print(network)\n",
      "57/12: # Where is the print:: in the base nn.module class\n",
      "57/13:\n",
      "# When we extend a class, we get all of its functionality, and to complement \n",
      "# this, we can add additional functionality. However, we can override\n",
      "## Example\n",
      "#repr = representation\n",
      "def __repr__(self):\n",
      "    return(\"Custom String\")\n",
      "\n",
      "## Hence this has been done in the base class nn.module already, so when\n",
      "## we call network with the nn module used in definition, we get back \n",
      "## the model we built\n",
      "## If we use only class Network()\n",
      "\n",
      "## we need to use def __repr__(self):\n",
      "##    return(\"Custom String\")\n",
      "57/14: network.conv1\n",
      "57/15: network.conv2\n",
      "57/16: network.fc1\n",
      "57/17: network.fc2\n",
      "57/18: network.out\n",
      "57/19: network.conv1.weight\n",
      "57/20:\n",
      "## Class Parameter(torch.tensor)\n",
      "## This overrides __repr__() and thus we see Parameter containing\n",
      "## Parameter Class actually extend on the tensor class\n",
      "## We can access the parameter class using \"Super\"\n",
      "57/21:\n",
      "network.conv1.weight.shape\n",
      "## The weights for each of the 6 kernels aren't stored separatelh\n",
      "## it represented by rank 4 tensor lol?\n",
      "\n",
      "##[6,1,5,5] 4 axis\n",
      "57/22:\n",
      "network.conv2.weight.shape\n",
      "## [12,6,5,5] #6 coming out of previous:: depth matches the number of chan \n",
      "network.conv2.weight[0].shape\n",
      "## [6,5,5] = [depth,height,width]\n",
      "57/23:\n",
      "network.fc1.weight.shape\n",
      "## rank 2 height and width\n",
      "57/24: network.fc2.weight.shape\n",
      "57/25: network.out.weight.shape\n",
      "57/26:\n",
      "import torch\n",
      "in_features=torch.tensor([1,2,3,4],dtype=torch.float32 )\n",
      "57/27:\n",
      "weight_matrix=torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6] ] , \n",
      "                           dtype=torch.float32)\n",
      "57/28: weight_matrix.matmul(in_features)\n",
      "57/29:\n",
      "for param in network.parameters():\n",
      "    print(param.shape)\n",
      "57/30:\n",
      "for name, param in network.named_parameters():\n",
      "    print(name, '\\t\\t', param.shape)\n",
      "57/31: fc=nn.Linear(in_features=4,out_features=3)\n",
      "57/32: fc(in_features)\n",
      "57/33: fc.weight=nn.Parameter(weight_matrix) #This is where we define our W matr\n",
      "57/34: fc(in_features)\n",
      "57/35:\n",
      "# fc=nn.Linear(in_features=4,out_features=3,bias=False) \n",
      "## Then we have tensor([30., 40., 50.] answer\n",
      "## y=Ax+b\n",
      "57/36: # Linear layer: y=Ax+b ??\n",
      "57/37:\n",
      "## Forward(input) -> We simply call an object Layer(INPUT) under\n",
      "## the hood this then calls __CALL__[INPUT] and then it then calls\n",
      "## Forward\n",
      "57/38: fc=nn.Linear(in_features=4, out_features=3)\n",
      "57/39:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "57/40:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "57/41:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "57/42:\n",
      "## Stop the gradient flow to reduce storage space yet\n",
      "## storage done as a dynamic Computational graph\n",
      "torch.set_grad_enabled(False)\n",
      "57/43: network = Network()\n",
      "57/44:\n",
      "sample=next(iter(train_set) )\n",
      "image, label = sample\n",
      "image.shape\n",
      "57/45: ## Need to send in batches\n",
      "57/46:\n",
      "## 4 dim tensor needed as we need a batch\n",
      "## For CNN (batch_size,in_channels,height,width)\n",
      "image.unsqueeze(0).shape ## Gives a batch size 1\n",
      "57/47:\n",
      "pred = network(image.unsqueeze(0))\n",
      "## image shape needs to be (batch_size,in_channels,height,width\n",
      "## But we have\n",
      "57/48: pred.argmax(dim=1)\n",
      "57/49: F.softmax(pred,dim=1)\n",
      "57/50: label\n",
      "57/51:\n",
      "print(torch.__version__)\n",
      "print(torchvision.__version__)\n",
      "57/52:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "57/53: nwtwork=Network()\n",
      "57/54: data_loader = torch.utils.data.DataLoader(train_set,batch_size=10)\n",
      "57/55: batch=next(iter(data_loader))\n",
      "57/56: images, labels=batch\n",
      "57/57:\n",
      "#unsqueezing isn't necessary since we work with data_loader\n",
      "images.shape\n",
      "57/58: labels.shape\n",
      "57/59:\n",
      "#network=Network()\n",
      "preds=network(images)\n",
      "57/60: preds.shape\n",
      "57/61: preds\n",
      "57/62: preds.argmax(dim=1)\n",
      "57/63: labels\n",
      "57/64: pred.argmax(dim=1).eq(labels)\n",
      "57/65: preds.argmax(dim=1).eq(labels).sum()\n",
      "57/66:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "57/67: ## Flow tracking\n",
      "57/68:\n",
      "import torch.optim as optim\n",
      "torch.set_grad_enabled(True)\n",
      "56/36:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "56/37:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/38:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/39:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/40: network = Network()\n",
      "56/41:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/42:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/43: network.conv1.weight\n",
      "56/44:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/45:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/46: network = Network()\n",
      "56/47:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/48:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/49:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "56/50:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/51:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/52:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/53: network = Network()\n",
      "56/54:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/55:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/56:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "56/57:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/58:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/59:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/60: network = Network()\n",
      "56/61:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/62:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/63:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/64:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      "        \n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)\n",
      "56/65:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/66: network = Network()\n",
      "56/67:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/68:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/69:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)   \n",
      "            \n",
      "            # (6) output layer\n",
      "            t=self.out(t)\n",
      "56/70:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)   \n",
      "            \n",
      "            # (6) output layer\n",
      "            t=self.out(t)\n",
      "            \n",
      "            return t\n",
      "56/71:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/72: network = Network()\n",
      "56/73:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/74:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/75:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/76:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)   \n",
      "            \n",
      "            # (6) output layer\n",
      "            t=self.out(t)\n",
      "            \n",
      "            return t\n",
      "56/77:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/78: network = Network()\n",
      "56/79:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/80:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/81:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "56/82:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "56/83:\n",
      "class Network(nn.Module):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "         \n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        def forward(self, t):\n",
      " \n",
      "            # (1)\n",
      "            t=t     \n",
      "\n",
      "            # (2)\n",
      "            t=self.conv1(t)\n",
      "\n",
      "            t=F.relu(t)\n",
      "\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (3)\n",
      "            t=self.conv2(t)\n",
      "            t=F.relu(t)\n",
      "            t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "\n",
      "            # (4)\n",
      "            t=t.reshape(-1,12*4*4)\n",
      "            t=self.fc1(t)\n",
      "            t=F.relu(t)\n",
      "\n",
      "            # (5)\n",
      "\n",
      "            t=self.fc2(t)\n",
      "            t=F.relu(t)   \n",
      "            \n",
      "            # (6) output layer\n",
      "            t=self.out(t)\n",
      "            \n",
      "            return t\n",
      "56/84:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/85: network = Network()\n",
      "56/86:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/87:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/88: network = Network()\n",
      "56/89:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/90:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/91:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "56/92:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "56/93: network = Network()\n",
      "56/94:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "56/95:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "57/69: nn.F.conv2d\n",
      "57/70: F.conv2d\n",
      "57/71: help(F.conv2d)\n",
      "51/1:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "    padding=nn.ConstantPad2d(padding)\n",
      "    \n",
      "    Y = F.conv2d(X,self.kernel, self.bias, self.stride, padding=nn.ConstantPad2d(self.padding))\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/2:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "51/3:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "51/4:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "    padding=nn.ConstantPad2d(padding)\n",
      "    \n",
      "    Y = F.conv2d(X,self.kernel, self.bias, self.stride, padding=nn.ConstantPad2d(self.padding))\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/5:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/6:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "    padding=nn.ConstantPad2d(padding)\n",
      "    \n",
      "    Y = F.conv2d(X,self.kernel, self.bias, self.stride, padding=nn.ConstantPad2d(padding))\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/7:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/8:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "    padding=nn.ConstantPad2d(padding)\n",
      "    \n",
      "    Y = F.conv2d(X,self.kernel, self.bias, self.stride, nn.ConstantPad2d(padding))\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/9:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/10:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    \n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/11:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/12:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    \n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return X\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/13:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/14:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "51/15:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "51/16:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/17:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/18:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights))) + \n",
      "    torch.ones(X.shape[0]).matmul(torch.transpose(self.bias))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/19:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights))) + torch.ones(X.shape[0]).matmul(torch.transpose(self.bias))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/20:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/21:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0]).matmul(torch.transpose(self.bias),1,0)\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/22:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/23:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias),1,0)\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/24:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/25:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias),1,0)\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/26:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias),1,1)\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/27:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/28:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias,1,0))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/29:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/30:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias,1,0))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/31:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/32:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias,1,0))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/33:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/34:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose(self.bias,1,0))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/35:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/36:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/37:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/38:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/39:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/40:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "51/41:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "51/42:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p=  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "   batch_size=pred_score.shape(0)\n",
      "    \n",
      "   loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1))\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "    return loss.mean()\n",
      "51/43:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "   batch_size=pred_score.shape(0)\n",
      "    \n",
      "   loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1))\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "    return loss.mean()\n",
      "51/44:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1))\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "    return loss.mean()\n",
      "51/45:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "    return loss.mean()\n",
      "51/46:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "   return loss.mean()\n",
      "51/47:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "   return loss.mean()\n",
      "51/48:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p= 2 \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/49:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.exp(scores).sum(dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/50:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.exp(scores).sum(dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/51:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/52:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/53: softmax1d(torch.ones(4,2))\n",
      "51/54:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "    print(score.shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/55:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(score.shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/56: softmax1d(torch.ones(4,2))\n",
      "51/57:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/58: softmax1d(torch.ones(4,2))\n",
      "51/59:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "    \n",
      "  ###  Star your code here ### \n",
      "  p=torch.exponential(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/60: softmax1d(torch.ones(4,2))\n",
      "51/61:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "    \n",
      "  ###  Star your code here ### \n",
      "  p=torch.exponential_(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/62: softmax1d(torch.ones(4,2))\n",
      "51/63:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "    \n",
      "  ###  Star your code here ### \n",
      "  p=torch.exponential(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/64:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exponent(scores).shape)  \n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/65: softmax1d(torch.ones(4,2))\n",
      "51/66:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/67: softmax1d(torch.ones(4,2))\n",
      "51/68:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.exp(scores)/torch.sum(torch.exp(scores),dim=1)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/69: softmax1d(torch.ones(4,2))\n",
      "51/70:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.divide(torch.exp(scores) ,torch.sum(torch.exp(scores),dim=1) )\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/71: softmax1d(torch.ones(4,2))\n",
      "51/72:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p=torch.div(torch.exp(scores) ,torch.sum(torch.exp(scores),dim=1) )\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/73: softmax1d(torch.ones(4,2))\n",
      "51/74:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/75: softmax1d(torch.ones(4,2))\n",
      "51/76: softmax1d(torch.ones(4,3))\n",
      "51/77:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "51/78:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3])\n",
      "51/79:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "51/80:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/81:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/82:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/83:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=pred_score.shape(0)\n",
      "  print(batch_size)\n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/84:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/85:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=torch.constant(pred_score.shape(0))\n",
      "  print(batch_size)\n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/86:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/87:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape(0))\n",
      "  print(batch_size)\n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/88:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/89:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape(0))\n",
      "  print(batch_size)\n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/90:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape(0))\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape(0)).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/91:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/92:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape(0))\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape(0)).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/93:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape(0)).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/94:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/95:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(preds_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/96:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/97:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "51/98:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.3,0.3,0.4],[0.4,0.3,0.3],[0.3,0.4,0.3] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/99:\n",
      "softmax1d(torch.ones(4,3))\n",
      "a=torch.eye(3)\n",
      "b=torch.tensor([[0.33,0.33,0.33],[0.33,0.33,0.33],[0.33,0.33,0.33] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/100:\n",
      "softmax1d(torch.ones(4,4))\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "51/101:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# a=torch.eye(4)\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print()\n",
      "print(cross_entropy_loss(b,a))\n",
      "56/96:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "56/97:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/1:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "58/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "58/3:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "58/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "58/5: network = Network()\n",
      "58/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "58/7:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "58/8:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/9: print(network.conv1.weight.grad)\n",
      "58/10: loss.backward()\n",
      "58/11: print(network.conv1.weight.grad.shape)\n",
      "58/12:\n",
      "##\n",
      "optimizer = optim.Adam( network.parametes(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/13:\n",
      "##\n",
      "optimizer = optim.Adam(network.parametes(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/14: ## Illustrate the effect of the optimizer step\n",
      "58/15:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "58/16:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "58/17:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super(Network,self).__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "58/18:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "58/19: network = Network()\n",
      "58/20:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "58/21:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "58/22:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/23: print(network.conv1.weight.grad)\n",
      "58/24: loss.backward()\n",
      "58/25: print(network.conv1.weight.grad.shape)\n",
      "58/26:\n",
      "##\n",
      "optimizer = optim.Adam(network.parametes(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/27:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "58/28:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "58/29: network = Network()\n",
      "58/30:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "58/31:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "58/32:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/33: print(network.conv1.weight.grad)\n",
      "58/34: loss.backward()\n",
      "58/35: print(network.conv1.weight.grad.shape)\n",
      "58/36:\n",
      "##\n",
      "optimizer = optim.Adam(network.parametes(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/37:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "58/38:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "58/39:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "58/40:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "58/41: network = Network()\n",
      "58/42:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "58/43:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "58/44:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/45: print(network.conv1.weight.grad)\n",
      "58/46: loss.backward()\n",
      "58/47: print(network.conv1.weight.grad.shape)\n",
      "58/48:\n",
      "##\n",
      "optimizer = optim.Adam(network.parametes(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/49: ## Illustrate the effect of the optimizer step\n",
      "58/50:\n",
      "##\n",
      "optimizer = optim.Adam(network.parameters(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "58/51:\n",
      "## Illustrate the effect of the optimizer step\n",
      "loss.item\n",
      "58/52:\n",
      "## Illustrate the effect of the optimizer step\n",
      "loss.item()\n",
      "58/53:\n",
      "##\n",
      "optimizer = optim.Adam(network.parameters(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "optimizer.step()\n",
      "58/54:\n",
      "## Illustrate the effect of the optimizer step\n",
      "loss.item()\n",
      "58/55:\n",
      "## Illustrate the effect of the optimizer step\n",
      "preds=network(images)\n",
      "loss.item()\n",
      "58/56:\n",
      "## Illustrate the effect of the optimizer step\n",
      "preds=network(images)\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/57: get_num_correct(preds,labels)\n",
      "58/58:\n",
      "##\n",
      "optimizer = optim.Adam(network.parameters(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "optimizer.step()\n",
      "58/59:\n",
      "## Illustrate the effect of the optimizer step\n",
      "preds=network(images)\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "58/60: get_num_correct(preds,labels)\n",
      "59/1:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "59/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "59/3:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "59/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "59/5: network = Network()\n",
      "59/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "59/7:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "59/8:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "59/9: print(network.conv1.weight.grad)\n",
      "59/10: loss.backward()\n",
      "59/11: print(network.conv1.weight.grad.shape)\n",
      "59/12:\n",
      "##\n",
      "optimizer = optim.Adam(network.parameters(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "optimizer.step()\n",
      "59/13:\n",
      "## Illustrate the effect of the optimizer step\n",
      "preds=network(images)\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "59/14: get_num_correct(preds,labels)\n",
      "59/15: get_num_correct(preds,labels)\n",
      "60/1:\n",
      "import asyncio \n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "torch.set_printoptions(linewidth=120)\n",
      "import torch.optim as optim\n",
      "torch.set_printoptions(linewidth=120)\n",
      "\n",
      "\n",
      "torch.set_grad_enabled(True)\n",
      "60/2:\n",
      "def get_num_correct(pred,labels):\n",
      "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
      "60/3:\n",
      "class Network(nn.Module):\n",
      "    ## Extending pytorch nn module class\n",
      "\n",
      "    def __init__(self):\n",
      "        #super(Network,self).__init()        \n",
      "        super().__init__() #__init__:mybug pred, mightbe inco               \n",
      "        #self.layer=None\n",
      "        self.conv1=nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
      "        self.conv2=nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
      "        \n",
      "        ## fc for fully connected layers or can call dense(keras/tf)\n",
      "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
      "        self.fc2=nn.Linear(in_features=120, out_features=60)\n",
      "        self.out=nn.Linear(in_features=60, out_features=10)\n",
      "        \n",
      "    def forward(self, t):\n",
      "        ## Lol identity map - input layer (1)\n",
      "        t=t \n",
      "        # We call the instance directly- let's call the conv\n",
      "        \n",
      "        \n",
      "        # (2)\n",
      "        t=self.conv1(t)\n",
      "        \n",
      "        ## relu and the pool don't have weights thus we call them here\n",
      "        ## Calling them pooling layers and activation layers\n",
      "        ## Doesn't do justice since they dont have weights\n",
      "        \n",
      "        \n",
      "        t=F.relu(t)\n",
      "        \n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        # (3)\n",
      "        t=self.conv2(t)\n",
      "        t=F.relu(t)\n",
      "        t=F.max_pool2d(t,kernel_size=2,stride=2)\n",
      "        \n",
      "        ## Need to reshape or faltten our tensor \n",
      "        ## Note:: 12*4*4 ?? 12 is the #output channels in the pre layer\n",
      "        ## We started with 1*28*28 input tensor \n",
      "        ## \n",
      "        \n",
      "        \n",
      "        # (4)\n",
      "        t=t.reshape(-1,12*4*4)\n",
      "        t=self.fc1(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        # (5)\n",
      "\n",
      "        t=self.fc2(t)\n",
      "        t=F.relu(t)\n",
      "        \n",
      "        ## When we pass this to our output layer, take care of 10 (classes)\n",
      "        \n",
      "        #(6)\n",
      "        t=self.out(t)\n",
      "        #t=F.softmax(t,dim=1) \n",
      "        ## Returns a probability for each of the prediction classes\n",
      "        ## Doubt: why is the dimension used here =1 ?\n",
      "        \n",
      "        # Loss function used is the cross entropy which implicitly uses\n",
      "        # a softmax:: Since it's implicitly used we can forget softmax\n",
      "        \n",
      "        # While training the softmax is gonna be used since its there \n",
      "        # in the loss function computation but softmax is not gonna \n",
      "        # be invoked during inference::: Pretty Cool!\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        ## Transformations are actually defined here\n",
      "        #t=self.layer(t)\n",
      "        return(t)\n",
      "60/4:\n",
      "train_set=torchvision.datasets.FashionMNIST(root='./data/FashionMNIST', train=True,\n",
      "                                           download=True,transform=transforms.Compose(\n",
      "                                           [transforms.ToTensor()]) )\n",
      "60/5: network = Network()\n",
      "60/6:\n",
      "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
      "batch = next(iter(train_loader))\n",
      "images, labels=batch\n",
      "60/7:\n",
      "##Loss\n",
      "preds = network(images)\n",
      "60/8: get_num_correct(preds,labels)\n",
      "60/9:\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "60/10: print(network.conv1.weight.grad)\n",
      "60/11: loss.backward()\n",
      "60/12: print(network.conv1.weight.grad.shape)\n",
      "60/13:\n",
      "##\n",
      "optimizer = optim.Adam(network.parameters(), lr=0.01 )\n",
      "loss.item()\n",
      "get_num_correct(preds,labels)\n",
      "optimizer.step()\n",
      "60/14:\n",
      "## Illustrate the effect of the optimizer step\n",
      "preds=network(images)\n",
      "loss=F.cross_entropy(preds,labels)\n",
      "loss.item()\n",
      "60/15: get_num_correct(preds,labels)\n",
      "51/102:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  weight=tensor.add_(weight,-lr*grad)\n",
      "  ###  End of the code ###\n",
      "51/103:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  weights = weights.add_(weights,alpha=-lr,grad)\n",
      "  ###  End of the code ###\n",
      "51/104:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "  ###  End of the code ###\n",
      "62/1:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "  ###  End of the code ###\n",
      "62/2:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "63/2:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "63/3:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "63/4:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "63/5:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "63/6:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "63/7:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/8:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# a=torch.eye(4)\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/9:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "  ###  End of the code ###\n",
      "63/10:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/11:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/12:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/13:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/14:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/15: np.zeros(3,4)\n",
      "63/16: np.zeros((3,4)\n",
      "63/17: np.zeros((3,4))\n",
      "63/18: A=np.zeros((3,4));A=A.reshape(-1,12)\n",
      "63/19: A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "63/20:\n",
      "A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "print(A.shape)\n",
      "63/21:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    \n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/22:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/23:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/24:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/25:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/26:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/27:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# a=torch.eye(4)\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "63/28:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/29:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/30:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(scores):\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/31:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/32:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(scores)\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/33:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/34:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/35:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/36:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/37:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/38:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/39:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/40:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/41:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/42:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/43:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/44:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "    \n",
      "  loss.type(torch.Float32)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/45:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# a=torch.eye(4)\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "63/46:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/47:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/48:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/49:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/50:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/51:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/52:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/53:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/54:\n",
      "softmax1d(torch.ones(4,4))\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/55:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "    \n",
      "  loss.type(torch.float32)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/56:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "63/57:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "63/58:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "63/59:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "63/60:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "63/61:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "63/62:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "    \n",
      "  loss.type(torch.float32)\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/63:\n",
      "softmax1d(torch.ones(4,4))\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/64:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/65:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/66:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/67:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/68:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/69:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/70:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/71:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/72:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/73:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32, device=tensor.device)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/74:\n",
      "softmax1d(torch.ones(4,4))\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/75:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/76:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/77:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/78:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/79:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/80:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/81:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/82:\n",
      "softmax1d(torch.ones(4,4))\n",
      "a=torch.eye(4,dtype=torch.float64)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/83:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/84:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/85:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/86:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/87:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  print(scores.shape)\n",
      "  print(torch.exp(scores).shape)  \n",
      "  print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  print(batch_size)\n",
      "  print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "63/88:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "63/89:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/90:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  print(weights.shape)\n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/91:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/92:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print('Printing weight list shape',weights.shape)\n",
      "    \n",
      "  weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/93:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/94:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/95:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  \n",
      "  t_weights = torch.stack(weights)  \n",
      "  t_weights = weights.add_(weights,-lr,grad)\n",
      "    \n",
      "  print('Printing weight list shape',t_weights.shape) \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/96:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/97:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  \n",
      "  t_weights = torch.stack(weights)  \n",
      "  t_weights = weights.add_(t_weights,-lr,grad)\n",
      "    \n",
      "  print('Printing weight list shape',t_weights.shape) \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/98:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "  \n",
      "  t_weights = torch.stack(weights)  \n",
      "  t_grad = torch.stack(grad)   \n",
      "    \n",
      "  t_weights = weights.add_(t_weights,-lr,t_grad)\n",
      "    \n",
      "  print('Printing weight list shape',t_weights.shape) \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/99:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/100:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  for weight_item and grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weights,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/101:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weights,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/102:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/103:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(weight_item)\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/104:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/105:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/106:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/107:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights))\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/108:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/109:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/110:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/111:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/112:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/113:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/114:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/115:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/116:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/117:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/118:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights.parameters(),grad):\n",
      "        weight_item = weight_item.add_(weight_item,-lr,grad_item)\n",
      "    \n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/119:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/120:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "#   for weight_item ,grad_item in zip(weights,grad):\n",
      "#         t_weight_item=weight_item.data\n",
      "#         t_weight_item = t_weight_item.add_(t_weight_item,-lr,grad_item)\n",
      "    \n",
      "  weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/121:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/122:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #t_weight_item=weight_item.data\n",
      "        weight_item.data = weight_item.data_(weight_item.data,-lr,grad_item)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/123:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/124:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data(weight_item.data,-lr,grad_item)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/125:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/126:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "  print(type(weights[0]))\n",
      "  print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data(weight_item.data,-lr,grad_item.data)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/127:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/128:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(grad_item)\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data(weight_item.data,-lr,grad_item.data)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/129:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(grad_item) )\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data(weight_item.data,-lr,grad_item.data)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/130:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/131:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(grad_item) )\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data.add_(weight_item.data, -lr , grad_item.data)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/132:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/133:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(grad_item) )\n",
      "        #t_weight_item=weight_item.data\n",
      "        \n",
      "        weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/134:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/135:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        print(type(grad_item) )\n",
      "        \n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/136:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/137:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/138:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/139:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/140:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/141:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/142:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #print(type(grad_item) )\n",
      "        \n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/143:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/144:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/145:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/146:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/147:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/148:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/149:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #print(type(grad_item) )\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/150:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/151:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "\n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/152:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/153:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/154:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/155:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    grad+=1e-4\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/156:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        \n",
      "        grad_item.zero()\n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/157:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    #grad+=1e-4\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/158:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        torch.add(grad_item,1e-4)\n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data = weight_item.data - lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/159:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/160:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    #grad+=1e-4\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/161:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/162:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    torch.add(grad,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/163:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/164:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= - lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/165:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/166:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/167:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/168:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/169:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "63/170:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "63/171:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "63/172:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "63/173:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "63/174:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "63/175:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "63/176:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "66/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "66/2:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "66/3:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "66/4:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "66/5:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "66/6:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "66/7:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "66/8:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "66/9:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "66/10:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "66/11:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "66/12:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "66/13:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "66/14:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "66/15:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "66/16:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "66/17: # dataset loading, training script and other codes starts here:\n",
      "66/18: # dataset loading, training script and other codes starts here:\n",
      "66/19:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "65/1:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(scores.max)  \n",
      "  p = torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "65/2:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "  p = torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "65/3:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "67/2:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "67/3:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "67/4:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "67/5:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "67/6:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "67/7:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "  p = torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/8:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/9:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/10:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/11:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/12:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/13:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/14:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/15:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/16:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "67/17: # dataset loading, training script and other codes starts here:\n",
      "67/18: # dataset loading, training script and other codes starts here:\n",
      "67/19:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "67/20:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "  p = torch.exp(torch.min(scores,15 ) ) / torch.sum(torch.exp(torch.min(scores,15 ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/21:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/22:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/23:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/24:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "    \n",
      "  score_check=torch.zeros_like(scores)  \n",
      "  score_check+=15\n",
      "    \n",
      "  p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/25:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/26:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        print(weight_item,grad_item)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/27:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/28:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/29:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/30:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/31:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/32:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/33:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "  ## BE CAREFUL HERE  torch.min\n",
      "  score_check=torch.zeros_like(scores)  \n",
      "  score_check+=15\n",
      "  ## BE CAREFUL_HERE \n",
      "\n",
      "  p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/34:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/35:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/36:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/37:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/38:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/39:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/40:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/41:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/42:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "  ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "#   ## BE CAREFUL_HERE \n",
      "\n",
      "  p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  #p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/43:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/44:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/45:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/46:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/47:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/48:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/49:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/50:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/51:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "  ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "#   ## BE CAREFUL_HERE \n",
      "\n",
      "  #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/52:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/53:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/54:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/55:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "  ## BE CAREFUL HERE  torch.min\n",
      "  score_check=torch.zeros_like(scores)  \n",
      "  score_check+=15\n",
      "#   ## BE CAREFUL_HERE \n",
      "\n",
      "  #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "  p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  \n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/56:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "#print(cross_entropy_loss(b,a))\n",
      "67/57:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "67/58:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "67/59:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/60:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/61:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/62:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/63:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/64:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/65:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/66:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "67/67: # dataset loading, training script and other codes starts here:\n",
      "67/68: # dataset loading, training script and other codes starts here:\n",
      "67/69:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "67/70:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "\n",
      "   scores_new=scores-torch.max(scores) \n",
      "   p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/71:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "\n",
      "  scores_new=scores-torch.max(scores) \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/72:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/73:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/74:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "    \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/75:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "67/76:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/77:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/78:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/79:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/80:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/81:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/82:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/83:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/84:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "67/85:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        print(grad)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/86:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/87:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/88:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/89:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/90:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/91:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/92:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/93:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/94:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/95:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/96:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/97:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/98:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/99:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/100:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/101:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/102:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/103:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/104:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/105:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/106:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/107:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "67/108:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        print(\"Now printing grad_item_data\")\n",
      "        print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/109:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/110:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/111:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/112:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/113:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/114:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/115:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  print(loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "67/116:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print(p,labels)\n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/117:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/118:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/119:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/120:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/121:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/122:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/123:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/124:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/125:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/126:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/127:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/128:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/129:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/130:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/131:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/132:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/133:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/134:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "67/135:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/136:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "67/137:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "67/138:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "67/139:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "67/140:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "67/141:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "67/142:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "68/2:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "68/3:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "68/4:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "68/5:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "68/6:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "68/7:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "  print(loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/8:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/9:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/10:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/11:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/12:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/13:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/14:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/15:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/16:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "68/17: # dataset loading, training script and other codes starts here:\n",
      "68/18: # dataset loading, training script and other codes starts here:\n",
      "68/19:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "68/20:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/21:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/22:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/23:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/24:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/25:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/26:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/27:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/28:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/29:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "    \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum(labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                                           +1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/30:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/31:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/32:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/33:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/34:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/35:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/36:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/37:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/38:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/39:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                                           +1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/40:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/41:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/42:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/43:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/44:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/45:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/46:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/47:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/48:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                                           +1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/49:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/50:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/51:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/52:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/53:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/54:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                                           +1e-5*torch.ones_like(pred_score.view(batch_size, -1)) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/55:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/56:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/57:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/58:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/59:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/60:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/61:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/62:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/63:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1))  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/64:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/65:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/66:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/67:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/68:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/69:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/70:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/71:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/72:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] )\n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/73:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/74:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/75:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/76:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/77:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/78:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/79:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/80:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/81:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  loss= torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/82:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/83:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/84:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/85:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/86:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/87:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/88:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/89:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/90:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "68/91: # dataset loading, training script and other codes starts here:\n",
      "68/92: # dataset loading, training script and other codes starts here:\n",
      "68/93:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "68/94:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1, dtype=torch.float32) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/95:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "a=torch.eye(4)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/96:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/97:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/98:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/99:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/100:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/101:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/102:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/103:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/104:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/105:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/106:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/107:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/108:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/109:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/110:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ))  \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/111:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/112:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/113:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1) ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/114:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/115:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                          ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/116:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                           ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/117:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=4) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                           ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/118:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/119:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/120:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 )) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                           ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/121:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/122:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/123:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3],dtype=torch.float32)\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/124:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/125:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/126:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/127:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/128:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/129:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/130:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/131:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/132:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/133:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],\n",
      "                [0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ],dtype=torch.float32)\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/134:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/135:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ), dtype=torch.float32 ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/136:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/137:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/138:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/139:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/140:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/141:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/142:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/143:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/144:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/145:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/146:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/147:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/148:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/149:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  print(torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "    \n",
      "  loss=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/150:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/151:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1)  \n",
      "  loss=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/152:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/153:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1,dtype=torch.float32)  \n",
      "  loss=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/154:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/155:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1,dtype=torch.float32)  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/156:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/157:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1,dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/158:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/159:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1,dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/160:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/161:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros(pred_score.shape[0] ,1,dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/162:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/163:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/164:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/165:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/166:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/167:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/168:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/169:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/170:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "    \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  print(type(loss))\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/171:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  print(type(loss))  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  print(type(loss))\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/172:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/173:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  print(type(loss))  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  print(torch.dtype(loss))\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/174:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/175:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  #print(type(loss))  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  print(torch.dtype(loss))\n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/176:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/177:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/178:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/179:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/180:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/181:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1.0*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/182:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1.*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/183:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "                                         + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/184:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/185:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  \n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/186:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/187:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  torch.sum(torch.log(pred_score.view(batch_size, -1)  )\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/188:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/189:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  torch.sum(torch.log(pred_score.view(batch_size, -1)  ) )\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/190:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/191:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum(torch.log(pred_score.view(batch_size, -1)  ) )\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/192:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/193:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum(torch.log(pred_score.view(batch_size, -1)  )  ,dim=1)\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/194:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/195:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum(-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  )  ,dim=1)\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/196:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/197:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  labels=F.one_hot(labels,num_classes=-1) \n",
      "  print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (pred_score.shape[0] ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum(-1.0*labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  )  ,dim=1)\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/198:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/199:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum(-1.0*labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  )  ,dim=1)\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/200:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/201:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32) \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  #                                       + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32)  \n",
      "  #loss+=-1*torch.sum( labels.view(batch_size, -1) ) \n",
      "  loss+=torch.sum( -1.0*labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1)  )  ,dim=1)\n",
      "   #print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "  \n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      "  #loss.type(torch.float32)\n",
      "  \n",
      "    ###  End of the code ###\n",
      "  \n",
      "  return loss.mean()\n",
      "68/202:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/203:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  prob_pred_score=  \n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "\n",
      "'''\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  ##print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "                                        + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "'''\n",
      "68/204:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "\n",
      "'''\n",
      "  #print(batch_size)\n",
      "  #print(torch.tensor(pred_score.shape[0]).shape)  \n",
      "  \n",
      "  #+1e-5*torch.ones_like(pred_score.view(batch_size, -1)) \n",
      "  #loss=torch.zeros(pred_score.shape[0],pred_score.shape[1] ,dtype=torch.float32)\n",
      "  \n",
      "  #print(torch.log(pred_score.view(batch_size, -1) \n",
      "  ##print(type(loss))  \n",
      "  \n",
      "   #loss+=-1.0*torch.sum( labels.view(batch_size, -1) * torch.log(pred_score.view(batch_size, -1) \n",
      "   #                                      + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ), dim=1)\n",
      "  \n",
      "  #loss = torch.as_tensor(loss, dtype=torch.float32)\n",
      "                                        + 1e-5*torch.ones_like(pred_score.view(batch_size, -1),dtype=torch.float32 ) ) ) \n",
      "'''\n",
      "68/205:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/206:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/207:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score))\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/208:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/209:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/210:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/211:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p= torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/212:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/213:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/214:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/215:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*ohe_labels*torch.log(pred_score) ,dim=1)\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/216:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1.0*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/217:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/218:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/219: torch.set_default_dtype(torch.float32)\n",
      "68/220:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "68/221:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "68/222:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "68/223:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "68/224:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "68/225:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      "\n",
      "\n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/226:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/227:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.torch.tensor.log(pred_score).dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/228:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/229:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.tensor.log(pred_score).dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/230:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.torch.log(pred_score) )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/231:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/232:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.torch.log(pred_score).dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/233:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/234:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.(ohe_labels*torch.log(pred_score) ).dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/235:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.ohe_labels*torch.log(pred_score) .dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/236:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.ohe_labels*torch.log(pred_score) .dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/237:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print(torch.ohe_labels*torch.log(pred_score) .dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1,dtype=torch.float32 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/238:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/239:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  loss+=torch.sum( -1*(ohe_labels*torch.log(pred_score)) ,dim=1,dtype=torch.float32 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/240:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/241:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score)  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/242:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/243:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score)  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/244:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/245:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log2(pred_score)  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/246:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/247:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log10(pred_score)  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/248:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/249:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score)  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/250:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/251:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + 1e-5*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/252:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/253:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + 1e-5*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/254:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + 1e-5*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/255:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/256:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1, )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/257:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/258:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=1 )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/259:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/260:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/261:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/262:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+=torch.sum( -1*(ohe_labels*logy_hat ) ,dim=0)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/263:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/264:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss+= -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/265:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/266:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss = -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/267:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/268:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "    \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/269:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print(ohe_labels.dtype)\n",
      "\n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/270:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/271:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.int32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print(ohe_labels.dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/272:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/273:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.int32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/274:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/275:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.int32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/276:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/277:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/278:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/279:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum(dim=1)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/280:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/281:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) )  \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/282:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/283:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score  ) \n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/284:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/285:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32) \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/286:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/287:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/288:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/289:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum()\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/290:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/291:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum(axis=0)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/292:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/293:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum(dim=1)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/294:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/295:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat ).sum(dim=0)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/296:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/297:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*torch.sum( (ohe_labels*logy_hat ) , dim=1)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/298:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/299:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*torch.sum( (ohe_labels*logy_hat ) , dim=0)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/300:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/301:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*torch.sum( (ohe_labels.reshape(batch_size,-1)*logy_hat.reshape(batch_size,-1) ) , dim=1)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/302:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/303:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels*logy_hat  ).dtype)\n",
      "  \n",
      "  loss += -1*torch.sum( (ohe_labels.reshape(batch_size,-1)*logy_hat.reshape(batch_size,-1) ) , dim=1)\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/304:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ).dtype)\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/305:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/306:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  loss += -1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/307:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/308:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in batch_size: \n",
      "    loss[sample] += torch.t(ohe_labels[sample]).matmul(logy_hat )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/309:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/310:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in batch_size: \n",
      "    loss[sample] += torch.t(ohe_labels[sample]).matmul(logy_hat )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/311:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += torch.t(ohe_labels[sample]).matmul(logy_hat )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/312:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/313:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/314:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/315:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "  ohe_labels=F.one_hot(labels,num_classes=-1) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/316:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/317:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/318:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/319:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/320:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/321:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/322:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/323:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "68/324: # dataset loading, training script and other codes starts here:\n",
      "68/325: # dataset loading, training script and other codes starts here:\n",
      "68/326:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 1\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/327:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([0,1,2,3])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/328:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,4])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/329:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,6])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/330:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "\n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/331:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,6])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/332:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/333:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/334:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/335:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/336:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/337:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/338:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 1\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/339:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "68/340:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "68/341:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,6])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "68/342:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "68/343:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "68/344:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "68/345:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "68/346:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "68/347:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "68/348:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "71/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "71/2:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "71/3:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "        \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "71/4:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "71/5:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "71/6:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "71/7:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "71/8:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    \n",
      "        \n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "71/9:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "print(y)\n",
      "71/10:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "71/11:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "print(y)\n",
      "72/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "72/2: torch.set_default_dtype(torch.float32)\n",
      "72/3:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "72/4:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "72/5:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "72/6:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "72/7:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "72/8:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/9:\n",
      "softmax1d(torch.ones(4,4))\n",
      "#a=torch.eye(4,dtype=torch.float64)\n",
      "#error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(4)\n",
      "a=torch.tensor([1,2,3,6])\n",
      "b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "print(cross_entropy_loss(b,a))\n",
      "72/10:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/11:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float64)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(4)\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/12:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/13:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/14:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/15:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/16:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/17:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/18:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/19:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print(logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/20:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float64)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(4)\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/21:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/22:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/23:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/24:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/25:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/26:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/27:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/28:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/29:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item.data\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "   \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/30:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/31:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/32:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/33:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/34:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/35:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/36:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/37:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/38:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/39:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/40:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/41:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/42:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    for grad_item in grad:\n",
      "        grad_item=torch.add(grad_item,1e-4)\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/43:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/44:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    ## Just in case we have square roots, this should fix it\n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/45:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        ### Might be wrong\n",
      "        net.zero_grad()\n",
      "        grad = torch.autograd.grad(loss, params)\n",
      "        ## Resetting the gradient after using\n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/46:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/47:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/48:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/49:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/50:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/51:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/52:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        ### Might be wrong\n",
      "        net.zero_grad()\n",
      "        #grad = torch.autograd.grad(loss, params)\n",
      "        ## Resetting the gradient after using\n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/53:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/54:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/55:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/56:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/57:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/58:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/59:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        with torch.no_grad():\n",
      "             weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/60:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/61:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/62:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/63:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/64:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/65:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/66:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  with torch.no_grad():\n",
      "        \n",
      "      for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "             weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/67:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  with torch.no_grad():\n",
      "        \n",
      "      for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        weight_item.data -= lr*grad_item\n",
      "        \n",
      "        weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/68:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/69:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  with torch.no_grad():\n",
      "        weights-=lr*grad\n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/70:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/71:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/72:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/73:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/74:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/75:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/76: # dataset loading, training script and other codes starts here:\n",
      "72/77: # dataset loading, training script and other codes starts here:\n",
      "72/78:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(grad)  \n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/79:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/80:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/81:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/82:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/83:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/84:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/85:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "72/86: # dataset loading, training script and other codes starts here:\n",
      "72/87: # dataset loading, training script and other codes starts here:\n",
      "72/88:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "72/89:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  print(type(weights[0]))  \n",
      "  #print(grad)  \n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/90:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/91:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/92:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/93:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/94:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/95:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/96:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print(weights.shape)\n",
      "  print(grad.shape)  \n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/97:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/98:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/99:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/100:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/101:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/102:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/103:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print(len(weights) )\n",
      "  print(grad.shape)  \n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/104:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/105:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/106:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/107:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/108:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/109:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/110:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      "\n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  \n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/111:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/112:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/113:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/114:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/115:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/116:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/117:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    \n",
      "        weights -= lr*grad\n",
      "    \n",
      "    #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/118:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/119:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/120:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/121:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/122:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/123:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/124:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  for iter in len(weights):\n",
      "    \n",
      "      with torch.no_grad():\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/125:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/126:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/127:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/128:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/129:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/130:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/131:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  for i in len(weights):\n",
      "    \n",
      "      with torch.no_grad():\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/132:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/133:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/134:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/135:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/136:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/137:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/138:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  for i in range(len(weights) ):\n",
      "    \n",
      "      with torch.no_grad():\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/139:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/140:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/141:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/142:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/143:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/144:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/145:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/146:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/147:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/148:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/149:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/150:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/151:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/152:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/153:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float64)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(4)\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/154:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/155:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/156:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/157:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/158:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/159:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/160:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/161:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/162:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/163:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/164:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/165:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/166:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/167:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    \n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/168:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/169:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/170:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/171:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/172:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/173:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = softmax1d(o_p7)\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/174:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/175:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/176:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/177:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/178:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/179:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/180:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/181:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/182:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/183:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/184:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/185:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/186:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/187:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/188:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 10\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/189:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 100\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/190:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/191:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float64)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(4)\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/192:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/193:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/194:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/195:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/196:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/197:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/198:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/199:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float64)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(4)\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/200:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/201:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "72/202:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/203:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  print(ohe_labels[sample].shape)\n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/204:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  print(ohe_labels[sample].shape)\n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/205:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "72/206:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "72/207:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/208:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/209:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/210:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/211:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/212:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/213:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/214:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  print(ohe_labels[0].shape)\n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/215:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/216:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "72/217:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "72/218:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  #print(ohe_labels[0].shape)\n",
      "    \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -ohe_labels[sample].reshape(1,-1).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "72/219:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "72/220:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "72/221:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "72/222:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "72/223:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "72/224:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "72/225:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.utils as vutils\n",
      "import numpy as np\n",
      "73/2: torch.set_default_dtype(torch.float32)\n",
      "73/3:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self):\n",
      "    super(LeNet, self).__init__()\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    self.c1 = Conv2D()\n",
      "    self.p2 = nn.AvgPool(2, stride=2)\n",
      "    self.c3 = Conv2D()\n",
      "    self.p4 = nn.AvgPool(2, stride=2)\n",
      "    self.c5 = Linear()\n",
      "    self.f6 = Linear()\n",
      "    \"\"\"\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    scores = self.net(imgs)\n",
      "    o = softmax(scores)\n",
      "    loss = objective(o, labels)\n",
      "    \"\"\"\n",
      "    return loss\n",
      "73/4:\n",
      "class Conv2D(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, kernel_size, stride, padding, device):\n",
      "    super(Conv2D, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      dim_in: integer, number of channels in the input\n",
      "      dim_out: integer, number of channels produced by the convolution\n",
      "      kernel_size: integer list of length 2, spatial size of the convolving kernel\n",
      "      stride: integer list of length 2, stride of the convolution along the the height dimension and width dimension\n",
      "      padding: integers list of length 4, zero-padding added to both sides of the height dimension and width dimension\n",
      "      \n",
      "    \"\"\"\n",
      "    # initialize kernel and bias\n",
      "    self.kernel = nn.Parameter(torch.randn([dim_out, dim_in]+kernel_size, dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    \n",
      "    self.dim_in = dim_in\n",
      "    self.dim_out = dim_out\n",
      "    \n",
      "    self.kernel_size = kernel_size\n",
      "    self.stride = stride\n",
      "    self.padding = padding\n",
      "    self.device = device\n",
      "    \n",
      "  def conv2d_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: input images\n",
      "    outputs:\n",
      "      Y: output produced by the convolution\n",
      "    \"\"\"\n",
      "\n",
      "    ###  Star your code here ###\n",
      "    m=nn.ConstantPad2d( (self.padding),0)\n",
      "    X=m(X)\n",
      "    \n",
      "    Y=F.conv2d(X, self.kernel, self.bias, self.stride)\n",
      "      \n",
      "      \n",
      "      \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "\n",
      "        \n",
      "  def forward(self, x):\n",
      "    return self.conv2d_forward(x)\n",
      "73/5:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(2,1,5,5).float()\n",
      "my_conv = Conv2D(1,2,[3,3],[3,3],[1,1,1,1], torch.device('cpu'))\n",
      "y = my_conv(x)\n",
      "#print(y)\n",
      "73/6:\n",
      "class Linear(nn.Module):\n",
      "  def __init__(self, dim_in, dim_out, device):\n",
      "    super(Linear, self).__init__()\n",
      "    \n",
      "    self.weights = nn.Parameter(torch.randn([dim_out, dim_in], dtype=torch.float32, device=device)*0.1, requires_grad=True)\n",
      "    \n",
      "    self.bias = nn.Parameter(torch.zeros([dim_out], dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.dim_out = dim_out\n",
      "    self.device = device\n",
      "    \n",
      "    ## Device=GPU/CPU ??\n",
      "    \n",
      "  def linear_forward(self, X):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     X: tensor of shape (batch_size, *, dim_in)\n",
      "    outputs:\n",
      "     Y: tensor of shape (batch_size, *, dim_out)\n",
      "    \"\"\"\n",
      "    \n",
      "    ###  Star your code here ### \n",
      "    Y = X.matmul((torch.transpose(self.weights,1,0))) + torch.ones(X.shape[0],1).matmul(torch.transpose((self.bias).reshape(-1,1),1,0))\n",
      "    \n",
      "    #Y=F.linear(X, self.weights, self.bias)\n",
      "    ###  End of the code ###\n",
      "\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.linear_forward(X)\n",
      "73/7:\n",
      "# correctness checking\n",
      "torch.random.manual_seed(0)\n",
      "x = torch.arange(50).view(5, 10).float()\n",
      "my_linear = Linear(10, 3, torch.device('cpu'))\n",
      "y = my_linear(x)\n",
      "#print(y)\n",
      "73/8:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "    \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/9:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "73/10:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "73/11:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "73/12:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "73/13:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "73/14:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/15:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/16:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/17:\n",
      "class Dropout(nn.Module):\n",
      "  def __init__(self, p, device):\n",
      "    super(Dropout, self).__init__()\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "     p: scalar, the probability of an element to be zeroed.\n",
      "    \"\"\"\n",
      "    self.p = p\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def dropout_forward(self, X, training=True):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      X: (N, *), input of dropout layer\n",
      "      training: boolean, apply dropout if true. Note: We do not apply dropout during testing.\n",
      "    outputs:\n",
      "      Y: (N, *)\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return Y\n",
      "  \n",
      "  def forward(self, X):\n",
      "    return self.dropout_forward(X)\n",
      "73/18: # dataset loading, training script and other codes starts here:\n",
      "73/19: # dataset loading, training script and other codes starts here:\n",
      "73/20:\n",
      "class BatchNorm(nn.Module):\n",
      "  def __init__(self, num_features, device, eps=1e-5):\n",
      "    super(BatchNorm, self).__init__()\n",
      "    self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32, device=device), requires_grad=True)\n",
      "    self.moving_mean_gamma = 0.\n",
      "    self.moving_mean_beta = 0.\n",
      "    self.eps = eps\n",
      "    self.num_features = num_features\n",
      "    self.device = device\n",
      "    \n",
      "  def batch_norm_forward(self, x):\n",
      "    \"\"\"\n",
      "    input:\n",
      "      X: (N, *), input of dropout layer, where N is the batch size\n",
      "    output:\n",
      "      Y: (N, *), where N is the batch size\n",
      "    \"\"\"\n",
      "    ### Start the code here  ###\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    \n",
      "    return Y\n",
      "    \n",
      "  def forward(self, inputs):\n",
      "    return self.batch_norm_forward(inputs)\n",
      "73/21:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/22:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/23:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/24:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "73/25:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "73/26:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "73/27:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "73/28:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/29:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/30:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/31:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "73/32:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  #logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  logy_hat=torch.log(pred_score ) \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/33:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "73/34:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "73/35:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "73/36:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "73/37:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "73/38:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/39:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/40:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/41:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-5)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/42:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "73/43:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "73/44:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "73/45:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "73/46:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "73/47:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/48:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/49:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/50:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/51:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "\n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "    \n",
      "  #print(scores)  \n",
      "  scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-8)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/52:\n",
      "# softmax1d(torch.ones(4,4))\n",
      "# # #a=torch.eye(4,dtype=torch.float32)\n",
      "# # #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "# #a=torch.eye(10)\n",
      "# a=torch.tensor(range(10))\n",
      "# # a=torch.tensor([1,2,3,6])\n",
      "# # b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "# b=0.1*torch.ones((10,10))\n",
      "# print(cross_entropy_loss(b,a))\n",
      "73/53:\n",
      "def step(weights, grad, lr):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    weights: list of learnable parameters\n",
      "    grad: list of gradient of the loss w.r.t the learnable parameters\n",
      "    lr: learning rate for gradient descent\n",
      "  outputs:\n",
      "    None. Make sure updating the weights with in-place operation, e.g. tensor.add_(). No output need be returned.\n",
      "  \"\"\"\n",
      "  #torch.add(input, alpha=1, other, out=None)\n",
      "  #out=input+alpha×other\n",
      "\n",
      "  ###  Star your code here ### \n",
      "  # Assuming they are tensors we can do the following\n",
      "\n",
      "#   print(type(weights[0]))\n",
      "#   print(type(grad), type(grad[0]))\n",
      "  #print(type(weights[0]))\n",
      "  print('len_weights',len(weights) )\n",
      " \n",
      "  print('weights_param.shape',weights[0].shape )\n",
      "  \n",
      "  print('len of grad',len(grad))  \n",
      "  print('grad0.shape',grad[0].shape )\n",
      "\n",
      "  with torch.no_grad():\n",
      "    for i in range(len(weights) ):\n",
      "    \n",
      "        weights[i] -= lr*grad[i]\n",
      "    \n",
      "        #for weight_item ,grad_item in zip(weights,grad):\n",
      "        # \n",
      "        #print(type(grad_item) )\n",
      "        #print('In the weight update step loop')\n",
      "        #print(weight_item,grad_item)\n",
      "        \n",
      "        #print(grad)\n",
      "        #print(\"Now printing grad_item_data\")\n",
      "        #print(grad_item.data)\n",
      "        \n",
      "        \n",
      "        #weight_item.data -= lr*grad_item\n",
      "        \n",
      "        #weight_item.copy_(weight_item)\n",
      "        \n",
      "        #weight_item.data = weight_item.data.add_(grad_item.data,-lr)\n",
      "        #grad_item=torch.zeros(grad_item.shape)\n",
      "        \n",
      "        \n",
      "  ## Is the grad a parameter or tensor ?? Not nec:: w1.grad.data\n",
      "  #weights.data -= learning_rate * w1.grad.data\n",
      "\n",
      "  #IF list of tensors, maybe different?  \n",
      "    \n",
      "  ###  End of the code ###\n",
      "73/54:\n",
      "# LeNet sketch code\n",
      "class LeNet(nn.Module):\n",
      "  def __init__(self, img_c, device):\n",
      "    super(LeNet, self).__init__()\n",
      "    self.c1 = Conv2D(img_c, 6, [5,5], [1,1], [2,2,2,2], device)\n",
      "    self.p2 = nn.MaxPool2d(2, stride=2)\n",
      "    self.c3 = Conv2D(6, 16, [5,5], [1,1], [0,0,0,0], device)\n",
      "    self.p4 = nn.MaxPool2d(2, stride=2)\n",
      "    self.f5 = Linear(400, 120, device)\n",
      "    self.f6 = Linear(120, 84, device)\n",
      "    self.f7 = Linear(84, 10, device)\n",
      "    self.device = device\n",
      "    \n",
      "    \n",
      "  def forward(self, imgs, labels):\n",
      "    \"\"\"\n",
      "    inputs:\n",
      "      imgs: (N, C, H, W), training samples from the MNIST training set, where N is the number of samples (batch_size),\n",
      "          C is the image color channle number, H and W are the spatial size of the input images.\n",
      "      labels: (N, L), ground truth for the input images, where N is the number of samples (batch_size) and L is the \n",
      "          number of classes.\n",
      "    outputs:\n",
      "      loss: (1,), mean loss value over this batch of inputs.\n",
      "    \n",
      "    \"\"\"\n",
      "    N = imgs.shape[0]\n",
      "    #print('imgs',imgs)\n",
      "    o_c1 = F.relu(self.c1(imgs))\n",
      "    o_p2 = self.p2(o_c1)\n",
      "    o_c3 = F.relu(self.c3(o_p2))\n",
      "    o_p4 = self.p4(o_c3)\n",
      "    \n",
      "    ### Start the code here  ###\n",
      "    # 1. Please complete the rest of LeNet to get the scores predicted by LeNet for each input images #\n",
      "    \n",
      "    \n",
      "    ## Since there was padding from 4 sides, 4/2/2=1\n",
      "    \n",
      "    ## Need to flatten/reshape?\n",
      "    o_reshape=o_p4.reshape(-1,16*5*5)\n",
      "    o_p5=F.relu( self.f5(o_reshape) )\n",
      "    o_p6=F.relu( self.f6(o_p5) )\n",
      "    o_p7=F.relu( self.f7(o_p6) )\n",
      "    \n",
      "    ## o_p7 is the output of the output layer\n",
      "    p = o_p7\n",
      "    \n",
      "    print('scores_before_softmax',o_p7)\n",
      "    #print(p)\n",
      "    ## Since it's been done here, no need of repeating inside the cross entropy\n",
      "    \n",
      "    p.type(torch.float32)\n",
      "    # 2. Please use the implemented objective function to obtain the losses of each input. #\n",
      "    print('labels',labels)\n",
      "    print('p',p)\n",
      "    \n",
      "    loss = cross_entropy_loss(p, labels)\n",
      "    \n",
      "    print(\"loss\",loss)\n",
      "    print(loss.mean())\n",
      "    ###### doubt ########\n",
      "    ### If the defined cross entropy already sends a mean, what other mean we need ??\n",
      "    #####################\n",
      "    \n",
      "    \n",
      "    # 3. We will return the mean value of the losses. #\n",
      "    \n",
      "    \n",
      "    \n",
      "    ###  End of the code ###\n",
      "    return loss.mean(), p\n",
      "73/55:\n",
      "# A=np.zeros((3,4));A=A.reshape(-1,12);\n",
      "# print(A.shape)\n",
      "73/56:\n",
      "import os\n",
      "import urllib.request\n",
      "\n",
      "data_path = './CS536_MNIST/'\n",
      "if not os.path.exists(data_path):\n",
      "  os.mkdir(data_path)\n",
      "  print(\"Starting downloading MNIST to {}\".format(data_path))\n",
      "  \n",
      "  import urllib\n",
      "  dataset_dict = {\n",
      "        'train_images': \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "        'train_labels': \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "        'test_images': \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "        'test_labels': \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",}\n",
      "\n",
      "  for f, url in dataset_dict.items():\n",
      "    urllib.request.urlretrieve(url, data_path + f)\n",
      "73/57:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/58:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/59:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 2\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/60:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 20\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/61:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "    \n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  scores_new=scores-torch.max(scores)\n",
      "  numerator = torch.exp(scores_new)\n",
      "  denominator = np.sum(numerator)\n",
      "  p = numerator/denominator\n",
      "  #return p \n",
      "    \n",
      "    \n",
      "  #print(scores)  \n",
      "  #scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  #p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  #p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-8)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/62:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/63:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "    \n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  scores_new=scores-torch.max(scores)\n",
      "  numerator = torch.exp(scores_new)\n",
      "  denominator = numerator.sum()\n",
      "  p = numerator/denominator\n",
      "  #return p \n",
      "    \n",
      "    \n",
      "  #print(scores)  \n",
      "  #scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  #p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  #p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log(pred_score + (1e-8)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/64:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/65:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 20\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/66:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-5\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 20\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/67:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "    \n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  scores_new=scores-torch.max(scores)\n",
      "  numerator = torch.exp(scores_new)\n",
      "  denominator = numerator.sum()\n",
      "  p = numerator/denominator\n",
      "  #return p \n",
      "    \n",
      "    \n",
      "  #print(scores)  \n",
      "  #scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  #p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  #p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log( torch.abs(pred_score) + (1e-8)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "\n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      " \n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n",
      "73/68:\n",
      "softmax1d(torch.ones(4,4))\n",
      "# #a=torch.eye(4,dtype=torch.float32)\n",
      "# #error- expected backend CPU and dtype Double but got backend CPU and dtype Float\n",
      "#a=torch.eye(10)\n",
      "a=torch.tensor(range(10))\n",
      "# a=torch.tensor([1,2,3,6])\n",
      "# b=torch.tensor([[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25] ])\n",
      "b=0.1*torch.ones((10,10))\n",
      "print(cross_entropy_loss(b,a))\n",
      "73/69:\n",
      "train_img_file = data_path + 'train_images'\n",
      "train_lb_file = data_path + 'train_labels'\n",
      "test_img_file = data_path + 'test_images'\n",
      "test_lb_file = data_path + 'test_labels'\n",
      "\n",
      "class MNISTDataset(Dataset):\n",
      "  def __init__(self, ds_size=10000, split='training'):\n",
      "    self.split = split\n",
      "    if self.split == 'training':\n",
      "      img_file = train_img_file\n",
      "      lb_file = train_lb_file\n",
      "      n_samples = 60000\n",
      "    else:\n",
      "      img_file = test_img_file\n",
      "      lb_file = test_lb_file\n",
      "      n_samples = 10000\n",
      "    self.ds_size = ds_size\n",
      "      \n",
      "    import gzip\n",
      "    with gzip.open(img_file, 'rb') as f:\n",
      "      imgs = f.read()\n",
      "    imgs = np.frombuffer(imgs[16:], dtype=np.uint8).astype(np.float32)\n",
      "    with gzip.open(lb_file, 'rb') as f:\n",
      "      lb = f.read()\n",
      "    lbs = np.frombuffer(lb[8:], dtype=np.uint8).astype(np.float32)\n",
      "    \n",
      "    imgs = torch.tensor(imgs).view(n_samples, 1, 28, 28) - 125.\n",
      "    lbs = torch.tensor(lbs).long()\n",
      "    \n",
      "    self.imgs = imgs[:ds_size]\n",
      "    self.lbs= lbs[:ds_size]\n",
      "    \n",
      "  def __len__(self):\n",
      "    return self.ds_size\n",
      "  \n",
      "  def __getitem__(self, idx):\n",
      "    return self.imgs[idx], self.lbs[idx]\n",
      "73/70:\n",
      "dataset_size = 1000\n",
      "validation_size = int(0.2 * 1000)\n",
      "\n",
      "ds = MNISTDataset(ds_size=dataset_size)\n",
      "\n",
      "# split the dataset into training set and validation set\n",
      "train_ds, val_ds = torch.utils.data.random_split(ds, [dataset_size - validation_size, validation_size])\n",
      "\n",
      "# training batch size, hyper-parameter\n",
      "batch_size = 24\n",
      "\n",
      "# dataset loader\n",
      "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "img_channel = 1\n",
      "73/71:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-5\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 20\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/72:\n",
      "# learning rate, hyper-parameter\n",
      "lr = 1e-4\n",
      "\n",
      "# using GPU if it's availble\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "net = LeNet(img_channel, device)\n",
      "# keep a list of moel parameters\n",
      "params = [p for (n, p) in net.named_parameters()]\n",
      "\n",
      "# training epochs, hyper-parameter\n",
      "epochs = 20\n",
      "\n",
      "# keep tracking of the changing of loss and accuracy of predictions\n",
      "train_loss_list = []\n",
      "val_loss_list = []\n",
      "train_acc_list = []\n",
      "val_acc_list = []\n",
      "\n",
      "# the printing frequency, feel free to change this\n",
      "print_interval = 50\n",
      "for e in range(epochs):\n",
      "  net.train()\n",
      "  for i, (imgs, lbs) in enumerate(train_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    net.zero_grad()\n",
      "    \n",
      "    grad = torch.autograd.grad(loss, params)\n",
      "    \n",
      "    \n",
      "    # update weights\n",
      "    step(params, grad, lr)\n",
      "    \n",
      "    # obtain the predictions\n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "    if i % print_interval == 0:\n",
      "      print(\"step {}, loss {}\".format(i + e*len(train_dl), loss))\n",
      "      print(\"Target:\\t {}\\nPred:\\t {}\".format(lbs[:8], pred[:8]))\n",
      "      # visualize some samples\n",
      "      imgs_to_vis = vutils.make_grid(imgs[:8].cpu()+125., nrow=8, pad_value=1)\n",
      "      plt.imshow(imgs_to_vis.permute(1,2,0).numpy().astype(np.uint8))\n",
      "      plt.axis(\"off\")\n",
      "      plt.show()\n",
      "      \n",
      "  train_loss_list.append(loss.detach().mean())\n",
      "  train_acc_list.append(acc.detach().mean())\n",
      "  \n",
      "  net.eval()\n",
      "  for i, (imgs, lbs) in enumerate(val_dl):\n",
      "    imgs = imgs.to(device)\n",
      "    lbs = lbs.to(device)\n",
      "    loss, prob = net(imgs, lbs)\n",
      "    \n",
      "    pred = prob.argmax(dim=-1).view(batch_size)\n",
      "    acc = (pred == lbs).float().mean()\n",
      "    \n",
      "  val_loss_list.append(loss.detach().mean())\n",
      "  val_acc_list.append(acc.detach().mean())\n",
      "  print(\"EPOCH_IS_COMPLETE\")    \n",
      "\n",
      "# ploting logs\n",
      "plt.plot(np.arange(epochs), train_loss_list, '-r',\n",
      "         np.arange(epochs), val_loss_list, '-g')\n",
      "plt.legend(('training error', 'validation error'))\n",
      "plt.show()\n",
      "plt.plot(np.arange(epochs), train_acc_list, '-r',\n",
      "         np.arange(epochs), val_acc_list, '-g')\n",
      "plt.legend(('training acc', 'validation acc'))\n",
      "plt.show()\n",
      "73/73:\n",
      "def softmax1d(scores):\n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    scores: (N, C), predicted scores for each input, where N is the number of samples and C is the number of\n",
      "            classes.\n",
      "  outputs:\n",
      "    p: (N, C), probability distribution over classes. Converted from input (scores) with a softmax operation.\n",
      "    \n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  #print(scores.shape)\n",
      "  #print(torch.exp(scores).shape)  \n",
      "  #print(torch.sum(torch.exp(scores),dim=1).shape)\n",
      "  ###  Star your code here ### \n",
      "  print(torch.max(scores))  \n",
      "   \n",
      "    \n",
      "#   ## BE CAREFUL HERE  torch.min\n",
      "#   score_check=torch.zeros_like(scores)  \n",
      "#   score_check+=15\n",
      "# #   ## BE CAREFUL_HERE \n",
      "\n",
      "#   #p= torch.exp(scores) / torch.sum(torch.exp(scores),dim=1)[:,None]\n",
      "#   p = torch.exp(torch.min(scores, score_check ) ) / torch.sum(torch.exp(torch.min(scores,score_check ) ),dim=1)[:,None]\n",
      "  scores_new=scores-torch.max(scores)\n",
      "  numerator = torch.exp(scores_new)\n",
      "  denominator = numerator.sum()\n",
      "  p = numerator/denominator\n",
      "  #return p \n",
      "    \n",
      "    \n",
      "  #print(scores)  \n",
      "  #scores_new=scores-torch.max(scores) \n",
      "  #print(scores_new)  \n",
      "  #p=torch.zeros((scores.shape[0],scores.shape[1]), dtype=torch.float32 )\n",
      "  #p+=torch.exp( scores_new) / torch.sum(torch.exp( scores_new),dim=1)[:,None] \n",
      "\n",
      "  ###  End of the code ###\n",
      "  \n",
      "  return p\n",
      "\n",
      "def cross_entropy_loss(pred_score, labels):\n",
      "    \n",
      "  print(\"pred_score_shape\",pred_score.shape)  \n",
      "  #pred_score = torch.as_tensor(pred_score, dtype=torch.float32)\n",
      "  #labels = torch.as_tensor(labels, dtype=torch.float32) \n",
      "  \"\"\"\n",
      "  inputs:\n",
      "    pred_score: (N, C), probaility distribution or pred_scores over classes, where N is the number of samples and C is the number of\n",
      "      classes.\n",
      "  outputs:\n",
      "    loss: (N,), cross entropy loss for each sample.\n",
      "\n",
      "  Note: Do be careful of the numerial error!\n",
      "  \"\"\"\n",
      "  ###  Star your code here ### \n",
      "    \n",
      "  ohe_labels=F.one_hot(labels,num_classes=10) \n",
      "  ohe_labels=torch.as_tensor(ohe_labels, dtype=torch.float32)\n",
      "  #print(ohe_labels)  \n",
      "  #print('Printing labels inside cross',labels) \n",
      "  \n",
      "  batch_size=torch.tensor(pred_score.shape[0])\n",
      "  \n",
      "  loss=torch.zeros( (batch_size ,1),dtype=torch.float32) \n",
      "   \n",
      "  #print( (ohe_labels*torch.log(pred_score)).dtype )\n",
      "  \n",
      "  #logy_hat=torch.log(pred_score ) \n",
      "   \n",
      "  logy_hat=torch.log( torch.abs(pred_score) + (1e-8)*torch.ones_like(pred_score) ) \n",
      "  \n",
      "\n",
      "  \n",
      "#   logy_hat=torch.log(pred_score ) \n",
      "  \n",
      "  print('logy_yhat',logy_hat)\n",
      "    \n",
      "  #print( (ohe_labels.matmul(torch.t(logy_hat) )  ))\n",
      "  \n",
      "  for sample in range(batch_size): \n",
      "    loss[sample] += -torch.t(ohe_labels[sample]).matmul(logy_hat[sample] )  \n",
      "    #-1*(ohe_labels*logy_hat )\n",
      "    loss[sample]=torch.min(0,loss[sample])\n",
      "  \n",
      "  print('Loss before in the loop',loss)\n",
      " \n",
      "  return loss.mean()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(HNE(W,x1_vec))\n",
      "print(HNE(W,x2_vec))\n",
      "167/125:\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    #print(len(x_as))\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        #a_el=((W.T).dot(x_as)) [ctr] # choose the ith one\n",
      "        \n",
      "        #Routine for this can be more explict\n",
      "        #Taking care of the previous updates\n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        #print(a_el)\n",
      "        # Doing a_i=\\sum_j(W_ij x_j)\n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1\n",
      "            #if el<0:\n",
      "                #print('gonna flip the mf', x_as[ctr])\n",
      "                #print(\"I am here-ctr\", ctr)\n",
      "                #update el by el thus changing original x_start\n",
      "                #print(x_as[ctr])\n",
      "            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "            #print(\"I am here\", ctr)\n",
      "            #x_as[ctr]=-1\n",
      "    #print(np.allclose(old_x_start, x_new))      \n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "#print(x_start_list.shape)\n",
      "#print(x_start_list.reshape(N,I))\n",
      "\n",
      "Exact_match=0\n",
      "min_flips=[]\n",
      "#print(HNE(W,x_start))\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    if np.allclose(x1_vec,x_run_new)==True:\n",
      "       Exact_match+=1\n",
      "\n",
      "        \n",
      "    if np.allclose(x2_vec,x_run_new)==True:\n",
      "       Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_flips.append(np.minimum(mismatch_with_x1,mismatch_with_x2))\n",
      "    \n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "    \n",
      "print(Exact_match)\n",
      "print(min_flips)\n",
      "#print(np.allclose(x_run,x1_vec) )\n",
      "#print(np.allclose(x_run,x2_vec) )\n",
      "plt.plot(range(N),E_runs)\n",
      "167/126:\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    #print(len(x_as))\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        #a_el=((W.T).dot(x_as)) [ctr] # choose the ith one\n",
      "        \n",
      "        #Routine for this can be more explict\n",
      "        #Taking care of the previous updates\n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        #print(a_el)\n",
      "        # Doing a_i=\\sum_j(W_ij x_j)\n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1\n",
      "            #if el<0:\n",
      "                #print('gonna flip the mf', x_as[ctr])\n",
      "                #print(\"I am here-ctr\", ctr)\n",
      "                #update el by el thus changing original x_start\n",
      "                #print(x_as[ctr])\n",
      "            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "            #print(\"I am here\", ctr)\n",
      "            #x_as[ctr]=-1\n",
      "    #print(np.allclose(old_x_start, x_new))      \n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "#print(x_start_list.shape)\n",
      "#print(x_start_list.reshape(N,I))\n",
      "\n",
      "Exact_match=0\n",
      "min_flips=[]\n",
      "#print(HNE(W,x_start))\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    if np.allclose(x1_vec,x_run_new)==True:\n",
      "       Exact_match+=1\n",
      "\n",
      "        \n",
      "    if np.allclose(x2_vec,x_run_new)==True:\n",
      "       Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_flips.append(np.minimum(mismatch_with_x1,mismatch_with_x2))\n",
      "    \n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "    \n",
      "print(Exact_match)\n",
      "print(min_flips)\n",
      "#print(np.allclose(x_run,x1_vec) )\n",
      "#print(np.allclose(x_run,x2_vec) )\n",
      "plt.plot(range(N),E_runs)\n",
      "167/127:\n",
      "import numpy as np\n",
      "import random \n",
      "from matplotlib import pyplot as plt\n",
      "167/128:\n",
      "I=400 # Total #spins\n",
      "\n",
      "np.random.seed(5)\n",
      "x1_vec=np.random.random_sample((I,))\n",
      "x1_vec=[1 if i >=0.5 else -1 for i in x1_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "np.random.seed(7)\n",
      "x2_vec=np.random.random_sample((I,))\n",
      "x2_vec=[1 if i >=0.5 else -1 for i in x2_vec]\n",
      "\n",
      "print(np.allclose(x1_vec,x2_vec) )\n",
      "#print(x2_vec)\n",
      "\n",
      "### Define N=2 distinct memories\n",
      "## Hebbian rule\n",
      "W=np.zeros((I,I))\n",
      "for i in range(I):\n",
      "    for j in range(I):\n",
      "        if i!=j:\n",
      "            W[i,j]= x1_vec[i]*x1_vec[j] + x2_vec[i]*x2_vec[j]\n",
      "print(np.unique(W))\n",
      "print(W)\n",
      "167/129:\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    #print(len(x_as))\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        #a_el=((W.T).dot(x_as)) [ctr] # choose the ith one\n",
      "        \n",
      "        #Routine for this can be more explict\n",
      "        #Taking care of the previous updates\n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        #print(a_el)\n",
      "        # Doing a_i=\\sum_j(W_ij x_j)\n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1\n",
      "            #if el<0:\n",
      "                #print('gonna flip the mf', x_as[ctr])\n",
      "                #print(\"I am here-ctr\", ctr)\n",
      "                #update el by el thus changing original x_start\n",
      "                #print(x_as[ctr])\n",
      "            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "            #print(\"I am here\", ctr)\n",
      "            #x_as[ctr]=-1\n",
      "    #print(np.allclose(old_x_start, x_new))      \n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "#print(x_start_list.shape)\n",
      "#print(x_start_list.reshape(N,I))\n",
      "\n",
      "Exact_match=0\n",
      "min_flips=[]\n",
      "#print(HNE(W,x_start))\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    if np.allclose(x1_vec,x_run_new)==True:\n",
      "        \n",
      "        Exact_match+=1\n",
      "\n",
      "        \n",
      "    if np.allclose(x2_vec,x_run_new)==True:\n",
      "        Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_flips.append(np.minimum(mismatch_with_x1,mismatch_with_x2))\n",
      "    \n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "    \n",
      "print(Exact_match)\n",
      "print(min_flips)\n",
      "#print(np.allclose(x_run,x1_vec) )\n",
      "#print(np.allclose(x_run,x2_vec) )\n",
      "plt.plot(range(N),E_runs)\n",
      "167/130: print(np.sum(min_flips==0) )\n",
      "167/131:\n",
      "min_flips\n",
      "print(np.sum( min_flips==0) )\n",
      "167/132:\n",
      "print(min_flips)\n",
      "print(np.sum( min_flips==0) )\n",
      "167/133:\n",
      "#print(min_flips)\n",
      "print(np.sum( min_flips==0) )\n",
      "167/134:\n",
      "#print(min_flips)\n",
      "print(np.sum( min_flips==np.zeros(I)) )\n",
      "167/135:\n",
      "#print(min_flips)\n",
      "print(np.sum( min_flips==np.zeros(I)) )\n",
      "167/136:\n",
      "#print(min_flips)\n",
      "print(np.sum( np.array(min_flips)==np.zeros(I)) )\n",
      "167/137:\n",
      "#print(min_flips)\n",
      "print(np.sum( np.array(min_flips)==np.array(np.zeros(I)) ) )\n",
      "167/138:\n",
      "#print(min_flips)\n",
      "np.sum( np.array(min_flips)==np.array(np.zeros(I)) )\n",
      "167/139:\n",
      "#print(min_flips)\n",
      "print(np.sum( np.array(min_flips)==np.array(np.zeros(I)) ) )\n",
      "167/140:\n",
      "#print(min_flips)\n",
      "ra=[0,3,4,0]\n",
      "print(np.sum(ra==np.zeros(len(ra)) ) )\n",
      "# print(np.sum( np.array(min_flips)==np.array(np.zeros(I)) ) )\n",
      "167/141:\n",
      "#print(min_flips)\n",
      "ra=[0,3,4,0]\n",
      "print(I)\n",
      "print(np.sum(ra==np.zeros(len(ra)) ) )\n",
      "# print(np.sum( np.array(min_flips)==np.array(np.zeros(I)) ) )\n",
      "167/142:\n",
      "#print(min_flips)\n",
      "# ra=[0,3,4,0]\n",
      "# print(I)\n",
      "#print(np.sum(ra==np.zeros(len(ra)) ) )\n",
      "print(np.sum( min_flips==np.zeros(len(min_flips))) ) )\n",
      "167/143:\n",
      "#print(min_flips)\n",
      "# ra=[0,3,4,0]\n",
      "# print(I)\n",
      "#print(np.sum(ra==np.zeros(len(ra)) ) )\n",
      "print(np.sum( min_flips==np.zeros(len(min_flips))) )\n",
      "167/144:\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    #print(len(x_as))\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        #a_el=((W.T).dot(x_as)) [ctr] # choose the ith one\n",
      "        \n",
      "        #Routine for this can be more explict\n",
      "        #Taking care of the previous updates\n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        #print(a_el)\n",
      "        # Doing a_i=\\sum_j(W_ij x_j)\n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1\n",
      "            #if el<0:\n",
      "                #print('gonna flip the mf', x_as[ctr])\n",
      "                #print(\"I am here-ctr\", ctr)\n",
      "                #update el by el thus changing original x_start\n",
      "                #print(x_as[ctr])\n",
      "            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "            #print(\"I am here\", ctr)\n",
      "            #x_as[ctr]=-1\n",
      "    #print(np.allclose(old_x_start, x_new))      \n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "#print(x_start_list.shape)\n",
      "#print(x_start_list.reshape(N,I))\n",
      "\n",
      "Exact_match=0\n",
      "min_flips=[]\n",
      "#print(HNE(W,x_start))\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    for cyc in range(2):\n",
      "        x_run_new=asynchronous_update(x_run_new)\n",
      "    \n",
      "    if np.allclose(x1_vec,x_run_new)==True:\n",
      "        \n",
      "        Exact_match+=1\n",
      "\n",
      "        \n",
      "    if np.allclose(x2_vec,x_run_new)==True:\n",
      "        Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum( np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_flips.append(np.minimum(mismatch_with_x1, mismatch_with_x2))\n",
      "    \n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "    \n",
      "print(Exact_match)\n",
      "#print(min_flips)\n",
      "#print(np.allclose(x_run,x1_vec) )\n",
      "#print(np.allclose(x_run,x2_vec) )\n",
      "plt.plot(range(N),E_runs)\n",
      "167/145:\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    #print(len(x_as))\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        #a_el=((W.T).dot(x_as)) [ctr] # choose the ith one\n",
      "        \n",
      "        #Routine for this can be more explict\n",
      "        #Taking care of the previous updates\n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        #print(a_el)\n",
      "        # Doing a_i=\\sum_j(W_ij x_j)\n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1\n",
      "            #if el<0:\n",
      "                #print('gonna flip the mf', x_as[ctr])\n",
      "                #print(\"I am here-ctr\", ctr)\n",
      "                #update el by el thus changing original x_start\n",
      "                #print(x_as[ctr])\n",
      "            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "            #print(\"I am here\", ctr)\n",
      "            #x_as[ctr]=-1\n",
      "    #print(np.allclose(old_x_start, x_new))      \n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=400\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "#print(x_start_list.shape)\n",
      "#print(x_start_list.reshape(N,I))\n",
      "\n",
      "Exact_match=0\n",
      "min_flips=[]\n",
      "#print(HNE(W,x_start))\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    for cyc in range(2):\n",
      "        x_run_new=asynchronous_update(x_run_new)\n",
      "    \n",
      "    if np.allclose(x1_vec,x_run_new)==True:\n",
      "        \n",
      "        Exact_match+=1\n",
      "\n",
      "        \n",
      "    if np.allclose(x2_vec,x_run_new)==True:\n",
      "        Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum( np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_flips.append(np.minimum(mismatch_with_x1, mismatch_with_x2))\n",
      "    \n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "    \n",
      "print(Exact_match)\n",
      "#print(min_flips)\n",
      "#print(np.allclose(x_run,x1_vec) )\n",
      "#print(np.allclose(x_run,x2_vec) )\n",
      "plt.plot(range(N),E_runs)\n",
      "167/146:\n",
      "#print(min_flips)\n",
      "# ra=[0,3,4,0]\n",
      "# print(I)\n",
      "#print(np.sum(ra==np.zeros(len(ra)) ) )\n",
      "print(np.sum( min_flips==np.zeros(len(min_flips))) )\n",
      "160/18:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "160/19:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "160/20:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/1:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "171/2:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "171/3:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/4:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "171/5:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "158/33:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\alphas$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "158/34:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/6:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_train_ord,X_test_ord,Y_train_ord,Y_test_ord=train_test_split(X_ordered,Y_ordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_train_cri,X_test_cri,Y_train_cri,Y_test_cri=train_test_split(X_critical,Y_critical,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "X_train_disord,X_test_disord,Y_train_disord,Y_test_disord=train_test_split(X_disordered,Y_disordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "\n",
      "## We used all three of the datasets rather than just 2 and concatenate \n",
      "## in the end \n",
      "\n",
      "X_train=np.concatenate((X_train_ord,X_train_disord))\n",
      "Y_train=np.concatenate((Y_train_ord,Y_train_disord))\n",
      "\n",
      "\n",
      "X_test=np.concatenate((X_test_ord,X_test_disord, X_test_cri,  X_train_cri))\n",
      "Y_test=np.concatenate((Y_test_ord,Y_test_disord, Y_test_cri,  Y_train_cri))\n",
      "\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "# X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "# X=np.concatenate((X_critical,X))\n",
      "# Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "171/7:\n",
      "##### plot a few Ising states\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "# set colourbar map\n",
      "cmap_args=dict(cmap='plasma_r')\n",
      "\n",
      "# plot states\n",
      "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
      "\n",
      "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
      "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
      "axarr[0].tick_params(labelsize=16)\n",
      "\n",
      "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
      "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
      "axarr[1].tick_params(labelsize=16)\n",
      "\n",
      "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
      "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
      "axarr[2].tick_params(labelsize=16)\n",
      "\n",
      "fig.subplots_adjust(right=2.0)\n",
      "\n",
      "plt.show()\n",
      "171/8:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    #critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/9:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/10:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "171/11:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "171/12:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_train_ord,X_test_ord,Y_train_ord,Y_test_ord=train_test_split(X_ordered,Y_ordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_train_cri,X_test_cri,Y_train_cri,Y_test_cri=train_test_split(X_critical,Y_critical,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "X_train_disord,X_test_disord,Y_train_disord,Y_test_disord=train_test_split(X_disordered,Y_disordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "\n",
      "## We used all three of the datasets rather than just 2 and concatenate \n",
      "## in the end \n",
      "\n",
      "X_train=np.concatenate((X_train_ord,X_train_disord))\n",
      "Y_train=np.concatenate((Y_train_ord,Y_train_disord))\n",
      "\n",
      "\n",
      "X_test= X_train_cri\n",
      "Y_test= Y_train_cri\n",
      "\n",
      "# X_test=np.concatenate((X_test_ord,X_test_disord, X_test_cri,  X_train_cri))\n",
      "# Y_test=np.concatenate((Y_test_ord,Y_test_disord, Y_test_cri,  Y_train_cri))\n",
      "\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "# X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "# X=np.concatenate((X_critical,X))\n",
      "# Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "171/13:\n",
      "##### plot a few Ising states\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "# set colourbar map\n",
      "cmap_args=dict(cmap='plasma_r')\n",
      "\n",
      "# plot states\n",
      "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
      "\n",
      "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
      "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
      "axarr[0].tick_params(labelsize=16)\n",
      "\n",
      "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
      "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
      "axarr[1].tick_params(labelsize=16)\n",
      "\n",
      "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
      "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
      "axarr[2].tick_params(labelsize=16)\n",
      "\n",
      "fig.subplots_adjust(right=2.0)\n",
      "\n",
      "plt.show()\n",
      "171/14:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    #critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/15:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/16:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_train_ord,X_test_ord,Y_train_ord,Y_test_ord=train_test_split(X_ordered,Y_ordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_train_cri,X_test_cri,Y_train_cri,Y_test_cri=train_test_split(X_critical,Y_critical,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "X_train_disord,X_test_disord,Y_train_disord,Y_test_disord=train_test_split(X_disordered,Y_disordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "\n",
      "## We used all three of the datasets rather than just 2 and concatenate \n",
      "## in the end \n",
      "\n",
      "X_train=np.concatenate((X_train_ord,X_train_disord))\n",
      "Y_train=np.concatenate((Y_train_ord,Y_train_disord))\n",
      "\n",
      "\n",
      "# X_test= X_train_cri\n",
      "# Y_test= Y_train_cri\n",
      "\n",
      "X_test=np.concatenate((X_test_cri,  X_train_cri))\n",
      "Y_test=np.concatenate(( Y_test_cri,  Y_train_cri))\n",
      "\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "# X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "# X=np.concatenate((X_critical,X))\n",
      "# Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "172/1:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "172/2:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "172/3:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_train_ord,X_test_ord,Y_train_ord,Y_test_ord=train_test_split(X_ordered,Y_ordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_train_cri,X_test_cri,Y_train_cri,Y_test_cri=train_test_split(X_critical,Y_critical,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "X_train_disord,X_test_disord,Y_train_disord,Y_test_disord=train_test_split(X_disordered,Y_disordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "\n",
      "## We used all three of the datasets rather than just 2 and concatenate \n",
      "## in the end \n",
      "\n",
      "X_train=np.concatenate((X_train_ord,X_train_disord))\n",
      "Y_train=np.concatenate((Y_train_ord,Y_train_disord))\n",
      "\n",
      "\n",
      "# X_test= X_train_cri\n",
      "# Y_test= Y_train_cri\n",
      "\n",
      "X_test=np.concatenate((X_test_cri,  X_train_cri))\n",
      "Y_test=np.concatenate(( Y_test_cri,  Y_train_cri))\n",
      "\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "# X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "# X=np.concatenate((X_critical,X))\n",
      "# Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "172/4:\n",
      "##### plot a few Ising states\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "# set colourbar map\n",
      "cmap_args=dict(cmap='plasma_r')\n",
      "\n",
      "# plot states\n",
      "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
      "\n",
      "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
      "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
      "axarr[0].tick_params(labelsize=16)\n",
      "\n",
      "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
      "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
      "axarr[1].tick_params(labelsize=16)\n",
      "\n",
      "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
      "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
      "axarr[2].tick_params(labelsize=16)\n",
      "\n",
      "fig.subplots_adjust(right=2.0)\n",
      "\n",
      "plt.show()\n",
      "172/5:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    #logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    #test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "172/6:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    #logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "    #                                       shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    #test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "172/7:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "    #                                       shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    #test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "172/8:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    #test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "172/9:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    #train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    #test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    #critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    #test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "172/10:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "#plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' test_critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "173/1:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "#plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' test_critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "173/2:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "173/3:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "173/4:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "X=np.concatenate((X_ordered,X_disordered))\n",
      "Y=np.concatenate((Y_ordered,Y_disordered))\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "X=np.concatenate((X_critical,X))\n",
      "Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "173/5:\n",
      "##### plot a few Ising states\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "# set colourbar map\n",
      "cmap_args=dict(cmap='plasma_r')\n",
      "\n",
      "# plot states\n",
      "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
      "\n",
      "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
      "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
      "axarr[0].tick_params(labelsize=16)\n",
      "\n",
      "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
      "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
      "axarr[1].tick_params(labelsize=16)\n",
      "\n",
      "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
      "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
      "axarr[2].tick_params(labelsize=16)\n",
      "\n",
      "fig.subplots_adjust(right=2.0)\n",
      "\n",
      "plt.show()\n",
      "173/6:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "    logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "                                           solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "    print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\n",
      "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "173/7:\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'o--b',label='train')\n",
      "#plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' test_critical')\n",
      "\n",
      "plt.xlabel('$\\\\alpha$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "#plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "167/147:\n",
      "#min_flips\n",
      "sns.distplot(min_flips_N6,bins=30,kde=False,rug=True );\n",
      "167/148:\n",
      "\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W2[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W2[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "\n",
      "Exact_match=0\n",
      "min_flips_N6=[]\n",
      "\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    #if np.allclose(x1_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "        \n",
      "    #if np.allclose(x2_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x3=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x4=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x5=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x6=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_1=np.amin([mismatch_with_x1,mismatch_with_x2, mismatch_with_x3])\n",
      "    min_2=np.amin([min_1,mismatch_with_x4,mismatch_with_x5,mismatch_with_x6])\n",
      "    min_flips_N6.append(min_2)            \n",
      "    #. \\ for newline\n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W2,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "167/149:\n",
      "\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W2[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W2[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "\n",
      "Exact_match=0\n",
      "min_flips_N6=[]\n",
      "\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    #if np.allclose(x1_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "        \n",
      "    #if np.allclose(x2_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x3=I-np.sum(np.array(x3_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x4=I-np.sum(np.array(x4_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x5=I-np.sum(np.array(x5_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x6=I-np.sum(np.array(x6_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_1=np.amin([mismatch_with_x1,mismatch_with_x2, mismatch_with_x3])\n",
      "    min_2=np.amin([min_1,mismatch_with_x4,mismatch_with_x5,mismatch_with_x6])\n",
      "    min_flips_N6.append(min_2)            \n",
      "    #. \\ for newline\n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W2,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "167/150:\n",
      "#min_flips\n",
      "sns.distplot(min_flips_N6,bins=30,kde=False,rug=True );\n",
      "167/151:\n",
      "#### With 6 memories\n",
      "\n",
      "I=400 # Total #spins\n",
      "#np.random.seed(10)\n",
      "x1_vec=np.random.random_sample((I,))\n",
      "x1_vec=[1 if i >=0.5 else -1 for i in x1_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x2_vec=np.random.random_sample((I,))\n",
      "x2_vec=[1 if i >=0.5 else -1 for i in x2_vec]\n",
      "\n",
      "x3_vec=np.random.random_sample((I,))\n",
      "x3_vec=[1 if i >=0.5 else -1 for i in x3_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x4_vec=np.random.random_sample((I,))\n",
      "x4_vec=[1 if i >=0.5 else -1 for i in x4_vec]\n",
      "\n",
      "\n",
      "x5_vec=np.random.random_sample((I,))\n",
      "\n",
      "x5_vec=[1 if i >=0.5 else -1 for i in x5_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x6_vec=np.random.random_sample((I,))\n",
      "x6_vec=[1 if i >=0.5 else -1 for i in x6_vec]\n",
      "\n",
      "print(np.allclose(x1_vec,x2_vec) )\n",
      "#print(x2_vec)\n",
      "\n",
      "### Define N=2 distinct memories\n",
      "## Hebbian rule\n",
      "W2=np.zeros((I,I))\n",
      "for i in range(I):\n",
      "    for j in range(I):\n",
      "        if i!=j:\n",
      "            W2[i,j]= x1_vec[i]*x1_vec[j] + x2_vec[i]*x2_vec[j]\n",
      "            \n",
      "            W2[i,j]+= x2_vec[i]*x2_vec[j]+ x3_vec[i]*x3_vec[j]\n",
      "            \n",
      "            W2[i,j]+= x4_vec[i]*x4_vec[j]+ x5_vec[i]*x5_vec[j]\n",
      "            \n",
      "print(np.unique(W2))\n",
      "print(W2)\n",
      "167/152:\n",
      "\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W2[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W2[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "\n",
      "Exact_match=0\n",
      "min_flips_N6=[]\n",
      "\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    #if np.allclose(x1_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "        \n",
      "    #if np.allclose(x2_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x3=I-np.sum(np.array(x3_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x4=I-np.sum(np.array(x4_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x5=I-np.sum(np.array(x5_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x6=I-np.sum(np.array(x6_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_1=np.amin([mismatch_with_x1,mismatch_with_x2, mismatch_with_x3])\n",
      "    min_2=np.amin([min_1,mismatch_with_x4,mismatch_with_x5,mismatch_with_x6])\n",
      "    min_flips_N6.append(min_2)            \n",
      "    #. \\ for newline\n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W2,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "167/153:\n",
      "#### With 6 memories\n",
      "\n",
      "I=400 # Total #spins\n",
      "#np.random.seed(10)\n",
      "x1_vec=np.random.random_sample((I,))\n",
      "x1_vec=[1 if i >=0.5 else -1 for i in x1_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x2_vec=np.random.random_sample((I,))\n",
      "x2_vec=[1 if i >=0.5 else -1 for i in x2_vec]\n",
      "\n",
      "x3_vec=np.random.random_sample((I,))\n",
      "x3_vec=[1 if i >=0.5 else -1 for i in x3_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x4_vec=np.random.random_sample((I,))\n",
      "x4_vec=[1 if i >=0.5 else -1 for i in x4_vec]\n",
      "\n",
      "\n",
      "x5_vec=np.random.random_sample((I,))\n",
      "\n",
      "x5_vec=[1 if i >=0.5 else -1 for i in x5_vec]\n",
      "#print(x1_vec)\n",
      "\n",
      "#np.random.seed(5)\n",
      "x6_vec=np.random.random_sample((I,))\n",
      "x6_vec=[1 if i >=0.5 else -1 for i in x6_vec]\n",
      "\n",
      "print(np.allclose(x1_vec,x2_vec) )\n",
      "#print(x2_vec)\n",
      "\n",
      "### Define N=2 distinct memories\n",
      "## Hebbian rule\n",
      "W2=np.zeros((I,I))\n",
      "for i in range(I):\n",
      "    for j in range(I):\n",
      "        if i!=j:\n",
      "            W2[i,j]= x1_vec[i]*x1_vec[j] + x2_vec[i]*x2_vec[j]\n",
      "            \n",
      "            W2[i,j]+= x2_vec[i]*x2_vec[j]+ x3_vec[i]*x3_vec[j]\n",
      "            \n",
      "            W2[i,j]+= x4_vec[i]*x4_vec[j]+ x5_vec[i]*x5_vec[j]\n",
      " \n",
      "            W2[i,j]+= x6_vec[i]*x6_vec[j]\n",
      " \n",
      "            \n",
      "print(np.unique(W2))\n",
      "print(W2)\n",
      "167/154:\n",
      "\n",
      "def asynchronous_update(x_as):\n",
      "    old_x_start=x_as\n",
      "    x_new=[]\n",
      "    for ctr in range(len(x_as)):\n",
      "        el=x_as[ctr]\n",
      "        \n",
      "        a_el=0\n",
      "        j=0\n",
      "        while j<ctr:\n",
      "            a_el+=W2[ctr,j]*x_new[j]\n",
      "            j+=1\n",
      "        while j>=ctr and j<len(x_as):\n",
      "            a_el+=W2[ctr,j]*x_as[j]\n",
      "            j+=1\n",
      "        \n",
      "        flip_count=0\n",
      "        if a_el>=0:\n",
      "            #print(el)\n",
      "            x_new.append(1)\n",
      "            flip_count+=1            \n",
      "        if a_el<0:\n",
      "            x_new.append(-1)\n",
      "    return x_new\n",
      "\n",
      "#c;arification to ask what is meanth by N=1000 runs\n",
      "E_runs=[]\n",
      "N=1000\n",
      "\n",
      "x_start_list=np.random.random((N,I))\n",
      "x_start_list.reshape(N,I)\n",
      "\n",
      "Exact_match=0\n",
      "min_flips_N6=[]\n",
      "\n",
      "for runs in range(N):\n",
      "    #print()\n",
      "    x_start=x_start_list[runs]\n",
      "    x_start=[1 if i >=0.5 else -1 for i in x_start_list[runs]]\n",
      "    #print(np.allclose(x_start, x1_vec ))\n",
      "    #print(x_start)\n",
      "    \n",
      "    x_run=x_start\n",
      "    \n",
      "    x_run_new=asynchronous_update(x_run)\n",
      "    \n",
      "    #if np.allclose(x1_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "        \n",
      "    #if np.allclose(x2_vec,x_run_new)==True:\n",
      "    #    Exact_match+=1\n",
      "\n",
      "    mismatch_with_x1=I-np.sum(np.array(x1_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x2=I-np.sum(np.array(x2_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x3=I-np.sum(np.array(x3_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x4=I-np.sum(np.array(x4_vec)==np.array(x_run_new) )\n",
      "\n",
      "    \n",
      "    mismatch_with_x5=I-np.sum(np.array(x5_vec)==np.array(x_run_new) )\n",
      "\n",
      "    mismatch_with_x6=I-np.sum(np.array(x6_vec)==np.array(x_run_new) )\n",
      "\n",
      "    min_1=np.amin([mismatch_with_x1,mismatch_with_x2, mismatch_with_x3])\n",
      "    min_2=np.amin([min_1,mismatch_with_x4,mismatch_with_x5,mismatch_with_x6])\n",
      "    min_flips_N6.append(min_2)            \n",
      "    #. \\ for newline\n",
      "    #print(HNE(W,x_run_new))\n",
      "    #print(x_run)\n",
      "    \n",
      "    E_runs.append(HNE(W2,x_run_new))\n",
      "    #print(np.allclose(x_start, x_run_new ))\n",
      "    X_run=x_run_new\n",
      "167/155:\n",
      "#min_flips\n",
      "sns.distplot(min_flips_N6,bins=30,kde=False,rug=True );\n",
      "175/1:\n",
      "# see what you get when you run\n",
      "# test_1.day, test_1.month, or test_1.year\n",
      "test_1.day\n",
      "175/2:\n",
      "# let's create a datetime\n",
      "test_1 = datetime(1992,7,27)\n",
      "175/3:\n",
      "# see what you get when you run\n",
      "# test_1.day, test_1.month, or test_1.year\n",
      "test_1.day\n",
      "175/4:\n",
      "# here we import the datetime class\n",
      "from datetime import datetime\n",
      "175/5:\n",
      "# let's create a datetime\n",
      "test_1 = datetime(1992,7,27)\n",
      "175/6:\n",
      "# see what you get when you run\n",
      "# test_1.day, test_1.month, or test_1.year\n",
      "test_1.day\n",
      "175/7:\n",
      "# What about test_1.hour, test_1.minute, \n",
      "# test_1.second, test_1.microsecond?\n",
      "\n",
      "test_1.hour\n",
      "175/8:\n",
      "# What about test_1.hour, test_1.minute, \n",
      "# test_1.second, test_1.microsecond?\n",
      "\n",
      "test_1.hour\n",
      "test_1.second\n",
      "175/9:\n",
      "# You can find the current day, or exact time using\n",
      "# datetime.today() and datetime.now()\n",
      "\n",
      "datetime.today()\n",
      "175/10:\n",
      "# You can find the current day, or exact time using\n",
      "# datetime.today() and datetime.now()\n",
      "\n",
      "datetime.today()\n",
      "datetime.now()\n",
      "175/11: import numpy as np\n",
      "175/12:\n",
      "# numpy datetime64s are created from strings\n",
      "# again the standard is year-month-day\n",
      "# followed by hour-minute-second\n",
      "# note months must be in double digit format\n",
      "test_2 = np.datetime64(\"1992-07-27 10:24:36\")\n",
      "175/13: test_2\n",
      "175/14:\n",
      "# numpy datetime64s are created from strings\n",
      "# again the standard is year-month-day\n",
      "# followed by hour-minute-second\n",
      "# note months must be in double digit format\n",
      "test_2 = np.datetime64(\"1992-07-27 10:24:36\")\n",
      "175/15: test_2\n",
      "175/16:\n",
      "# You can't access day, month etc\n",
      "# as with datetime objects, but\n",
      "# you can restrict to the level of time that\n",
      "# you want like so\n",
      "np.array(test_2, dtype=\"datetime64[D]\")\n",
      "175/17:\n",
      "# numpy datetime64s are created from strings\n",
      "# again the standard is year-month-day\n",
      "# followed by hour-minute-second\n",
      "# note months must be in double digit format\n",
      "test_2 = np.datetime64(\"1992-07-27 10:24:36\")\n",
      "175/18: test_2\n",
      "175/19:\n",
      "# You can't access day, month etc\n",
      "# as with datetime objects, but\n",
      "# you can restrict to the level of time that\n",
      "# you want like so\n",
      "np.array(test_2, dtype=\"datetime64[D]\")\n",
      "175/20: test_2\n",
      "175/21:\n",
      "# You can't access day, month etc\n",
      "# as with datetime objects, but\n",
      "# you can restrict to the level of time that\n",
      "# you want like so\n",
      "np.array(test_2, dtype=\"datetime64[D]\")\n",
      "175/22:\n",
      "# try the getting the year using Y\n",
      "# try getting the month using M\n",
      "# try getting the hour using h\n",
      "\n",
      "np.array(test_2, dtype=\"datetime64[Y]\")\n",
      "175/23:\n",
      "# try the getting the year using Y\n",
      "# try getting the month using M\n",
      "# try getting the hour using h\n",
      "\n",
      "np.array(test_2, dtype=\"datetime64[Y]\")\n",
      "175/24: import pandas as pd\n",
      "175/25: import pandas as pd\n",
      "175/26:\n",
      "# try the getting the year using Y\n",
      "# try getting the month using M\n",
      "# try getting the hour using h\n",
      "\n",
      "np.array(test_2, dtype=\"datetime64[Y]\")\n",
      "\n",
      "test_2.day()\n",
      "175/27:\n",
      "# try the getting the year using Y\n",
      "# try getting the month using M\n",
      "# try getting the hour using h\n",
      "\n",
      "np.array(test_2, dtype=\"datetime64[Y]\")\n",
      "175/28:\n",
      "# you can get an np array of dates at\n",
      "# fixed intervals like so\n",
      "np.arange(np.datetime64('2018-07-27'),\n",
      "         np.datetime64('2020-07-28'),\n",
      "         dtype=\"datetime64[M]\")\n",
      "175/29: import pandas as pd\n",
      "175/30:\n",
      "# you can make it work like np.datetime64\n",
      "pd.Timestamp(\"1992-7-27\")\n",
      "175/31:\n",
      "# or like datetime\n",
      "pd.Timestamp(1992,7,27)\n",
      "175/32:\n",
      "# you can even make an array of datetimes\n",
      "# using to_datetime\n",
      "pd.to_datetime([str(year) + \"-7-27\" for year in range(1992,2020)])\n",
      "175/33:\n",
      "# make a pd timestamp of your birthday\n",
      "pd.to_datetime([str(year) + \"-7-27\" if year==1996])\n",
      "175/34:\n",
      "# make a pd timestamp of your birthday\n",
      "pd.to_datetime([str(year) + \"-7-27\" for year in range(1995,1996)])\n",
      "175/35:\n",
      "# make a pd timestamp of your birthday\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,1996)])\n",
      "175/36:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,today.year)])\n",
      "175/37:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,pd.today.year)])\n",
      "175/38:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,datetime.today()[0])])\n",
      "175/39:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,np.array(datetime.today())[0])])\n",
      "175/40:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,np.array(datetime.today()))])\n",
      "175/41:\n",
      "# make an array of timestamps of your birthday\n",
      "# every year since you were born\n",
      "pd.to_datetime([str(year) + \"-11-23\" for year in range(1995,2020)])\n",
      "175/42:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(\"whitegrid\")\n",
      "175/43:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style(\"whitegrid\")\n",
      "175/44:\n",
      "measles = pd.read_csv(\"measles.csv\")\n",
      "\n",
      "# pandas reads the dates in as string by default\n",
      "# we have to make them in the right format\n",
      "measles['month'] = pd.to_datetime(measles['month'])\n",
      "175/45:\n",
      "# this data collects the number of measles \n",
      "# cases in London each month from Jan 1928\n",
      "# to June 1972\n",
      "measles.head(10)\n",
      "175/46:\n",
      "# matplotlib does a good job handling\n",
      "# datetime data\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.plot(measles.month, measles.cases)\n",
      "\n",
      "plt.xlabel(\"Date\", fontsize=16)\n",
      "plt.ylabel(\"Cases of Measles \\n in London\", fontsize=16)\n",
      "\n",
      "case_1963 = measles.loc[measles.month == pd.Timestamp(1963,1,1),'cases'].values[0]\n",
      "plt.text(pd.Timestamp(1963,1,1), case_1963+15000,\n",
      "         \"Measles Vaccine \\n Developed\", fontsize=16)\n",
      "\n",
      "plt.arrow(pd.Timestamp(1963,1,1), case_1963+15000,\n",
      "          0,-15000, width=50, shape='left', color=\"black\")\n",
      "\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "175/47:\n",
      "# Now you try by plotting this data\n",
      "ibm = pd.read_csv(\"ibm.csv\")\n",
      "175/48:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "175/49:\n",
      "# Now you try by plotting this data\n",
      "ibm = pd.read_csv(\"ibm.csv\")\n",
      "175/50:\n",
      "# Now you try by plotting this data\n",
      "ibm = pd.read_csv(\"ibm.csv\")\n",
      "ibm\n",
      "175/51:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "measles['days'] = pd.to_datetime(measles['days'])\n",
      "175/52:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "measles['days'] = pd.to_datetime(measles[0])\n",
      "175/53:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "measles['days'] = pd.to_datetime(measles[0])\n",
      "175/54:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "measles['month'] = pd.to_datetime(measles['month'])\n",
      "175/55:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm[] = pd.to_datetime(ibm[])\n",
      "175/56:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm[0] = pd.to_datetime(ibm[])\n",
      "175/57:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/58:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm[0]\n",
      "# ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/59:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm.head()[0]\n",
      "# ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/60:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm.index()\n",
      "# ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/61:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "# ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/62:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm.iloc[0]\n",
      "for i in \n",
      "# ibm[0] = pd.to_datetime(ibm[0])\n",
      "175/63:\n",
      "# Notice we don't have dates, but the data is daily\n",
      "# you can just use the index to count the days\n",
      "# chronologically\n",
      "ibm.head()\n",
      "ibm.iloc[0]\n",
      "176/1:\n",
      "# import the packages we'll use\n",
      "from datetime import datetime\n",
      "\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "176/2:\n",
      "# read in ausbeer\n",
      "# adding in parse_dates, saves us a line of coding\n",
      "ausbeer = pd.read_csv(\"ausbeer.csv\")\n",
      "\n",
      "plt.plot(ausbeer)\n",
      "\n",
      "plt.show()\n",
      "176/3:\n",
      "ausbeer_train = ausbeer.iloc[:-15,].copy()\n",
      "ausbeer_test = ausbeer.drop(ausbeer_train.index).copy()\n",
      "176/4:\n",
      "# This function makes a lag_df \n",
      "# for easy plotting\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "176/5:\n",
      "# A function to make our plotting easier\n",
      "def plot_lag(df,feature,lag,ax=None):\n",
      "    df = make_lag_df(df,feature,lag).dropna()\n",
      "    if ax:\n",
      "        ax.scatter(df[feature+'_lag'],df[feature])\n",
      "\n",
      "        ax.plot(np.linspace(350,550,10),\n",
      "                np.linspace(350,550,10),\n",
      "               'k--', alpha = .6)\n",
      "\n",
      "        ax.set_title(\"lag = \" + str(lag),fontsize=12)\n",
      "    else:\n",
      "        plt.scatter(df[feature+'_lag'],df[feature])\n",
      "\n",
      "        plt.plot(np.linspace(350,550,10),\n",
      "                np.linspace(350,550,10),\n",
      "               'k--', alpha = .6)\n",
      "\n",
      "        plt.show()\n",
      "176/6:\n",
      "# we'll make lag plot for lag = 1 to 9\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(ausbeer_train,'production',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/7:\n",
      "def get_autocorr(df,feature,lag):\n",
      "    df = make_lag_df(df,feature,lag)\n",
      "    mean_y = df[feature].mean()\n",
      "    \n",
      "    \n",
      "    \n",
      "    y_ts = df[feature].values\n",
      "    y_lags = df.dropna()[feature + '_lag'].values\n",
      "    \n",
      "    numerator = np.sum((y_ts[lag:] - mean_y)*(y_lags - mean_y))\n",
      "    denom = np.sum(np.power(y_ts - mean_y,2))\n",
      "    \n",
      "    return numerator/denom\n",
      "176/8:\n",
      "# for lag = 1 to 9\n",
      "lags = np.arange(1,9,1)\n",
      "\n",
      "print(lags)\n",
      "print([get_autocorr(ausbeer_train,'production',lag) for lag in lags])\n",
      "176/9:\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(ausbeer_train,'production',lag) for lag in np.arange(1,20,1)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(ausbeer_train,'production',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/10:\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "plt.plot(np.linspace(0,4*np.pi,100),\n",
      "            np.cos(np.linspace(0,4*np.pi,100)),\n",
      "            'r', label=\"$\\cos$\")\n",
      "\n",
      "plt.plot(np.linspace(0,4*np.pi,100),\n",
      "            np.cos(np.pi + np.linspace(0,4*np.pi,100)),\n",
      "            'b', label=\"$\\cos$ shifted by $\\pi$\")\n",
      "\n",
      "plt.xlabel(\"$t$\",fontsize=16)\n",
      "plt.ylabel(\"$y_t$\",fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(fontsize=12)\n",
      "plt.show()\n",
      "176/11:\n",
      "## Code here\n",
      "measles = pd.read_csv(\"measles.csv\", parse_dates = ['month'])\n",
      "\n",
      "measles_copy = measles.loc[measles.month < datetime(1963,1,1),].copy()\n",
      "measles_train = measles_copy.loc[measles.month < datetime(1959,1,1),]\n",
      "measles_test = measles_copy.drop(measles_train.index)\n",
      "176/12:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(ausbeer_train,'production',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/13:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,'production',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/14:\n",
      "## Code here\n",
      "measles = pd.read_csv(\"measles.csv\", parse_dates = ['month'])\n",
      "\n",
      "measles_copy = measles.loc[measles.month < datetime(1963,1,1),].copy()\n",
      "measles_train = measles_copy.loc[measles.month < datetime(1959,1,1),]\n",
      "measles_test = measles_copy.drop(measles_train.index)\n",
      "176/15:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,'production',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/16:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,'production',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/17:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,'Cases of Measles \\n in London',i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/18:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,\"Cases of Measles \\n in London\",i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/19:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,\"Cases\",i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/20:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,\"cases\",i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "176/21:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(ausbeer_train,'production',lag) for lag in np.arange(1,20,1)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(ausbeer_train,'production',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/22:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,20,1)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/23:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,\"cases\",i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "def get_autocorr(df,feature,lag):\n",
      "    df = make_lag_df(df,feature,lag)\n",
      "    mean_y = df[feature].mean()\n",
      "    \n",
      "    \n",
      "    \n",
      "    y_ts = df[feature].values\n",
      "    y_lags = df.dropna()[feature + '_lag'].values\n",
      "    \n",
      "    numerator = np.sum((y_ts[lag:] - mean_y)*(y_lags - mean_y))\n",
      "    denom = np.sum(np.power(y_ts - mean_y,2))\n",
      "    \n",
      "    return numerator/denom\n",
      "\n",
      "# for lag = 1 to 9\n",
      "lags = np.arange(1,72,8)\n",
      "\n",
      "print(lags)\n",
      "print([get_autocorr(ausbeer_train,'production',lag) for lag in lags])\n",
      "176/24:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,20,1)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/25:\n",
      "## Code here\n",
      "\n",
      "\n",
      "def make_lag_df(df,feature,lag):\n",
      "    lag_df = df.copy()\n",
      "    lag_df[feature + '_lag'] = np.nan\n",
      "    \n",
      "    lag_df.loc[lag:,feature + '_lag'] = lag_df.loc[0:len(lag_df)-(lag+1),feature].values\n",
      "    return lag_df\n",
      "\n",
      "\n",
      "fig,ax = plt.subplots(3,3,\n",
      "                      figsize = (12,12))\n",
      "\n",
      "for i in range(1,10):\n",
      "    plot_lag(measles_train,\"cases\",i,ax[(i-1)//3, (i-1)%3])\n",
      "\n",
      "    \n",
      "fig.text(0.5, 0.04, '$y_{t-k}$', ha='center',fontsize=18)\n",
      "fig.text(0.04, 0.5, '$y_t$', va='center', rotation='vertical',fontsize=18)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "def get_autocorr(df,feature,lag):\n",
      "    df = make_lag_df(df,feature,lag)\n",
      "    mean_y = df[feature].mean()\n",
      "    \n",
      "    \n",
      "    \n",
      "    y_ts = df[feature].values\n",
      "    y_lags = df.dropna()[feature + '_lag'].values\n",
      "    \n",
      "    numerator = np.sum((y_ts[lag:] - mean_y)*(y_lags - mean_y))\n",
      "    denom = np.sum(np.power(y_ts - mean_y,2))\n",
      "    \n",
      "    return numerator/denom\n",
      "\n",
      "# for lag = 1 to 9\n",
      "lags = np.arange(1,72,8)\n",
      "\n",
      "print(lags)\n",
      "print([get_autocorr(measles_train,'cases',lag) for lag in lags])\n",
      "176/26:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,20,1)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/27:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,72,8)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,20,1):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "177/1:\n",
      "# import the packages we'll use\n",
      "from datetime import datetime\n",
      "\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "177/2: marathon = pd.read_csv(\"marathon.csv\", parse_dates = ['year'])\n",
      "176/28:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,72,8)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,72,8):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,20,1), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/29:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,20,1), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,72,8)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,72,8):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,72,8), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "176/30:\n",
      "## Code here\n",
      "max_lag = 72\n",
      "\n",
      "# make a figure\n",
      "plt.figure(figsize=(10,6))\n",
      "\n",
      "plt.ylim(-1,1)\n",
      "\n",
      "\n",
      "plt.axhline(y=0, xmin=0, xmax=20, color = \"blue\")\n",
      "plt.scatter(np.arange(1,72,8), \n",
      "           [get_autocorr(measles_train,'cases',lag) for lag in np.arange(1,72,8)],\n",
      "           c='b')\n",
      "\n",
      "for i in np.arange(1,72,8):\n",
      "    plt.plot(i*np.ones(2),[0,get_autocorr(measles_train,'cases',i)],'b')\n",
      "\n",
      "plt.xlabel(\"Lag\", fontsize=16)\n",
      "plt.ylabel(\"ACF\", fontsize=16)\n",
      "\n",
      "plt.xticks(np.arange(1,72,8), fontsize=12)\n",
      "plt.yticks(fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "177/3:\n",
      "# import the packages we'll use\n",
      "from datetime import datetime\n",
      "\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "177/4: marathon = pd.read_csv(\"marathon.csv\", parse_dates = ['year'])\n",
      "177/5: marathon = pd.read_csv(\"marathon.csv\", parse_dates = ['year'])\n",
      "177/6:\n",
      "marathon = pd.read_csv(\"marathon.csv\", parse_dates = ['year'])\n",
      "print(marathon)\n",
      "177/7:\n",
      "## Make the train test split here.\n",
      "\n",
      "marathon_train = marathon.loc[:5].copy()\n",
      "measles_test = marathon.loc[5].copy()\n",
      "177/8:\n",
      "## Make the train test split here.\n",
      "\n",
      "marathon_train = marathon.loc[:5].copy()\n",
      "measles_test = marathon.loc[5:].copy()\n",
      "177/9:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "177/10:\n",
      "## Make the train test split here.\n",
      "\n",
      "marathon_train = marathon.loc[:5].copy()\n",
      "marathon_test = marathon.loc[5:].copy()\n",
      "177/11:\n",
      "## Calculate the Average Method RMSE here\n",
      "\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "177/12:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "##apply function (faster loops - 2,3 times)\n",
      "# Ner- naive error\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "177/13:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "##apply function (faster loops - 2,3 times)\n",
      "# Ner- naive error\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "\n",
      "print(marathon_test)\n",
      "177/14:\n",
      "## Make the train test split here.\n",
      "\n",
      "marathon_train = marathon.loc[5:].copy()\n",
      "marathon_test = marathon.loc[:5].copy()\n",
      "177/15:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "##apply function (faster loops - 2,3 times)\n",
      "# Ner- naive error\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "\n",
      "print(marathon_test)\n",
      "177/16:\n",
      "## Calculate the Average Method RMSE here\n",
      "\n",
      "print(x-marathon_train.iloc[-1,1])\n",
      "\n",
      "marathon_test['AvEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "177/17:\n",
      "## Calculate the Average Method RMSE here\n",
      "marathon_test['AvEr'] = marathon_test.time.apply(lambda x: x-marathon_train.iloc[-1,1])\n",
      "177/18:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "##apply function (faster loops - 2,3 times)\n",
      "# Ner- naive error\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: (x-marathon_train.iloc[-1,1])**2.mean() )\n",
      "\n",
      "print(marathon_test)\n",
      "177/19:\n",
      "## Calculate the Naive Method RMSE here\n",
      "\n",
      "##apply function (faster loops - 2,3 times)\n",
      "# Ner- naive error\n",
      "marathon_test['NEr'] = marathon_test.time.apply(lambda x: (x-marathon_train.iloc[-1,1])**2 )\n",
      "\n",
      "print(marathon_test)\n",
      "178/1:\n",
      "# import the packages we'll use\n",
      "from datetime import datetime\n",
      "\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "178/2: oil = pd.read_csv(\"oil.csv\", parse_dates=['year'])\n",
      "178/3:\n",
      "plt.plot(oil.year,oil.production)\n",
      "\n",
      "plt.show()\n",
      "178/4:\n",
      "oil_train = oil.loc[oil.year < datetime(2010,1,1),].copy()\n",
      "oil_test = oil.drop(oil_train.index).copy()\n",
      "178/5:\n",
      "# Import the SimpleExpSmoothing object\n",
      "from statsmodels.tsa.api import SimpleExpSmoothing\n",
      "178/6:\n",
      "# We make a SimpleExpSmoothing object like this\n",
      "# the data you want to train on is the input\n",
      "ses = SimpleExpSmoothing(oil_train.production.values)\n",
      "178/7:\n",
      "# Then you make a fitted object\n",
      "# you can put in a value for alpha\n",
      "# like so\n",
      "fit = ses.fit(smoothing_level=0.5, optimized=False)\n",
      "\n",
      "# or you can go the this route and\n",
      "# let the algorithm find the optimal alpha\n",
      "# for the training data\n",
      "# fit = ses.fit(optimized=True)\n",
      "178/8:\n",
      "# we can now make a plot of the fitted values \n",
      "# and the forecast on the test data\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "# plot the training data\n",
      "plt.plot(oil_train.year, oil_train.production,'b',\n",
      "            label = \"Training Data\")\n",
      "\n",
      "# plot the fit\n",
      "plt.plot(oil_train.year, fit.fittedvalues,'r-',\n",
      "            label = \"Fitted Values\")\n",
      "\n",
      "# plot the forecast\n",
      "plt.plot(oil_test.year, fit.forecast(len(oil_test)),'r--',\n",
      "            label = \"Forecast\")\n",
      "\n",
      "plt.plot(oil_test.year, oil_test.production,'b--',\n",
      "            label = \"Test Data\")\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "\n",
      "plt.xlabel(\"Year\", fontsize=16)\n",
      "plt.ylabel(\"Production \\n in millions of tonnes\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "178/9: oil = pd.read_csv(\"oil.csv\", parse_dates=['year'])\n",
      "178/10:\n",
      "plt.plot(oil.year,oil.production)\n",
      "\n",
      "plt.show()\n",
      "178/11:\n",
      "oil_train = oil.loc[oil.year < datetime(2010,1,1),].copy()\n",
      "oil_test = oil.drop(oil_train.index).copy()\n",
      "178/12:\n",
      "# Import the SimpleExpSmoothing object\n",
      "from statsmodels.tsa.api import SimpleExpSmoothing\n",
      "178/13:\n",
      "# We make a SimpleExpSmoothing object like this\n",
      "# the data you want to train on is the input\n",
      "ses = SimpleExpSmoothing(oil_train.production.values)\n",
      "178/14:\n",
      "# Then you make a fitted object\n",
      "# you can put in a value for alpha\n",
      "# like so\n",
      "fit = ses.fit(smoothing_level=0.5, optimized=False)\n",
      "\n",
      "# or you can go the this route and\n",
      "# let the algorithm find the optimal alpha\n",
      "# for the training data\n",
      "# fit = ses.fit(optimized=True)\n",
      "178/15:\n",
      "# we can now make a plot of the fitted values \n",
      "# and the forecast on the test data\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "# plot the training data\n",
      "plt.plot(oil_train.year, oil_train.production,'b',\n",
      "            label = \"Training Data\")\n",
      "\n",
      "# plot the fit\n",
      "plt.plot(oil_train.year, fit.fittedvalues,'r-',\n",
      "            label = \"Fitted Values\")\n",
      "\n",
      "# plot the forecast\n",
      "plt.plot(oil_test.year, fit.forecast(len(oil_test)),'r--',\n",
      "            label = \"Forecast\")\n",
      "\n",
      "plt.plot(oil_test.year, oil_test.production,'b--',\n",
      "            label = \"Test Data\")\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "\n",
      "plt.xlabel(\"Year\", fontsize=16)\n",
      "plt.ylabel(\"Production \\n in millions of tonnes\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "178/16:\n",
      "# Then you make a fitted object\n",
      "# you can put in a value for alpha\n",
      "# like so\n",
      "fit = ses.fit(smoothing_level=0.5, optimized=False)\n",
      "\n",
      "# or you can go the this route and\n",
      "# let the algorithm find the optimal alpha\n",
      "# for the training data\n",
      "fit = ses.fit(optimized=True)\n",
      "178/17:\n",
      "# we can now make a plot of the fitted values \n",
      "# and the forecast on the test data\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "# plot the training data\n",
      "plt.plot(oil_train.year, oil_train.production,'b',\n",
      "            label = \"Training Data\")\n",
      "\n",
      "# plot the fit\n",
      "plt.plot(oil_train.year, fit.fittedvalues,'r-',\n",
      "            label = \"Fitted Values\")\n",
      "\n",
      "# plot the forecast\n",
      "plt.plot(oil_test.year, fit.forecast(len(oil_test)),'r--',\n",
      "            label = \"Forecast\")\n",
      "\n",
      "plt.plot(oil_test.year, oil_test.production,'b--',\n",
      "            label = \"Test Data\")\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "\n",
      "plt.xlabel(\"Year\", fontsize=16)\n",
      "plt.ylabel(\"Production \\n in millions of tonnes\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "178/18:\n",
      "# Then you make a fitted object\n",
      "# you can put in a value for alpha\n",
      "# like so\n",
      "fit = ses.fit(smoothing_level=0.5, optimized=False)\n",
      "\n",
      "# or you can go the this route and\n",
      "# let the algorithm find the optimal alpha\n",
      "# for the training data\n",
      "# fit = ses.fit(optimized=True)\n",
      "179/1:\n",
      "import numpy as np\n",
      "\n",
      "import warnings\n",
      "#Comment this to turn on warnings\n",
      "#warnings.filterwarnings('ignore')\n",
      "\n",
      "np.random.seed() # shuffle random seed generator\n",
      "\n",
      "# Ising model parameters\n",
      "L=40 # linear system size\n",
      "J=-1.0 # Ising interaction\n",
      "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
      "T_c=2.26 # Onsager critical temperature in the TD limit\n",
      "179/2:\n",
      "import pickle, os\n",
      "from urllib.request import urlopen \n",
      "\n",
      "# url to data\n",
      "url_main = 'https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/';\n",
      "\n",
      "######### LOAD DATA\n",
      "# The data consists of 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25):\n",
      "data_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \n",
      "# The labels are obtained from the following file:\n",
      "label_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
      "\n",
      "\n",
      "#DATA\n",
      "data = pickle.load(urlopen(url_main + data_file_name)) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
      "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
      "data=data.astype('int')\n",
      "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
      "\n",
      "#LABELS (convention is 1 for ordered states and 0 for disordered states)\n",
      "labels = pickle.load(urlopen(url_main + label_file_name)) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
      "179/3:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "###### define ML parameters\n",
      "num_classes=2\n",
      "train_to_test_ratio=0.5 # training samples\n",
      "\n",
      "# divide data into ordered, critical and disordered\n",
      "X_ordered=data[:70000,:]\n",
      "Y_ordered=labels[:70000]\n",
      "\n",
      "X_train_ord,X_test_ord,Y_train_ord,Y_test_ord=train_test_split(X_ordered,Y_ordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "X_critical=data[70000:100000,:]\n",
      "Y_critical=labels[70000:100000]\n",
      "\n",
      "X_train_cri,X_test_cri,Y_train_cri,Y_test_cri=train_test_split(X_critical,Y_critical,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "X_disordered=data[100000:,:]\n",
      "Y_disordered=labels[100000:]\n",
      "\n",
      "X_train_disord,X_test_disord,Y_train_disord,Y_test_disord=train_test_split(X_disordered,Y_disordered,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "del data,labels\n",
      "\n",
      "# define training and test data sets\n",
      "\n",
      "## We used all three of the datasets rather than just 2 and concatenate \n",
      "## in the end \n",
      "\n",
      "X_train=np.concatenate((X_train_ord,X_train_disord, X_train_cri))\n",
      "Y_train=np.concatenate((Y_train_ord,Y_train_disord, Y_train_cri))\n",
      "\n",
      "\n",
      "X_test=np.concatenate((X_test_ord,X_test_disord, X_test_cri))\n",
      "Y_test=np.concatenate((Y_test_ord,Y_test_disord, Y_test_cri))\n",
      "\n",
      "\n",
      "# pick random data points from ordered and disordered states \n",
      "# to create the training and test sets\n",
      "# X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n",
      "\n",
      "# full data set\n",
      "# X=np.concatenate((X_critical,X))\n",
      "# Y=np.concatenate((Y_critical,Y))\n",
      "\n",
      "print('X_train shape:', X_train.shape)\n",
      "print('Y_train shape:', Y_train.shape)\n",
      "print()\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_critical.shape[0], 'critical samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "179/4:\n",
      "##### plot a few Ising states\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "# set colourbar map\n",
      "cmap_args=dict(cmap='plasma_r')\n",
      "\n",
      "# plot states\n",
      "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
      "\n",
      "axarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\n",
      "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
      "axarr[0].tick_params(labelsize=16)\n",
      "\n",
      "axarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\n",
      "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
      "axarr[1].tick_params(labelsize=16)\n",
      "\n",
      "im=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\n",
      "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
      "axarr[2].tick_params(labelsize=16)\n",
      "\n",
      "fig.subplots_adjust(right=2.0)\n",
      "\n",
      "plt.show()\n",
      "179/5:\n",
      "###### apply logistic regression\n",
      "from sklearn import linear_model\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "\n",
      "# define regularisation parameter\n",
      "lmbdas=np.logspace(-5,5,11)\n",
      "\n",
      "# preallocate data\n",
      "train_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "train_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "test_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "critical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n",
      "\n",
      "# loop over regularisation strength\n",
      "for i,lmbda in enumerate(lmbdas):\n",
      "\n",
      "    # define logistic regressor\n",
      "#     logreg=linear_model.LogisticRegression(C=1.0/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n",
      "#                                            solver='liblinear')\n",
      "\n",
      "    # fit training data\n",
      "    #logreg.fit(X_train, Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy[i]=logreg.score(X_train,Y_train)\n",
      "    test_accuracy[i]=logreg.score(X_test,Y_test)\n",
      "    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('accuracy: train, test, critical')\n",
      "#     print('liblin: %0.4f, %0.4f, %0.4f' %(train_accuracy[i],test_accuracy[i],critical_accuracy[i]) )\n",
      "\n",
      "    # define SGD-based logistic regression\n",
      "    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n",
      "                                           shuffle=True, random_state=1, learning_rate='optimal')\n",
      "\n",
      "    # fit training data\n",
      "    logreg_SGD.fit(X_train,Y_train)\n",
      "\n",
      "    # check accuracy\n",
      "    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n",
      "    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n",
      "    #critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n",
      "    \n",
      "    print('SGD: %0.4f, %0.4f, %0.4f' %(train_accuracy_SGD[i],test_accuracy_SGD[i],critical_accuracy_SGD[i]) )\n",
      "\n",
      "    print('finished computing %i/11 iterations' %(i+1))\n",
      "\n",
      "# plot accuracy against regularisation strength\n",
      "# plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\n",
      "# plt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n",
      "#SGD\n",
      "plt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='train')\n",
      "plt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label=' test')\n",
      "# plt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label=' critical')\n",
      "\n",
      "plt.xlabel('$\\\\lambda$')\n",
      "plt.ylabel('$\\\\mathrm{accuracy}$')\n",
      "\n",
      "plt.grid()\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "180/1:\n",
      "# to get the iris data\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# for data handling \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# for plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style(\"white\")\n",
      "180/2:\n",
      "# Load the data\n",
      "iris = load_iris()\n",
      "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
      "iris_df['iris_class'] = iris['target']\n",
      "180/3:\n",
      "# This chunk of code is going to plot the data\n",
      "sns.lmplot(data = iris_df, x = 'sepal_length', \n",
      "            y = 'sepal_width',hue = 'iris_class',fit_reg=False,\n",
      "            height = 8,legend=False)\n",
      "\n",
      "plt.legend(title='Iris Class', loc='upper left', \n",
      "           labels=['setosa', 'versicolor', 'virginica'], \n",
      "           fontsize = 12)\n",
      "plt.xlabel(\"Sepal Length\",fontsize = 12)\n",
      "plt.ylabel(\"Sepal Width\",fontsize = 12)\n",
      "\n",
      "plt.show()\n",
      "180/4:\n",
      "# Here is some random data\n",
      "# to illustrate the knn concept\n",
      "np.random.seed(440)\n",
      "xs = np.random.randn(50,2)\n",
      "os = np.random.randn(50,2)-np.array([3,0])\n",
      "\n",
      "unlabeled = [0,0]\n",
      "180/5:\n",
      "# We now plot that data\n",
      "plt.figure(figsize = (12,9))\n",
      "\n",
      "plt.plot(xs[:,0],xs[:,1],'rx',label = \"X\",markersize=8)\n",
      "plt.plot(os[:,0],os[:,1],'bo',label = \"O\",markersize=8)\n",
      "plt.plot(unlabeled[0],unlabeled[1],'gv',label = \"Unknown\",markersize=12)\n",
      "plt.xlabel(\"Feature 1\", fontsize = 16)\n",
      "plt.ylabel(\"Feature 2\", fontsize = 16)\n",
      "plt.legend(fontsize = 14)\n",
      "plt.show()\n",
      "180/6:\n",
      "# import train_test_split\n",
      "from sklearn.model_selection import train_test_split\n",
      "180/7:\n",
      "# We can turn the data in numpy arrays\n",
      "# like so\n",
      "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
      "y = iris_df[['iris_class']].to_numpy()\n",
      "\n",
      " \n",
      "# make the test data 25% of the total data\n",
      "# Set a random seed\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "     X, y, test_size=0.25, random_state=440)\n",
      "180/8:\n",
      "# import NearestNeighbors\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "180/9:\n",
      "# Make the model\n",
      "knn = KNeighborsClassifier(n_neighbors = 3)\n",
      "180/10:\n",
      "# Fit the model\n",
      "knn.fit(X_train,y_train.ravel())\n",
      "180/11:\n",
      "# Make a prediction on our train set\n",
      "y_predict = knn.predict(X_train)\n",
      "180/12:\n",
      "# calculate the accuracy here\n",
      "# sum a list of booleans and True gets cast as 1\n",
      "# False gets cast as 0\n",
      "print(\"Our model has a \",\n",
      "      np.round(sum(y_predict == y_train.ravel())/len(y_train)*100,2),\n",
      "      \"% accuracy on the training set\")\n",
      "180/13:\n",
      "# Make a prediction on our train set\n",
      "y_predict = knn.predict(X_train)\n",
      "180/14:\n",
      "# calculate the accuracy here\n",
      "# sum a list of booleans and True gets cast as 1\n",
      "# False gets cast as 0\n",
      "print(\"Our model has a \",\n",
      "      np.round(sum(y_predict == y_train.ravel())/len(y_train)*100,2),\n",
      "      \"% accuracy on the training set\")\n",
      "183/1:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "183/2:\n",
      "# Generate the random data\n",
      "np.random.seed(440)\n",
      "n_rows = 100\n",
      "diff = .1\n",
      "X = np.random.random((n_rows,2))\n",
      "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
      "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
      "\n",
      "del X\n",
      "X = np.append(X_prime,X_2prime,axis = 0)\n",
      "\n",
      "y = np.empty(np.shape(X)[0])\n",
      "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
      "y[((X[:,1] - X[:,0]) >= diff)] = 0\n",
      "183/3:\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/4:\n",
      "plt.figure(figsize = (12,12))\n",
      "\n",
      "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
      "plt.plot(np.linspace(0,1,100),np.linspace(0,1,100),'k',label=\"x_2-x_1 = 0\")\n",
      "plt.plot(np.linspace(0,1,100),1.5*np.linspace(0,1,100)-.25,'b--',label=\"x_2-1.5x_1 = -0.25\")\n",
      "plt.plot(np.linspace(0,1,100),(1/1.5)*np.linspace(0,1,100)+.19,'r-.',label = \"x_2-(1/1.5)x_1 = 0.19\")\n",
      "\n",
      "\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
      "\n",
      "plt.xlim((0,1))\n",
      "plt.ylim((0,1))\n",
      "\n",
      "plt.show()\n",
      "183/5:\n",
      "# First import the classifier\n",
      "from sklearn.svm import LinearSVC\n",
      "183/6:\n",
      "# Now we make the classifier\n",
      "# For now don't worry about c, we'll talk about it soon\n",
      "# Same with loss\n",
      "max_margin = LinearSVC(C=1, loss=\"hinge\")\n",
      "183/7:\n",
      "# You can take over here\n",
      "# fit the model\n",
      "max_margin.fit(X,y)\n",
      "183/8:\n",
      "def plot_bound(clf,X,y,pts = False):\n",
      "    plt.figure(figsize = (10,10))\n",
      "    coefs = clf.coef_[0]\n",
      "    intercept = clf.intercept_[0]\n",
      "    \n",
      "    if coefs[0] != 0:\n",
      "        plt.plot(np.linspace(np.min(X[:,0]),np.max(X[:,0]),1000),\n",
      "                     (-intercept/coefs[1])-(coefs[0]/coefs[1])*np.linspace(np.min(X[:,0]),np.max(X[:,0]),1000),\n",
      "                        'k',label = \"SVM Boundary\")\n",
      "    else:\n",
      "        plt.plot((-intercept/coefs[0])*np.ones(1000),\n",
      "                     np.linspace(np.min(X[:,1],np.max(X[:,1]),1000),\n",
      "                        'k'),label = \"SVM Boundary\")\n",
      "        \n",
      "    if pts:\n",
      "        plt.scatter(X[y == 0,0],X[y == 0,1],color=\"orange\",label=\"0\",s=50)\n",
      "        plt.scatter(X[y == 1,0],X[y == 1,1],color=\"blue\",label=\"1\", alpha = 1,s=50)\n",
      "\n",
      "        plt.legend(fontsize=16)\n",
      "\n",
      "    plt.xlabel(\"$x_1$\",fontsize=16)\n",
      "    plt.ylabel(\"$x_2$\",fontsize=16)\n",
      "    plt.title(\"Decision Boundary\",fontsize = 20)\n",
      "    \n",
      "\n",
      "    plt.show()\n",
      "183/9:\n",
      "## Import the data here\n",
      "from sklearn import datasets\n",
      "\n",
      "iris = datasets.load_iris()\n",
      "183/10:\n",
      "# Get the data and the targets here \n",
      "# Grab Features\n",
      "X = iris['data']\n",
      "\n",
      "# Grab Targets\n",
      "y = np.ones(np.shape(iris['target']))\n",
      "# Reclassify as setosa and not setosa\n",
      "y = np.where(iris['target']!=0, 0, y)\n",
      "183/11:\n",
      "# Make the train test split here\n",
      "# Use 440 as the random_state\n",
      "# remember to stratify\n",
      "y_train, y_test = train_test_split(y, test_size=.2, shuffle = True)\n",
      "183/12:\n",
      "## Import the data here\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "iris = datasets.load_iris()\n",
      "183/13:\n",
      "# Get the data and the targets here \n",
      "# Grab Features\n",
      "X = iris['data']\n",
      "\n",
      "# Grab Targets\n",
      "y = np.ones(np.shape(iris['target']))\n",
      "# Reclassify as setosa and not setosa\n",
      "y = np.where(iris['target']!=0, 0, y)\n",
      "183/14:\n",
      "# Make the train test split here\n",
      "# Use 440 as the random_state\n",
      "# remember to stratify\n",
      "y_train, y_test = train_test_split(y, test_size=.2, shuffle = True)\n",
      "183/15:\n",
      "# Make the train test split here\n",
      "# Use 440 as the random_state\n",
      "# remember to stratify\n",
      "x_train, x_test = train_test_split(X, test_size=.2, shuffle = True)\n",
      "y_train, y_test = train_test_split(y, test_size=.2, shuffle = True)\n",
      "183/16:\n",
      "# Code here\n",
      "\n",
      "max_margin = LinearSVC(C=1, loss=\"hinge\")\n",
      "\n",
      "max_margin.fit(X_train,y_train)\n",
      "183/17:\n",
      "# Code here\n",
      "\n",
      "max_margin = LinearSVC(C=1, loss=\"hinge\")\n",
      "\n",
      "max_margin.fit(x_train,y_train)\n",
      "183/18:\n",
      "# Generate the random data\n",
      "np.random.seed(440)\n",
      "n_rows = 100\n",
      "diff = .1\n",
      "X = np.random.random((n_rows,2))\n",
      "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
      "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
      "X_3prime = [[.4,.9],[.6,.45],[.7,.9],[.3,.19],[.1,.4]]\n",
      "\n",
      "del X\n",
      "X = np.append(X_prime,np.append(X_2prime,X_3prime,axis = 0),axis=0)\n",
      "\n",
      "y = np.empty(np.shape(X)[0])\n",
      "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
      "y[((X[:,1] - X[:,0]) >= diff)] = 0\n",
      "y[-5] = 1\n",
      "y[-4] = 0\n",
      "y[-3] = 1\n",
      "y[-2] = 0\n",
      "y[-1] = 1\n",
      "183/19:\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/20:\n",
      "# Generate the random data\n",
      "np.random.seed(440)\n",
      "n_rows = 100\n",
      "diff = .1\n",
      "X = np.random.random((n_rows,2))\n",
      "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
      "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
      "X_3prime = [[.4,.9],[.6,.45],[.7,.9],[.3,.19],[.1,.4]]\n",
      "\n",
      "del X\n",
      "X = np.append(X_prime,np.append(X_2prime,X_3prime,axis = 0),axis=0)\n",
      "\n",
      "y = np.empty(np.shape(X)[0])\n",
      "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
      "y[((X[:,1] - X[:,0]) >= diff)] = 0\n",
      "y[-5] = 1\n",
      "y[-4] = 0\n",
      "y[-3] = 1\n",
      "y[-2] = 0\n",
      "y[-1] = 1\n",
      "183/21:\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/22:\n",
      "# Code here\n",
      "# Sample Solution\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X,y)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(test,X,y,True)\n",
      "183/23:\n",
      "from sklearn.model_selection import train_test_split\n",
      "# import the data set\n",
      "cancer = pd.read_csv(\"cancer_pca.csv\")\n",
      "\n",
      "X = cancer[['pca_1','pca_2']]\n",
      "y = cancer['cancer_outcome']\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=440,test_size = .25,stratify = y)\n",
      "183/24:\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "plt.scatter(X_train.loc[y_train==0,'pca_1'],X_train.loc[y_train==0,'pca_2'],c = 'orange',label=\"malignant\")\n",
      "plt.scatter(X_train.loc[y_train==1,'pca_1'],X_train.loc[y_train==1,'pca_2'],c = 'blue',label=\"benign\",alpha = .7)\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"pca 1\",fontsize = 16)\n",
      "plt.ylabel(\"pca 2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/25:\n",
      "from sklearn.model_selection import train_test_split\n",
      "# import the data set\n",
      "cancer = pd.read_csv(\"cancer_pca.csv\")\n",
      "\n",
      "X = cancer[['pca_1','pca_2']]\n",
      "y = cancer['cancer_outcome']\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=440,test_size = .25,stratify = y)\n",
      "183/26:\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "plt.scatter(X_train.loc[y_train==0,'pca_1'],X_train.loc[y_train==0,'pca_2'],c = 'orange',label=\"malignant\")\n",
      "plt.scatter(X_train.loc[y_train==1,'pca_1'],X_train.loc[y_train==1,'pca_2'],c = 'blue',label=\"benign\",alpha = .7)\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"pca 1\",fontsize = 16)\n",
      "plt.ylabel(\"pca 2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/27:\n",
      "## Code here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X,y)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(test,X,y,True)\n",
      "183/28:\n",
      "## Code here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X_train,y_train)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(test,X_test,y_test,True)\n",
      "183/29:\n",
      "from sklearn.model_selection import train_test_split\n",
      "# import the data set\n",
      "cancer = pd.read_csv(\"cancer_pca.csv\")\n",
      "\n",
      "X = cancer[['pca_1','pca_2']]\n",
      "y = cancer['cancer_outcome']\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=440,test_size = .25,stratify = y)\n",
      "183/30:\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "plt.scatter(X_train.loc[y_train==0,'pca_1'],X_train.loc[y_train==0,'pca_2'],c = 'orange',label=\"malignant\")\n",
      "plt.scatter(X_train.loc[y_train==1,'pca_1'],X_train.loc[y_train==1,'pca_2'],c = 'blue',label=\"benign\",alpha = .7)\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"pca 1\",fontsize = 16)\n",
      "plt.ylabel(\"pca 2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/31:\n",
      "## Code here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X_train,y_train)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(test,X_test,y_test,True)\n",
      "183/32:\n",
      "## Code here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X_train,y_train)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(X_test,y_test,True)\n",
      "183/33:\n",
      "## Code here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
      "    test = LinearSVC(C=i, loss=\"hinge\")\n",
      "    test.fit(X_train,y)\n",
      "\n",
      "    print(\"C is\", i)\n",
      "    plot_bound(test,X_test,y_test,True)\n",
      "183/34:\n",
      "# Import make_circles function\n",
      "from sklearn.datasets import make_circles\n",
      "183/35:\n",
      "# Generate data\n",
      "X, y = make_circles(n_samples=500, factor=.3, noise=.1)\n",
      "183/36:\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/37:\n",
      "np.random.seed(440)\n",
      "\n",
      "X = 2*np.random.random((50,1)) - 1\n",
      "y = np.ones(np.shape(X))\n",
      "\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "\n",
      "plt.scatter(X[(X > .3) | (X < -.3)],y[(X > .3) | (X < -.3)],c=\"blue\",label=\"1\")\n",
      "plt.scatter(X[(X <= .3) & (X >= -.3)],y[(X <= .3) & (X >= -.3)],c=\"orange\",label=\"0\")\n",
      "plt.yticks([])\n",
      "\n",
      "plt.legend(fontsize=16)\n",
      "plt.xlabel(\"x_1\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "183/38:\n",
      "## Add a feature x_2 to X, that is just x_1^2\n",
      "## make sure it is an np array with the first column as x_1\n",
      "## and the second column as x_2\n",
      "X = np.append(X,X**2,axis=1)\n",
      "y[(X[:,0] <= .3) & (X[:,0] >= -.3)] = 0\n",
      "\n",
      "## Now plot it using the below code\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "\n",
      "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],X[(X[:,0] > .3) | (X[:,0] < -.3),1],c=\"blue\",label=\"1\")\n",
      "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],X[(X[:,0] <= .3) & (X[:,0] >= -.3),1],c=\"orange\",label=\"0\")\n",
      "\n",
      "\n",
      "plt.legend(fontsize=16)\n",
      "plt.xlabel(\"x_1\",fontsize=16)\n",
      "plt.ylabel(\"x_2\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "183/39: from sklearn.preprocessing import PolynomialFeatures\n",
      "183/40:\n",
      "# LinearSVC can find a perfectly classifying decision boundary\n",
      "clf = Pipeline([\n",
      "    (\"poly_features\",PolynomialFeatures(degree=2,include_bias=False)),\n",
      "    (\"scaler\",StandardScaler()),\n",
      "    (\"svm_clf\",LinearSVC(C=5,loss=\"hinge\"))\n",
      "])\n",
      "\n",
      "\n",
      "\n",
      "clf.fit(X[:,0].reshape(-1,1),y.ravel())\n",
      "\n",
      "poly = clf.named_steps['poly_features']\n",
      "scaler = clf.named_steps['scaler']\n",
      "\n",
      "plot_bound(clf.named_steps['svm_clf'],scaler.fit_transform(poly.fit_transform(X[:,0].reshape(-1,1))),y.ravel(),True)\n",
      "183/41:\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "\n",
      "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],c=\"blue\",label=\"1\")\n",
      "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],c=\"orange\",label=\"0\")\n",
      "plt.yticks([])\n",
      "\n",
      "plt.legend(fontsize=16)\n",
      "plt.xlabel(\"x_1\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "183/42:\n",
      "# First import SVC\n",
      "from sklearn.svm import SVC\n",
      "183/43:\n",
      "# Now create an SVC instance with polynomial kernel\n",
      "svm_clf = SVC(kernel = \"poly\", degree = 2, coef0 = 1, C=1)\n",
      "\n",
      "# Fit, remember to only use the 0th column of X\n",
      "svm_clf.fit(X[:,0].reshape(-1,1),y.ravel())\n",
      "183/44:\n",
      "# We'll use this to test the SVM we just made\n",
      "test = np.linspace(-1,1,1000)\n",
      "183/45:\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],\n",
      "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],\n",
      "            c=\"blue\",label=\"training 1\",marker=\"*\")\n",
      "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],\n",
      "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],\n",
      "            c=\"orange\",label=\"training 0\",marker=\"*\")\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/46:\n",
      "# Circle Data Set\n",
      "X_train, y_train = make_circles(n_samples=500, factor=.4, noise=.15)\n",
      "X_test, y_test = make_circles(n_samples=100, factor=.4, noise=.15)\n",
      "183/47:\n",
      "# Plot it\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/48:\n",
      "# LinearSVC can find a perfectly classifying decision boundary\n",
      "clf = Pipeline([\n",
      "    (\"poly_features\",PolynomialFeatures(degree=2,include_bias=False)),\n",
      "    (\"scaler\",StandardScaler()),\n",
      "    (\"svm_clf\",LinearSVC(C=5,loss=\"hinge\"))\n",
      "])\n",
      "\n",
      "\n",
      "\n",
      "clf.fit(X[:,0].reshape(-1,1),y.ravel())\n",
      "\n",
      "poly = clf.named_steps['poly_features']\n",
      "scaler = clf.named_steps['scaler']\n",
      "\n",
      "plot_bound(clf.named_steps['svm_clf'],scaler.fit_transform(poly.fit_transform(X[:,0].reshape(-1,1))),y.ravel(),True)\n",
      "183/49:\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "\n",
      "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],c=\"blue\",label=\"1\")\n",
      "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],c=\"orange\",label=\"0\")\n",
      "plt.yticks([])\n",
      "\n",
      "plt.legend(fontsize=16)\n",
      "plt.xlabel(\"x_1\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "183/50:\n",
      "# We'll use this to test the SVM we just made\n",
      "test = np.linspace(-1,1,1000)\n",
      "183/51:\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],\n",
      "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],\n",
      "            c=\"blue\",label=\"training 1\",marker=\"*\")\n",
      "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],\n",
      "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],\n",
      "            c=\"orange\",label=\"training 0\",marker=\"*\")\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/52:\n",
      "# Circle Data Set\n",
      "X_train, y_train = make_circles(n_samples=500, factor=.4, noise=.15)\n",
      "X_test, y_test = make_circles(n_samples=100, factor=.4, noise=.15)\n",
      "183/53:\n",
      "# Plot it\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "plt.show()\n",
      "183/54:\n",
      "# Your Code here\n",
      "\n",
      "svm_clf = SVC(kernel = \"rbf\", degree = 2, coef0 = 1, C=1)\n",
      "\n",
      "# Fit, remember to only use the 0th column of X\n",
      "svm_clf.fit(X[:,0].reshape(-1,1),y.ravel())\n",
      "183/55:\n",
      "# Your Code here\n",
      "\n",
      "svm_clf = SVC(kernel = \"rbf\", degree = 2, coef0 = 1, C=1)\n",
      "\n",
      "# Fit, remember to only use the 0th column of X\n",
      "svm_clf.fit(X_train[:,0].reshape(-1,1),y_train.ravel())\n",
      "183/56:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/57:\n",
      "# Your Code here\n",
      "\n",
      "svm_clf = SVC(kernel = \"rbf\", degree = 2, coef0 = 1, C=1)\n",
      "\n",
      "# Fit, remember to only use the 0th column of X\n",
      "svm_clf.fit(X_train[:,0].reshape(-1,1),y_train.ravel())\n",
      "183/58:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/59:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(X_test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/60:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(X_test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "plt.ylim([.8,1.1])\n",
      "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/61:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
      "plt.scatter(X_test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "# plt.ylim([.8,1.1])\n",
      "# plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "# plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/62:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[y_test == 1,0],X_test[y_test == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_test[y_test == 0,0],X_test[y_test == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "# plt.ylim([.8,1.1])\n",
      "# plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "# plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/63:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[y_test == 1,0],X_test[y_test == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_test[y_test == 0,0],X_test[y_test == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "# plt.ylim([.8,1.1])\n",
      "# plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "# plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "183/64:\n",
      "# Your Code here\n",
      "\n",
      "# predict on test\n",
      "pred = svm_clf.predict(test.reshape(-1,1))\n",
      "\n",
      "# Now plot the prediction along with the true boundary\n",
      "# and the training data\n",
      "plt.figure(figsize = (10,10))\n",
      "\n",
      "# plot the predictions\n",
      "plt.scatter(X_test[y_test == 1,0],X_test[y_test == 1,1],c = \"blue\",label=\"1\")\n",
      "plt.scatter(X_test[y_test == 0,0],X_test[y_test == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "# Plot the true boundary\n",
      "#plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
      "\n",
      "# plot the training data\n",
      "# plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
      "# plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
      "\n",
      "plt.legend(fontsize = 16)\n",
      "plt.xlabel(\"x_1\",fontsize = 16)\n",
      "plt.ylabel(\"x_2\",fontsize = 16)\n",
      "\n",
      "\n",
      "plt.yticks([])\n",
      "# plt.ylim([.8,1.1])\n",
      "# plt.xlabel(\"$x_1$\",fontsize = 16)\n",
      "# plt.legend(fontsize=16)\n",
      "\n",
      "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
      "\n",
      "plt.show()\n",
      "184/1:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "184/2:\n",
      "# This loads the data\n",
      "from sklearn.datasets import load_digits\n",
      "184/3:\n",
      "# load in the data\n",
      "X, y = load_digits(n_class=10, return_X_y=True)\n",
      "images = load_digits()['images']\n",
      "184/4:\n",
      "# This data set only has a \n",
      "# small sample of the 60,000\n",
      "# digits\n",
      "\n",
      "# Each row is an image, and each column\n",
      "# is the value for an 8x8 grid\n",
      "np.shape(X)\n",
      "184/5:\n",
      "# show some samples for each\n",
      "# digit\n",
      "fig,ax = plt.subplots(2,5,figsize=(14,8))\n",
      "\n",
      "for i in range(0,10):\n",
      "    ax[i//5,i%5].set_axis_off()\n",
      "    ax[i//5,i%5].imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
      "    ax[i//5,i%5].text(.5,0,i,fontsize=14)\n",
      "\n",
      "plt.show()\n",
      "184/6:\n",
      "## Build your classification model here\n",
      "## choose either a voting model\n",
      "## a bagging model\n",
      "## or a pasting model\n",
      "\n",
      "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = True\n",
      "                           )\n",
      "\n",
      "\n",
      "# Let's use a knn classifier\n",
      "# if bootstrap = True we use bagging\n",
      "# if bootstrap = False it is pasting\n",
      "# n_estimators is how many models we fit\n",
      "# max_samples is the number of training points sampled\n",
      "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = False\n",
      "                           )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "knn = KNeighborsClassifier(20)\n",
      "# Fit knn\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"KNN Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Bagged Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Bagging Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Paste Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Pasting Accuracy\", np.round(acc,5))\n",
      "184/7:\n",
      "# This loads the data\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "184/8:\n",
      "## Okay now choose a digit as a target\n",
      "## then make a new vector from y\n",
      "## where all instances of your digit are labeled as 1\n",
      "## and all other instances all 0\n",
      "\n",
      "X_train,y_train = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "X_test,y_test = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "184/9:\n",
      "## Okay now choose a digit as a target\n",
      "## then make a new vector from y\n",
      "## where all instances of your digit are labeled as 1\n",
      "## and all other instances all 0\n",
      "from sklearn.datasets import make_circles\n",
      "X_train,y_train = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "X_test,y_test = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "184/10:\n",
      "## Build your classification model here\n",
      "## choose either a voting model\n",
      "## a bagging model\n",
      "## or a pasting model\n",
      "\n",
      "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = True\n",
      "                           )\n",
      "\n",
      "\n",
      "# Let's use a knn classifier\n",
      "# if bootstrap = True we use bagging\n",
      "# if bootstrap = False it is pasting\n",
      "# n_estimators is how many models we fit\n",
      "# max_samples is the number of training points sampled\n",
      "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = False\n",
      "                           )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "knn = KNeighborsClassifier(20)\n",
      "# Fit knn\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"KNN Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Bagged Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Bagging Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Paste Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Pasting Accuracy\", np.round(acc,5))\n",
      "184/11:\n",
      "## Additional code here if needed\n",
      "\n",
      "# Fit knn\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"KNN Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Bagged Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Bagging Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Paste Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Pasting Accuracy\", np.round(acc,5))\n",
      "184/12:\n",
      "## Okay now choose a digit as a target\n",
      "## then make a new vector from y\n",
      "## where all instances of your digit are labeled as 1\n",
      "## and all other instances all 0\n",
      "from sklearn.datasets import make_circles\n",
      "X_train,y_train = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "X_test,y_test = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n",
      "184/13:\n",
      "## Build your classification model here\n",
      "## choose either a voting model\n",
      "## a bagging model\n",
      "## or a pasting model\n",
      "\n",
      "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = True\n",
      "                           )\n",
      "\n",
      "\n",
      "# Let's use a knn classifier\n",
      "# if bootstrap = True we use bagging\n",
      "# if bootstrap = False it is pasting\n",
      "# n_estimators is how many models we fit\n",
      "# max_samples is the number of training points sampled\n",
      "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = False\n",
      "                           )\n",
      "184/14:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "184/15: from sklearn.datasets import make_moons\n",
      "184/16:\n",
      "# import our classifiers\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "# The VotingClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "184/17:\n",
      "## Build your classification model here\n",
      "## choose either a voting model\n",
      "## a bagging model\n",
      "## or a pasting model\n",
      "\n",
      "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = True\n",
      "                           )\n",
      "\n",
      "\n",
      "# Let's use a knn classifier\n",
      "# if bootstrap = True we use bagging\n",
      "# if bootstrap = False it is pasting\n",
      "# n_estimators is how many models we fit\n",
      "# max_samples is the number of training points sampled\n",
      "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = False\n",
      "                           )\n",
      "184/18:\n",
      "## Additional code here if needed\n",
      "\n",
      "# Fit knn\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"KNN Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Bagged Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Bagging Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Paste Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Pasting Accuracy\", np.round(acc,5))\n",
      "184/19:\n",
      "## Build your classification model here\n",
      "## choose either a voting model\n",
      "## a bagging model\n",
      "## or a pasting model\n",
      "\n",
      "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = True\n",
      "                           )\n",
      "\n",
      "\n",
      "# Let's use a knn classifier\n",
      "# if bootstrap = True we use bagging\n",
      "# if bootstrap = False it is pasting\n",
      "# n_estimators is how many models we fit\n",
      "# max_samples is the number of training points sampled\n",
      "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
      "                            n_estimators = 1000,\n",
      "                            max_samples = 1000,\n",
      "                            bootstrap = False\n",
      "                           )\n",
      "\n",
      "\n",
      "\n",
      "knn = KNeighborsClassifier(20)\n",
      "184/20:\n",
      "## Additional code here if needed\n",
      "\n",
      "# Fit knn\n",
      "knn.fit(X_train,y_train)\n",
      "y_pred = knn.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"KNN Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Bagged Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Bagging Accuracy\", np.round(acc,5))\n",
      "\n",
      "# Fit Paste Data\n",
      "bag_clf.fit(X_train,y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "acc = sum(y_test == y_pred)/len(y_pred)\n",
      "\n",
      "print(\"Pasting Accuracy\", np.round(acc,5))\n",
      "185/1:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "185/2:\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "185/3:\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "\n",
      "plt.show()\n",
      "185/4:\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "\n",
      "plt.show()\n",
      "185/5:\n",
      "plt.figure(figsize=(8,6))\n",
      "\n",
      "sample = np.random.choice(range(0,200),100,replace=False)\n",
      "not_sample = [i for i in range(0,200) if i not in sample]\n",
      "\n",
      "\n",
      "plt.scatter(X[not_sample,0],X[not_sample,1], label=\"Cluster 1\")\n",
      "plt.scatter(X[sample,0],X[sample,1], label=\"Cluster 2\")\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "\n",
      "plt.show()\n",
      "185/6:\n",
      "# the function is stored in sklearn.cluster\n",
      "from sklearn.cluster import KMeans\n",
      "185/7:\n",
      "# Let's try just 2 clusters\n",
      "kmeans = KMeans(2)\n",
      "185/8: kmeans.fit(X)\n",
      "185/9:\n",
      "clusters = kmeans.predict(X)\n",
      "\n",
      "clusters[:50]\n",
      "185/10:\n",
      "# the function is stored in sklearn.cluster\n",
      "from sklearn.cluster import KMeans\n",
      "185/11:\n",
      "# Let's try just 2 clusters\n",
      "kmeans = KMeans(2)\n",
      "185/12: kmeans.fit(X)\n",
      "185/13:\n",
      "clusters = kmeans.predict(X)\n",
      "\n",
      "clusters[:50]\n",
      "185/14:\n",
      "clusters = kmeans.predict(X)\n",
      "\n",
      "clusters[:50]\n",
      "185/15:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "\n",
      "ax[0].scatter(X[clusters==1,0],X[clusters==1,1], label=\"$k$ Cluster 1\",c=\"b\")\n",
      "ax[0].scatter(X[clusters==0,0],X[clusters==0,1], label=\"$k$ Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/16:\n",
      "# Let's try 5 clusters\n",
      "kmeans = KMeans(5)\n",
      "185/17: kmeans.fit(X)\n",
      "185/18:\n",
      "clusters = kmeans.predict(X)\n",
      "\n",
      "clusters[:10]\n",
      "185/19:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/20:\n",
      "# try this first\n",
      "X = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
      "\n",
      "X.head()\n",
      "\n",
      "# if that doesn't work comment it out then uncomment this\n",
      "# from sklearn.datasets import load_digits\n",
      "# X, y = load_digits(n_class=10, return_X_y=True)\n",
      "185/21:\n",
      "# try this first\n",
      "# X = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
      "\n",
      "# X.head()\n",
      "\n",
      "# if that doesn't work comment it out then uncomment this\n",
      "from sklearn.datasets import load_digits\n",
      "X, y = load_digits(n_class=10, return_X_y=True)\n",
      "185/22:\n",
      "from sklearn.cluster import KMeans\n",
      "kmeans = KMeans(10)\n",
      "kmeans.fit(X)\n",
      "185/23: clusters = kmeans.predict(X)\n",
      "185/24:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/25:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"--*b\"]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/26:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"--*b\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/27:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"-*b\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/28:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"*b\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/29:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"--b\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/30:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"*b\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/31:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\",\"*\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/32:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "\n",
      "for i in range(6):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/33:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "ax[1].legend(fontsize=14)\n",
      "ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/34:\n",
      "print(X.shape)\n",
      "clusters = kmeans.predict(X)\n",
      "185/35:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# ax[1].legend(fontsize=14)\n",
      "# ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/36:\n",
      "print(X)\n",
      "from sklearn.cluster import KMeans\n",
      "kmeans = KMeans(10)\n",
      "kmeans.fit(X)\n",
      "185/37:\n",
      "print(X.shape)\n",
      "clusters = kmeans.predict(X)\n",
      "185/38:\n",
      "fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# c=cs[i]\n",
      "\n",
      "for i in range(5):\n",
      "    ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "ax[0].legend(fontsize=14)\n",
      "ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# ax[1].legend(fontsize=14)\n",
      "# ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/39:\n",
      "\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/40:\n",
      "print(X.shape)\n",
      "clusters = kmeans.predict(X)\n",
      "185/41:\n",
      "clusters\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/42:\n",
      "clusters.centroid()\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/43:\n",
      "clusters.centroid[]\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/44:\n",
      "clusters.centroid\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/45:\n",
      "clusters[0]\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/46:\n",
      "for i in range (10):\n",
      "    print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/47:\n",
      "for i in range (20):\n",
      "    print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/48:\n",
      "for i in range (X.shape[0]):\n",
      "    print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/49:\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        lablcount.append(np.sum[clusters[el]=='i'])\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/50:\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.sum[clusters[el]=='i'])\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/51:\n",
      "clusters.shape\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.sum[clusters[el]=='i'])\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/52:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.sum[clusters[el]=='i'])\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/53:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.sum([clusters[el]=='i']))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/54:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.count([clusters[el]=='i']))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/55:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.count(clusters=='i'))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/56:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    for el in X:\n",
      "        labelcount.append(np.sum(clusters=='i'))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/57:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    #for el in X:\n",
      "    labelcount.append(np.sum(clusters=='i'))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/58: print(labelcount)\n",
      "185/59:\n",
      "print(clusters.shape)\n",
      "labelcount=[]\n",
      "for i in range (10):\n",
      "    #for el in X:\n",
      "    labelcount.append(np.sum(clusters==i))\n",
      "    #print(clusters[i])\n",
      "#X.iloc[:,1:]\n",
      "\n",
      "# fig,ax = plt.subplots(1,2,figsize=(14,6),sharex=True,sharey=True)\n",
      "\n",
      "# cs = [\"b\",\"r\",\"g\",\"orange\",\"c\"]\n",
      "# # c=cs[i]\n",
      "\n",
      "# for i in range(5):\n",
      "#     ax[0].scatter(X[clusters==i,0],X[clusters==i,1], label=\"$k$ Cluster \" + str(i),c=cs[i])\n",
      "\n",
      "# ax[0].legend(fontsize=14)\n",
      "# ax[0].set_title(\"$k$ Means Output\",fontsize=16)\n",
      "\n",
      "# # ax[1].scatter(X[:100,0],X[:100,1], label=\"Actual Cluster 1\",c=\"b\")\n",
      "# # ax[1].scatter(X[100:,0],X[100:,1], label=\"Actual Cluster 0\",c=\"r\")\n",
      "\n",
      "# # ax[1].legend(fontsize=14)\n",
      "# # ax[1].set_title(\"Actual Clusters\",fontsize=16)\n",
      "\n",
      "# plt.show()\n",
      "185/60: print(labelcount)\n",
      "185/61:\n",
      "# This is where the functions are stored\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
      "185/62:\n",
      "np.random.seed(440)\n",
      "X = np.random.random((5,2))\n",
      "185/63:\n",
      "plt.figure(figsize=(6,6))\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "\n",
      "\n",
      "for i in range(0,5):\n",
      "    plt.text(X[i,0] + .01,X[i,1]+.01,str(i),fontsize=14)\n",
      "    \n",
      "    \n",
      "plt.show()\n",
      "185/64:\n",
      "# This is where the functions are stored\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
      "185/65:\n",
      "np.random.seed(440)\n",
      "X = np.random.random((5,2))\n",
      "185/66:\n",
      "plt.figure(figsize=(6,6))\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "\n",
      "\n",
      "for i in range(0,5):\n",
      "    plt.text(X[i,0] + .01,X[i,1]+.01,str(i),fontsize=14)\n",
      "    \n",
      "    \n",
      "plt.show()\n",
      "185/67:\n",
      "# You input the data\n",
      "# then tell the method how you want measure\n",
      "# closeness\n",
      "# here I've chose 'centroid'\n",
      "Z = linkage(X, 'centroid')\n",
      "\n",
      "# Z returns an array it is called a linkage vector\n",
      "# I'll use a dataframe to examine it\n",
      "# it returns four columns\n",
      "# that I'll describe below\n",
      "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])\n",
      "185/68:\n",
      "# I can plot the dendrogram like so\n",
      "# I input a linkage vector\n",
      "dendrogram(Z)\n",
      "\n",
      "plt.show()\n",
      "185/69:\n",
      "# This is where the functions are stored\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
      "185/70:\n",
      "np.random.seed(440)\n",
      "X = np.random.random((5,2))\n",
      "185/71:\n",
      "# I can plot the dendrogram like so\n",
      "# I input a linkage vector\n",
      "dendrogram(Z)\n",
      "\n",
      "plt.show()\n",
      "185/72:\n",
      "np.random.seed(440)\n",
      "X = np.random.random((5,2))\n",
      "print(X)\n",
      "185/73:\n",
      "plt.figure(figsize=(6,6))\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "\n",
      "\n",
      "for i in range(0,5):\n",
      "    plt.text(X[i,0] + .01,X[i,1]+.01,str(i),fontsize=14)\n",
      "    \n",
      "    \n",
      "plt.show()\n",
      "185/74:\n",
      "# We now use fcluster\n",
      "# Input the linkage vector\n",
      "# My distance cutpoint is .4\n",
      "# We set R = None and\n",
      "# monocrit = None\n",
      "# since we only want the list of clusters returned\n",
      "fcluster(Z, t=.4, criterion='distance')\n",
      "185/75:\n",
      "# A return to the synthetic data\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "185/76:\n",
      "# Code Here\n",
      "\n",
      "Z = linkage(X, 'centroid')\n",
      "\n",
      "# Z returns an array it is called a linkage vector\n",
      "# I'll use a dataframe to examine it\n",
      "# it returns four columns\n",
      "# that I'll describe below\n",
      "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])\n",
      "185/77:\n",
      "# Code Here\n",
      "\n",
      "Z = linkage(X, 'centroid')\n",
      "\n",
      "# Z returns an array it is called a linkage vector\n",
      "# I'll use a dataframe to examine it\n",
      "# it returns four columns\n",
      "# that I'll describe below\n",
      "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])\n",
      "\n",
      "print(X.shape)\n",
      "185/78:\n",
      "# Code Here\n",
      "\n",
      "Z = linkage(X, 'centroid')\n",
      "\n",
      "# Z returns an array it is called a linkage vector\n",
      "# I'll use a dataframe to examine it\n",
      "# it returns four columns\n",
      "# that I'll describe below\n",
      "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])\n",
      "\n",
      "print(X.shape)\n",
      "print(Z)\n",
      "185/79:\n",
      "# Code Here\n",
      "\n",
      "Z = linkage(X, 'centroid')\n",
      "\n",
      "# Z returns an array it is called a linkage vector\n",
      "# I'll use a dataframe to examine it\n",
      "# it returns four columns\n",
      "# that I'll describe below\n",
      "pd.DataFrame(Z,columns = ['cluster_1','cluster_2','distance','new_cluster_size'])\n",
      "\n",
      "print(X.shape)\n",
      "dendrogram(Z)\n",
      "\n",
      "plt.show()\n",
      "185/80:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=.4, criterion='distance')\n",
      "185/81:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=.8, criterion='distance')\n",
      "185/82:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=1, criterion='distance')\n",
      "185/83:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=.5, criterion='distance')\n",
      "185/84:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=5, criterion='distance')\n",
      "185/85:\n",
      "# Code Here\n",
      "\n",
      "fcluster(Z, t=5, criterion='distance')\n",
      "185/86: from sklearn.cluster import DBSCAN\n",
      "185/87:\n",
      "x = np.array([1.1, 1, .9, .7, 1.75, \n",
      "             2.2, 2.3, 2.35, 2.4])\n",
      "185/88:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "plt.scatter(x,np.ones(len(x)))\n",
      "\n",
      "plt.xlabel(\"$x$\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/89:\n",
      "cs = ['r','g','b']\n",
      "\n",
      "# Let's go through and. see what happens as we increase eps\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 2,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/90:\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "185/91: from sklearn.cluster import DBSCAN\n",
      "185/92:\n",
      "x = np.array([1.1, 1, .9, .7, 1.75, \n",
      "             2.2, 2.3, 2.35, 2.4])\n",
      "185/93:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "plt.scatter(x,np.ones(len(x)))\n",
      "\n",
      "plt.xlabel(\"$x$\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "185/94:\n",
      "# Code Here\n",
      "\n",
      "for e in X:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 2,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/95:\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X)\n",
      "185/96:\n",
      "# Code Here\n",
      "\n",
      "for e in X:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 2,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/97:\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X.shape)\n",
      "185/98:\n",
      "# Code Here\n",
      "\n",
      "for e in X:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/99:\n",
      "np.random.seed(614)\n",
      "X = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X.shape)\n",
      "185/100:\n",
      "# Code Here\n",
      "\n",
      "for e in X:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/101:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/102:\n",
      "# Code Here\n",
      "\n",
      "for e in X:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/103:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/104: # Code Here\n",
      "185/105: # Code Here\n",
      "185/106:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/107:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/108:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/109:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,2))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/110:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "#     pred = db.fit_predict(np.array(x).reshape(-1,2))\n",
      "#     pred_set = list(set(pred))\n",
      "#     pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/111:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(x).reshape(-1,2))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/112:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(x.mean[0]).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/113:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(x.average[0]).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/114:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(np.mean(x,1) ).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/115:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(np.mean(x,0) ).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/116:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "    \n",
      "    pred = db.fit_predict(np.array(np.mean(x,0)).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/117:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/118:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(np.mean(x,0)).reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/119:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1))\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/120:\n",
      "# Code Here\n",
      "\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/121:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in x:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/122:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in eps:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/123:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "#     for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(x[pred==pred_set[i]])),\n",
      "#                     c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "#     plt.legend(fontsize=12)\n",
      "#     plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "#     plt.show()\n",
      "185/124:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/125:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/126:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/127:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/128:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/129:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/130:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/131:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/132:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/133:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/134:\n",
      "# Code Here\n",
      "np.random.seed(614)\n",
      "#x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/135:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/136:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/137:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/138:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/139:\n",
      "np.random.seed(614)\n",
      "x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(x.shape)\n",
      "185/140:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/141:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/142:\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "185/143:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/144:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/145:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/146:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/147:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/148:\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "185/149:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/150:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/151:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(x[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/152:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(x.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/153:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/154:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/155:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/156:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/157:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/158: cs = ['r','g']\n",
      "185/159: #cs = ['r','g']\n",
      "185/160:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/161:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/162:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/163:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/164: # Code Here\n",
      "185/165:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/166:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/167:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/168:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/169:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/170:\n",
      "# This is where the functions are stored\n",
      "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
      "185/171:\n",
      "np.random.seed(440)\n",
      "X = np.random.random((5,2))\n",
      "print(X)\n",
      "185/172:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/173:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 2,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/174:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 1,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/175:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 2,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/176:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    c=cs[i],label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/177:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/178:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    label=\"Cluster \" + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/179:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "        plt.scatter(x[pred==pred_set[i]],\n",
      "                    np.ones(len(X1[pred==pred_set[i]])),\n",
      "                    + str(pred_set[i]))\n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/180:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i]], np.ones(len(X1[pred==pred_set[i]])))\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/181:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i]], np.ones(len(X1[pred==pred_set[i]])))\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/182:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i]], np.ones(len(X1[pred==pred_set[i]])))\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/183:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0], np.ones(len(X1[pred==pred_set[i]])))\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/184:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1] )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/185:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1] )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/186:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,1)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1] )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/187:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1] )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/188:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/189:\n",
      "#cs = ['r','g']\n",
      "cs = ['r','g','b']\n",
      "185/190:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    #print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/191:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.05,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/192:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [.005,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/193:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [2,.1,.15,.2,.5,1]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/194:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [1.5,1.8,2.0,2.1,3.1,4]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "185/195:\n",
      "# Code Here\n",
      "# np.random.seed(614)\n",
      "# #x = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "#                    [2,2] + 1.7*np.random.randn(100,2)])\n",
      "#x=(200,2)\n",
      "np.random.seed(614)\n",
      "X1 = np.concatenate([[-2,-2] + 1.7*np.random.randn(100,2),\n",
      "                   [2,2] + 1.7*np.random.randn(100,2)])\n",
      "print(X1.shape)\n",
      "\n",
      "for e in [1.5,1.8,2.0,2.1,2.8,4]:\n",
      "    plt.figure(figsize=(14,8))\n",
      "    db = DBSCAN(min_samples = 50,eps=e)\n",
      "\n",
      "    pred = db.fit_predict(np.array(X1.reshape(-1,2)) )\n",
      "    \n",
      "    print(pred)\n",
      "    \n",
      "    pred_set = list(set(pred))\n",
      "    \n",
      "    print(len(pred_set))\n",
      "    \n",
      "    pred_set.sort()\n",
      "    \n",
      "    for i in range(len(pred_set)):\n",
      "#         plt.scatter(x[pred==pred_set[i]],\n",
      "#                     np.ones(len(X1[pred==pred_set[i]])),\n",
      "#                     + str(pred_set[i]))\n",
      "        plt.scatter(X1[pred==pred_set[i],0],X1[pred==pred_set[i],1], c=cs[i],label=\"Cluster \" + str(pred_set[i]) )\n",
      "    \n",
      "    \n",
      "    plt.legend(fontsize=12)\n",
      "    plt.title(\"Epsilon is \" + str(e), fontsize=15)\n",
      "    plt.show()\n",
      "187/1:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "187/2:\n",
      "# Make some random data\n",
      "np.random.seed(440)\n",
      "\n",
      "x1 = 8*np.random.randn(500)\n",
      "x2 = 3*np.random.randn(500)\n",
      "\n",
      "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
      "\n",
      "angle = -np.pi/4\n",
      "\n",
      "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1])\n",
      "\n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "187/3:\n",
      "# PCA is stored in decomposition\n",
      "from sklearn.decomposition import PCA\n",
      "187/4:\n",
      "# make the PCA object\n",
      "# we'll project down to 2-D\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "# Fit the data\n",
      "pca.fit(X)\n",
      "187/5:\n",
      "def draw_vector(v0, v1, ax=None):\n",
      "    ax = ax or plt.gca()\n",
      "    arrowprops=dict(arrowstyle='->',\n",
      "                    linewidth=2,\n",
      "                    shrinkA=0, \n",
      "                    shrinkB=0,\n",
      "                    color=\"black\")\n",
      "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# plot data\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1], alpha=.6)\n",
      "\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",
      "    draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "187/6:\n",
      "# import the data\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "cancer = load_breast_cancer()\n",
      "X,y = load_breast_cancer(return_X_y = True)\n",
      "\n",
      "# test train split\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state = 614,shuffle = True,stratify = y)\n",
      "187/7:\n",
      "# Let's view the training data\n",
      "# Note this may take a bit.\n",
      "fig, axes = plt.subplots(15, 2, figsize = (12,30))\n",
      "\n",
      "ax = axes.ravel()\n",
      "\n",
      "for i in range(30):\n",
      "    _, bins = np.histogram(X_train[:, i], bins = 50)\n",
      "    ax[i].hist(X_train[y_train == 1, i], bins = bins, color = 'red', alpha = .5)\n",
      "    ax[i].hist(X_train[y_train == 0, i], bins = bins, color = 'blue', alpha = .5)\n",
      "    ax[i].set_title(\"feature \" + str(i) + \": \" + cancer.feature_names[i])\n",
      "    ax[i].set_yticks(())\n",
      "\n",
      "ax[0].legend(['malignant', 'benign'], loc = 'best')\n",
      "    \n",
      "fig.tight_layout()\n",
      "187/8:\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "187/9:\n",
      "pca_pipe = Pipeline([('scaler',StandardScaler()),\n",
      "                ('pca',PCA(n_components=2))])\n",
      "187/10: X_pca = pca_pipe.fit_transform(X_train)\n",
      "187/11:\n",
      "plt.figure(figsize = (8,8))\n",
      "\n",
      "plt.scatter(X_pca[y_train==0,0],X_pca[y_train==0,1],c='blue',label='benign')\n",
      "plt.scatter(X_pca[y_train==1,0],X_pca[y_train==1,1],c='red',label='malignant')\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "plt.xlabel(\"pca 1\",fontsize=16)\n",
      "plt.ylabel(\"pca 2\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "187/12: # Code here\n",
      "187/13:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "187/14:\n",
      "# Make some random data\n",
      "np.random.seed(440)\n",
      "\n",
      "x1 = 8*np.random.randn(500)\n",
      "x2 = 3*np.random.randn(500)\n",
      "\n",
      "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
      "\n",
      "angle = -np.pi/4\n",
      "\n",
      "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1])\n",
      "\n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "187/15:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "187/16:\n",
      "# Make some random data\n",
      "np.random.seed(440)\n",
      "\n",
      "x1 = 8*np.random.randn(500)\n",
      "x2 = 3*np.random.randn(500)\n",
      "\n",
      "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
      "\n",
      "angle = -np.pi/4\n",
      "\n",
      "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1])\n",
      "\n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "187/17:\n",
      "# PCA is stored in decomposition\n",
      "from sklearn.decomposition import PCA\n",
      "187/18:\n",
      "# make the PCA object\n",
      "# we'll project down to 2-D\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "# Fit the data\n",
      "pca.fit(X)\n",
      "187/19:\n",
      "def draw_vector(v0, v1, ax=None):\n",
      "    ax = ax or plt.gca()\n",
      "    arrowprops=dict(arrowstyle='->',\n",
      "                    linewidth=2,\n",
      "                    shrinkA=0, \n",
      "                    shrinkB=0,\n",
      "                    color=\"black\")\n",
      "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# plot data\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1], alpha=.6)\n",
      "\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",
      "    draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "187/20:\n",
      "# import the data\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "cancer = load_breast_cancer()\n",
      "X,y = load_breast_cancer(return_X_y = True)\n",
      "\n",
      "# test train split\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state = 614,shuffle = True,stratify = y)\n",
      "187/21:\n",
      "# import the data\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "cancer = load_breast_cancer()\n",
      "X,y = load_breast_cancer(return_X_y = True)\n",
      "\n",
      "# test train split\n",
      "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state = 614,shuffle = True,stratify = y)\n",
      "187/22:\n",
      "# Let's view the training data\n",
      "# Note this may take a bit.\n",
      "fig, axes = plt.subplots(15, 2, figsize = (12,30))\n",
      "\n",
      "ax = axes.ravel()\n",
      "\n",
      "for i in range(30):\n",
      "    _, bins = np.histogram(X_train[:, i], bins = 50)\n",
      "    ax[i].hist(X_train[y_train == 1, i], bins = bins, color = 'red', alpha = .5)\n",
      "    ax[i].hist(X_train[y_train == 0, i], bins = bins, color = 'blue', alpha = .5)\n",
      "    ax[i].set_title(\"feature \" + str(i) + \": \" + cancer.feature_names[i])\n",
      "    ax[i].set_yticks(())\n",
      "\n",
      "ax[0].legend(['malignant', 'benign'], loc = 'best')\n",
      "    \n",
      "fig.tight_layout()\n",
      "187/23:\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "187/24:\n",
      "pca_pipe = Pipeline([('scaler',StandardScaler()),\n",
      "                ('pca',PCA(n_components=2))])\n",
      "187/25: X_pca = pca_pipe.fit_transform(X_train)\n",
      "187/26:\n",
      "plt.figure(figsize = (8,8))\n",
      "\n",
      "plt.scatter(X_pca[y_train==0,0],X_pca[y_train==0,1],c='blue',label='benign')\n",
      "plt.scatter(X_pca[y_train==1,0],X_pca[y_train==1,1],c='red',label='malignant')\n",
      "\n",
      "plt.legend(fontsize=14)\n",
      "plt.xlabel(\"pca 1\",fontsize=16)\n",
      "plt.ylabel(\"pca 2\",fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "187/27:\n",
      "# Code here \n",
      "\n",
      "pca.fit(X_pca)\n",
      "187/28:\n",
      "# Code here \n",
      "\n",
      "def draw_vector(v0, v1, ax=None):\n",
      "    ax = ax or plt.gca()\n",
      "    arrowprops=dict(arrowstyle='->',\n",
      "                    linewidth=2,\n",
      "                    shrinkA=0, \n",
      "                    shrinkB=0,\n",
      "                    color=\"black\")\n",
      "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# plot data\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "    v = vector * 3 * np.sqrt(length)\n",
      "    draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "187/29:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "187/30: nba = pd.read_csv(\"nba_team_shots.csv\")\n",
      "187/31: nba.head()\n",
      "187/32:\n",
      "# Make a PCA object\n",
      "pca = PCA(n_components = 2)\n",
      "187/33:\n",
      "# fit it on the desired columns\n",
      "nba_pca = pca.fit_transform(nba[['zone_' + str(i) + '_perc_att' for i in range(1,16)]])\n",
      "\n",
      "nba['pca_1'] = nba_pca[:,0]\n",
      "nba['pca_2'] = nba_pca[:,1]\n",
      "187/34:\n",
      "pd.DataFrame({'pca_1':pca.components_[0,:],\n",
      "              'pca_2':pca.components_[1,:]},\n",
      "            index = ['zone_' + str(i) + '_perc_att' for i in range(1,16)]).sort_values('pca_1')\n",
      "187/35:\n",
      "pd.DataFrame({'pca_1':pca.components_[0,:],\n",
      "              'pca_2':pca.components_[1,:]},\n",
      "            index = ['zone_' + str(i) + '_perc_att' for i in range(1,16)]).sort_values('pca_2')\n",
      "187/36:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,y_pca,\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/37:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,X_pca[y_train==0,:],\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/38:\n",
      "X_pca = pca_pipe.fit_transform(X_train)\n",
      "X_pca\n",
      "187/39:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca[y_train==:,0],X_pca[y_train==:,1],\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/40:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca[y_train==0:1,0],X_pca[y_train==0:1,1],\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/41: print(pca.explained_variance_ratio_)\n",
      "187/42: X = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
      "187/43:\n",
      "pca = PCA(n_components=200)\n",
      "\n",
      "pca.fit(X[X.columns[1:]])\n",
      "187/44:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/45:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "[X_pca[y_train==0,0],X_pca[y_train==1,0]].shape()\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "187/46:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca[y_train==0,0],X_pca[:,1]),\n",
      "                                                 test_size = .2,\n",
      "                                                 random_state = 614,\n",
      "                                                 shuffle = True,\n",
      "                                                 stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/47:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca[y_train==0,0],X_pca[:,1],\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/48:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca[:,0],X_pca[:,1],\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/49:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "187/50:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "\n",
      "print(X_pca)\n",
      "187/51:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "\n",
      "print(X_pca[y_train==0,0])\n",
      "190/1: X = 10*np.random.randn(1000) + 20\n",
      "190/2:\n",
      "# let's make a train test split\n",
      "from sklearn.model_selection import train_test_split\n",
      "190/3:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "190/4: X = 10*np.random.randn(1000) + 20\n",
      "190/5:\n",
      "# let's make a train test split\n",
      "from sklearn.model_selection import train_test_split\n",
      "190/6: X_train, X_test = train_test_split(X, test_size = .25, random_state = 440, shuffle=True)\n",
      "190/7:\n",
      "# look at the means of the test and train\n",
      "print(\"Train Mean\",np.mean(X_train))\n",
      "print(\"Test Mean\",np.mean(X_test))\n",
      "print(\"Train SD\",np.std(X_train))\n",
      "print(\"Test SD\",np.std(X_test))\n",
      "190/8: from sklearn.preprocessing import StandardScaler\n",
      "190/9:\n",
      "# now we scale\n",
      "scaler = StandardScaler()\n",
      "\n",
      "X_train_scale = scaler.fit_transform(X_train.reshape(-1,1))\n",
      "X_test_scale = scaler.transform(X_test.reshape(-1,1))\n",
      "190/10:\n",
      "print(\"Train Mean\",np.mean(X_train_scale))\n",
      "print(\"Test Mean\",np.mean(X_test_scale))\n",
      "print(\"Train SD\",np.std(X_train_scale))\n",
      "print(\"Test SD\",np.std(X_test_scale))\n",
      "190/11:\n",
      "# now we scale\n",
      "scaler = StandardScaler()\n",
      "\n",
      "perturb = 100\n",
      "\n",
      "X_train_scale = scaler.fit_transform(X_train.reshape(-1,1))\n",
      "X_test_scale = scaler.transform(X_test.reshape(-1,1) + perturb)\n",
      "\n",
      "print(\"Train Mean\",np.mean(X_train_scale))\n",
      "print(\"Test Mean\",np.mean(X_test_scale))\n",
      "print(\"Train SD\",np.std(X_train_scale))\n",
      "print(\"Test SD\",np.std(X_test_scale))\n",
      "190/12:\n",
      "# now we scale\n",
      "scaler = StandardScaler()\n",
      "\n",
      "perturb = 100\n",
      "\n",
      "X_train_scale = scaler.fit_transform(X_train.reshape(-1,1))\n",
      "X_test_scale = scaler.transform(X_test.reshape(-1,1) + perturb)\n",
      "\n",
      "print(\"Train Mean\",np.mean(X_train_scale))\n",
      "print(\"Test Mean\",np.mean(X_test_scale))\n",
      "print(\"Train SD\",np.std(X_train_scale))\n",
      "print(\"Test SD\",np.std(X_test_scale))\n",
      "190/13:\n",
      "# Make data\n",
      "X = np.array([2,4])*np.random.randn(100,2) + [-1,2]\n",
      "\n",
      "# I need to scale my data\n",
      "scaler = StandardScaler()\n",
      "\n",
      "X_scaled = scaler.fit_transform(X)\n",
      "\n",
      "# Now I do a train test split\n",
      "X_train, X_test = train_test_split(X_scaled,test_size=.2,random_state=44,shuffle=True)\n",
      "190/14:\n",
      "## This line of code is correct!\n",
      "from sklearn.decomposition import PCA\n",
      "190/15:\n",
      "# Make Data\n",
      "X = np.random.randn(1000,50) + np.random.randint(-100,100,(1000,50))\n",
      "\n",
      "# train test split\n",
      "X_train, X_test = train_test_split(X,test_size=.1,shuffle=True,random_state=44)\n",
      "\n",
      "# I want to perform PCA\n",
      "pca = PCA(n_components=10)\n",
      "\n",
      "X_train_pca = pca.fit_transform(X_train)\n",
      "X_test_pca = pca.fit_transform(X_test)\n",
      "190/16:\n",
      "## This chunk of code is correct!\n",
      "from sklearn.preprocessing import PolynomialFeatures,FunctionTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "187/52:\n",
      "X_pca = pca_pipe.fit_transform(X_train)\n",
      "y_train\n",
      "#X_pca\n",
      "187/53:\n",
      "X_pca = pca_pipe.fit_transform(X_train)\n",
      "y_train.shape()\n",
      "#X_pca\n",
      "187/54:\n",
      "X_pca = pca_pipe.fit_transform(X_train)\n",
      "y_train.shape\n",
      "#X_pca\n",
      "187/55:\n",
      "X_pca = pca_pipe.fit_transform(X_train)\n",
      "X_pca\n",
      "y_train.shape\n",
      "#X_pca\n",
      "187/56:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/57:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "\n",
      "print(X_pca[y_train==0,0])\n",
      "187/58:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "\n",
      "# print(X_pca[y_train==0,0])\n",
      "187/59:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/60:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .1,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/61:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train,X_test,y_train,y_test = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/62:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_tr,X_te,y_tr,y_te = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/63:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "X_pca.shape\n",
      "y_train.shape\n",
      "# print(X_pca[y_train==0,0])\n",
      "187/64:\n",
      "# Code here \n",
      "\n",
      "# def draw_vector(v0, v1, ax=None):\n",
      "#     ax = ax or plt.gca()\n",
      "#     arrowprops=dict(arrowstyle='->',\n",
      "#                     linewidth=2,\n",
      "#                     shrinkA=0, \n",
      "#                     shrinkB=0,\n",
      "#                     color=\"black\")\n",
      "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
      "\n",
      "# # plot data\n",
      "# plt.figure(figsize=(10,8))\n",
      "# plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=.6)\n",
      "\n",
      "# for length, vector in zip(pca.explained_variance_, pca.components_):\n",
      "#     v = vector * 3 * np.sqrt(length)\n",
      "#     draw_vector(pca.mean_, pca.mean_ + v)\n",
      "    \n",
      "    \n",
      "# plt.xlabel(\"$x_1$\", fontsize=14)\n",
      "# plt.ylabel(\"$x_2$\", fontsize=14)\n",
      "\n",
      "# plt.axis('equal')\n",
      "# plt.show()\n",
      "\n",
      "# X_pca_train=np.concatenate(X_pca[y_train==0,0],X_pca[y_train==0,1])\n",
      "print(X_pca.shape)\n",
      "print(y_train.shape)\n",
      "# print(X_pca[y_train==0,0])\n",
      "187/65:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_tr,X_te,y_tr,y_te = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True,\n",
      "                                                stratify = y)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/66:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_tr,X_te,y_tr,y_te = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/67:\n",
      "# Code here \n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_tr,X_te,y_tr,y_te = train_test_split(X_pca,y_train,\n",
      "                                                test_size = .2,\n",
      "                                                random_state = 614,\n",
      "                                                shuffle = True)\n",
      "\n",
      "\n",
      "\n",
      "# X_train,X_test,y_train,y_test = train_test_split([X_pca[y_train==0,0],X_pca[y_train==1,0]],[X_pca[y_train==0,1],X_pca[y_train==1,1]],\n",
      "#                                                  test_size = .2,\n",
      "#                                                  random_state = 614,\n",
      "#                                                  shuffle = True,\n",
      "#                                                  stratify = y)\n",
      "187/68:\n",
      "# Code here \n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "log_reg = LogisticRegression()\n",
      "190/17:\n",
      "# make data\n",
      "X = np.zeros((1000,9))\n",
      "\n",
      "X[:,0] = np.random.randint(0,3,1000)\n",
      "X[:,1:8] = np.random.randn(1000,7)\n",
      "X[:,8] = X[:,1] + 2*X[:,3] - 4*X[:,6] + np.random.randn(1000)\n",
      "190/18:\n",
      "# We first create this function that takes in \n",
      "# X and makes one hot encoded columns\n",
      "def get_X_ready(X):\n",
      "    new_X = np.zeros((np.shape(X)[0],10))\n",
      "    \n",
      "    # one hot encode\n",
      "    new_X[X[:,0]==0,0] = 1\n",
      "    new_X[X[:,0]==1,1] = 1\n",
      "\n",
      "    # copy the rest\n",
      "    new_X[:,2:] = X[:,1:]\n",
      "    \n",
      "    return new_X\n",
      "190/19:\n",
      "# This allows you to maek\n",
      "# a custom transformer\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "190/20:\n",
      "# Define our custom transformer\n",
      "# It should take in our X with the one hot encoded columns\n",
      "# and return the scaled continuous columns \n",
      "class Scaler(BaseEstimator, TransformerMixin):\n",
      "    #Class Constructor \n",
      "    # This allows you to initiate the class when you call\n",
      "    # Scaler\n",
      "    def __init__(self):\n",
      "        # I want to initiate each object with\n",
      "        # the StandardScaler method\n",
      "        self.StandardScaler = StandardScaler()\n",
      "    \n",
      "    # For my fit method I'm just going to \"steal\"\n",
      "    # StandardScaler's fit method using only the\n",
      "    # columns I want\n",
      "    def fit(self, X, y = None ):\n",
      "        self.StandardScaler.fit(X[:,2:])\n",
      "        return self\n",
      "    \n",
      "    # Now I want to transform the columns I want\n",
      "    # and return it with scaled columns\n",
      "    def transform(self, X, y = None):\n",
      "        X[:,2:] = self.StandardScaler.transform(X[:,2:])\n",
      "        return X\n",
      "190/21:\n",
      "from sklearn.decomposition import PCA\n",
      "# we now make a custom PCA transform\n",
      "class CustomPCA(BaseEstimator, TransformerMixin):\n",
      "    #Class Constructor \n",
      "    # This allows you to initiate the class when you call\n",
      "    # CustomPCA\n",
      "    def __init__(self):\n",
      "        # I want to initiate each object with\n",
      "        # the PCA method\n",
      "        self.PCA = PCA()\n",
      "    \n",
      "    # For my fit method I'm just going to \"steal\"\n",
      "    # PCA's fit method using only the\n",
      "    # columns I want\n",
      "    def fit(self, X, y = None ):\n",
      "        self.PCA.fit(X[:,2:])\n",
      "        return self\n",
      "    \n",
      "    # Now I want to transform the columns\n",
      "    # and return it with PCA\n",
      "    def transform(self, X, y = None):\n",
      "        X[:,2:] = self.PCA.transform(X[:,2:])\n",
      "        return X\n",
      "190/22:\n",
      "# Now we put it all together with a pipe\n",
      "pipe = Pipeline([('get_X_ready',FunctionTransformer(get_X_ready)),\n",
      "                ('scale',Scaler()),\n",
      "                ('pca',CustomPCA())])\n",
      "190/23:\n",
      "# train test split\n",
      "X_train, X_test = train_test_split(X,test_size=.1,shuffle=True,random_state=44)\n",
      "190/24:\n",
      "# Processed training set\n",
      "X_train_processed = pipe.fit_transform(X_train)\n",
      "190/25:\n",
      "# Processed testing set\n",
      "X_test_processed = pipe.transform(X_test)\n",
      "190/26:\n",
      "# We first create this function that takes in \n",
      "# X and makes one hot encoded columns\n",
      "def get_X_ready(X):\n",
      "    new_X = np.zeros((np.shape(X)[0],10))\n",
      "    \n",
      "    # one hot encode\n",
      "    new_X[X[:,0]==0,0] = 1\n",
      "    new_X[X[:,0]==1,1] = 1\n",
      "\n",
      "    # copy the rest\n",
      "    new_X[:,2:] = X[:,1:]\n",
      "    \n",
      "    return new_X\n",
      "190/27:\n",
      "# This allows you to maek\n",
      "# a custom transformer\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "190/28:\n",
      "# This allows you to maek\n",
      "# a custom transformer\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "190/29:\n",
      "from sklearn.decomposition import PCA\n",
      "# we now make a custom PCA transform\n",
      "class CustomPCA(BaseEstimator, TransformerMixin):\n",
      "    #Class Constructor \n",
      "    # This allows you to initiate the class when you call\n",
      "    # CustomPCA\n",
      "    def __init__(self):\n",
      "        # I want to initiate each object with\n",
      "        # the PCA method\n",
      "        self.PCA = PCA()\n",
      "    \n",
      "    # For my fit method I'm just going to \"steal\"\n",
      "    # PCA's fit method using only the\n",
      "    # columns I want\n",
      "    def fit(self, X, y = None ):\n",
      "        self.PCA.fit(X[:,2:])\n",
      "        return self\n",
      "    \n",
      "    # Now I want to transform the columns\n",
      "    # and return it with PCA\n",
      "    def transform(self, X, y = None):\n",
      "        X[:,2:] = self.PCA.transform(X[:,2:])\n",
      "        return X\n",
      "190/30:\n",
      "# test mean\n",
      "np.mean(X_test_processed[:,2:],axis=0)\n",
      "190/31:\n",
      "# a processing function\n",
      "def process(X):\n",
      "    process_X = np.zeros((np.shape(X)[0],4))\n",
      "    \n",
      "    process_X[:,0] = X[:,0]\n",
      "    process_X[:,1] = np.sqrt(X[:,1])\n",
      "    process_X[:,2] = X[:,2]\n",
      "    \n",
      "    scale = StandardScaler()\n",
      "    process_X[:,1:3] = scale.fit_transform(X[:,1:3])\n",
      "    \n",
      "    process_X[:,3] = process_X[:,0]*process_X[:,1]\n",
      "    \n",
      "    \n",
      "    return process_X\n",
      "192/1:\n",
      "def make_hello(N=1000, rseed=42):\n",
      "    # Make a plot with \"HELLO\" text; save as PNG\n",
      "    fig, ax = plt.subplots(figsize=(4, 1))\n",
      "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
      "    ax.axis('off')\n",
      "    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)\n",
      "    fig.savefig('hello.png')\n",
      "    plt.close(fig)\n",
      "    \n",
      "    # Open this PNG and draw random points from it\n",
      "    from matplotlib.image import imread\n",
      "    data = imread('hello.png')[::-1, :, 0].T\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    X = rng.rand(4 * N, 2)\n",
      "    i, j = (X * data.shape).astype(int).T\n",
      "    mask = (data[i, j] < 1)\n",
      "    X = X[mask]\n",
      "    X[:, 0] *= (data.shape[0] / data.shape[1])\n",
      "    X = X[:N]\n",
      "    return X[np.argsort(X[:, 0])]\n",
      "192/2:\n",
      "# Make our data\n",
      "X = make_hello(1000)\n",
      "192/3:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import mpl_toolkits.mplot3d.axes3d as p3\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"whitegrid\")\n",
      "192/4:\n",
      "def make_hello(N=1000, rseed=42):\n",
      "    # Make a plot with \"HELLO\" text; save as PNG\n",
      "    fig, ax = plt.subplots(figsize=(4, 1))\n",
      "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
      "    ax.axis('off')\n",
      "    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)\n",
      "    fig.savefig('hello.png')\n",
      "    plt.close(fig)\n",
      "    \n",
      "    # Open this PNG and draw random points from it\n",
      "    from matplotlib.image import imread\n",
      "    data = imread('hello.png')[::-1, :, 0].T\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    X = rng.rand(4 * N, 2)\n",
      "    i, j = (X * data.shape).astype(int).T\n",
      "    mask = (data[i, j] < 1)\n",
      "    X = X[mask]\n",
      "    X[:, 0] *= (data.shape[0] / data.shape[1])\n",
      "    X = X[:N]\n",
      "    return X[np.argsort(X[:, 0])]\n",
      "192/5:\n",
      "# Make our data\n",
      "X = make_hello(1000)\n",
      "192/6:\n",
      "# Plot it in 2D\n",
      "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X[:, 0], X[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/7:\n",
      "# More code from Jake\n",
      "def rotate(X, angle):\n",
      "    theta = np.deg2rad(angle)\n",
      "    R = [[np.cos(theta), np.sin(theta)],\n",
      "         [-np.sin(theta), np.cos(theta)]]\n",
      "    return np.dot(X, R)\n",
      "    \n",
      "X2 = rotate(X, 20) + 5\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X2[:, 0], X2[:, 1], **colorize)\n",
      "plt.axis('equal');\n",
      "192/8:\n",
      "# MDS is stored in\n",
      "from sklearn.manifold import MDS\n",
      "192/9:\n",
      "# MDS is stored in\n",
      "from sklearn.manifold import MDS\n",
      "192/10:\n",
      "# Set the number of output dimensions to be 2\n",
      "mds = MDS(n_components = 2)\n",
      "X_mds = mds.fit_transform(X2)\n",
      "192/11:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_mds[:, 0], X_mds[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/12:\n",
      "# What about for 1D?\n",
      "mds = MDS(n_components = 1)\n",
      "X_mds = mds.fit_transform(X2)\n",
      "192/13:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_mds[:, 0], np.ones(len(X[:,0])), **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/14:\n",
      "# More code from Jake\n",
      "def random_projection(X, dimension=3, rseed=42):\n",
      "    assert dimension >= X.shape[1]\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    C = rng.randn(dimension, dimension)\n",
      "    e, V = np.linalg.eigh(np.dot(C, C.T))\n",
      "    return np.dot(X, V[:X.shape[1]])\n",
      "    \n",
      "X3 = random_projection(X, 3)\n",
      "X3.shape\n",
      "\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "ax = plt.axes(projection='3d')\n",
      "ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],\n",
      "             **colorize)\n",
      "ax.view_init(azim=70, elev=50)\n",
      "192/15:\n",
      "# apply MDS to X3 here.\n",
      "mds = MDS(n_components = 3)\n",
      "X_mds = mds.fit_transform(X2)\n",
      "192/16:\n",
      "# More code from Jake\n",
      "def random_projection(X, dimension=3, rseed=42):\n",
      "    assert dimension >= X.shape[1]\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    C = rng.randn(dimension, dimension)\n",
      "    e, V = np.linalg.eigh(np.dot(C, C.T))\n",
      "    return np.dot(X, V[:X.shape[1]])\n",
      "    \n",
      "X3 = random_projection(X, 3)\n",
      "X3.shape\n",
      "\n",
      "plt.figure(figsize=(10,10))\n",
      "\n",
      "ax = plt.axes(projection='3d')\n",
      "ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],\n",
      "             **colorize)\n",
      "ax.view_init(azim=70, elev=50)\n",
      "192/17:\n",
      "# apply MDS to X3 here.\n",
      "mds = MDS(n_components = 3)\n",
      "X_mds = mds.fit_transform(X3)\n",
      "192/18:\n",
      "# plot the result in 2D here\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_mds[:, 0], X_mds[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/19:\n",
      "# Thanks again for more code Jake\n",
      "def make_hello_s_curve(X):\n",
      "    t = (X[:, 0] - 2) * 0.75 * np.pi\n",
      "    x = np.sin(t)\n",
      "    y = X[:, 1]\n",
      "    z = np.sign(t) * (np.cos(t) - 1)\n",
      "    return np.vstack((x, y, z)).T\n",
      "\n",
      "XS = make_hello_s_curve(X)\n",
      "\n",
      "plt.figure(figsize=(12,12))\n",
      "\n",
      "ax = plt.axes(projection='3d')\n",
      "ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],\n",
      "             **colorize);\n",
      "192/20:\n",
      "# apply MDS to XS here.\n",
      "\n",
      "mds = MDS(n_components = 3)\n",
      "X_mds = mds.fit_transform(XS)\n",
      "192/21:\n",
      "# plot the result in 2D here\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_mds[:, 0], X_mds[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/22:\n",
      "# plot the PCA transform on X3 here\n",
      "\n",
      "\n",
      "pca.fit(X)\n",
      "192/23:\n",
      "# As a comparison perform PCA on X3 and XS\n",
      "# plot the comparisons below in 2D\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "192/24:\n",
      "# plot the PCA transform on X3 here\n",
      "\n",
      "\n",
      "pca.fit(X3)\n",
      "192/25:\n",
      "# plot the PCA transform on X3 here\n",
      "\n",
      "\n",
      "pca.fit(X3)\n",
      "\n",
      "X_pca = pca.fit_transform(X3)\n",
      "192/26:\n",
      "# plot the PCA transform on X3 here\n",
      "\n",
      "\n",
      "pca.fit(X3)\n",
      "\n",
      "X_pca = pca.fit_transform(X3)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/27:\n",
      "# plot the PCA transform on XS here\n",
      "\n",
      "pca.fit(XS)\n",
      "\n",
      "X_pca = pca.fit_transform(XS)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/28:\n",
      "# import it from manifold\n",
      "from sklearn.manifold import LocallyLinearEmbedding\n",
      "192/29:\n",
      "# We use a modified version of the algorithm to get nice\n",
      "# results. Check out the docs to see what that means\n",
      "lle = LocallyLinearEmbedding(n_neighbors=100, n_components=2,method='modified',\n",
      "                               eigen_solver='dense')\n",
      "X_lle = lle.fit_transform(XS)\n",
      "192/30:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_lle[:, 0], X_lle[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/31:\n",
      "# we import isomap from manifold\n",
      "from sklearn.manifold import Isomap\n",
      "192/32:\n",
      "# we import isomap from manifold\n",
      "from sklearn.manifold import Isomap\n",
      "192/33:\n",
      "# you control the number\n",
      "iso = Isomap(n_neighbors=160, n_components=2)\n",
      "X_iso = iso.fit_transform(XS)\n",
      "192/34:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/35: from sklearn.manifold import TSNE\n",
      "192/36:\n",
      "# we set perplexity = 50\n",
      "tsne = TSNE(n_components = 2, perplexity=50, random_state = 440)\n",
      "X_tsne = tsne.fit_transform(XS)\n",
      "192/37:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/38: from sklearn.manifold import TSNE\n",
      "192/39:\n",
      "# we set perplexity = 50\n",
      "tsne = TSNE(n_components = 2, perplexity=50, random_state = 440)\n",
      "X_tsne = tsne.fit_transform(XS)\n",
      "192/40:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/41:\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "192/42:\n",
      "## Explore what happens when you change the number of \n",
      "## number of neighbors in the local linear embedding\n",
      "## on the XS data\n",
      "tsne=[]\n",
      "for per in range(5):\n",
      "    tsne.append(TSNE(n_components = 2, perplexity=per*10+5, random_state = 440))\n",
      "    \n",
      "for per in range(5):\n",
      "    X_tsne = tsne[per].fit_transform(XS)\n",
      "    plt.figure(figsize=(10,8))\n",
      "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
      "    plt.axis('equal')\n",
      "\n",
      "    plt.show()\n",
      "192/43:\n",
      "## Explore what happens when you change the number of \n",
      "## number of neighbors in the isomap on the XS data\n",
      "\n",
      "#X_iso = iso.fit_transform(XS)\n",
      "\n",
      "iso=[]\n",
      "for per in range(5):\n",
      "    iso.append(iso = Isomap(n_neighbors=160, n_components=2))\n",
      "    \n",
      "for per in range(5):\n",
      "    X_iso = iso[per].fit_transform(XS)\n",
      "    plt.figure(figsize=(10,8))\n",
      "    plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
      "    plt.axis('equal')\n",
      "\n",
      "    plt.show()\n",
      "192/44:\n",
      "## Explore what happens when you change the number of \n",
      "## number of neighbors in the isomap on the XS data\n",
      "\n",
      "#X_iso = iso.fit_transform(XS)\n",
      "\n",
      "iso=[]\n",
      "for per in range(5):\n",
      "    iso.append(iso = Isomap(n_neighbors=160, n_components=2))\n",
      "    \n",
      "for per in range(5):\n",
      "    X_iso = iso[per].fit_transform(XS)\n",
      "    plt.figure(figsize=(10,8))\n",
      "    plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
      "    #plt.axis('equal')\n",
      "\n",
      "    plt.show()\n",
      "192/45:\n",
      "## Explore what happens when you change the number of \n",
      "## number of neighbors in the isomap on the XS data\n",
      "\n",
      "#X_iso = iso.fit_transform(XS)\n",
      "\n",
      "iso=[]\n",
      "for per in range(5):\n",
      "    iso.append(Isomap(n_neighbors=per*100+40, n_components=2))\n",
      "    \n",
      "for per in range(5):\n",
      "    X_iso = iso[per].fit_transform(XS)\n",
      "    plt.figure(figsize=(10,8))\n",
      "    plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
      "    #plt.axis('equal')\n",
      "\n",
      "    plt.show()\n",
      "194/1:\n",
      "## For data handling\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "## For plotting\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "## This sets the plot style\n",
      "## to have a grid on a white background\n",
      "sns.set_style(\"white\")\n",
      "194/2:\n",
      "nums = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
      "\n",
      "X = np.array(nums.iloc[:,1:])\n",
      "y = np.array(nums.iloc[:,0])\n",
      "194/3:\n",
      "# Remember what the data looks like\n",
      "fig,ax = plt.subplots(5,2,figsize=(16,40))\n",
      "\n",
      "for i in range(10):\n",
      "    ax[i//2,i%2].imshow(X[i,:].reshape(28, 28), cmap='gray_r')\n",
      "    ax[i//2,i%2].text(1,1,str(y[i]),fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/4:\n",
      "# Note this can take a long time.\n",
      "from sklearn.datasets import fetch_lfw_people\n",
      "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
      "194/5:\n",
      "## Code from Chapter 3\n",
      "image_shape = people.images[0].shape\n",
      "fig, ax = plt.subplots(2,5, figsize=(15,8),\n",
      "                      subplot_kw = {'xticks':(),'yticks':()})\n",
      "\n",
      "for target, image, ax in zip(people.target, people.images, ax.ravel()):\n",
      "    ax.imshow(image)\n",
      "    ax.set_title(people.target_names[target])\n",
      "    \n",
      "plt.show()\n",
      "194/6:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "194/7:\n",
      "X2,X3,y2,y3 = train_test_split(X,y,train_size = 2000, shuffle = True, random_state=440, stratify=y)\n",
      "\n",
      "# we won't need these\n",
      "del X3,y3\n",
      "194/8: from sklearn.model_selection import train_test_split\n",
      "194/9:\n",
      "X2,X3,y2,y3 = train_test_split(X,y,train_size = 2000, shuffle = True, random_state=440, stratify=y)\n",
      "\n",
      "# we won't need these\n",
      "del X3,y3\n",
      "194/10: len(X2)\n",
      "194/11:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "194/12:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(range(1,201),\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance Explained\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/13:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance Explained\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/14:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(range(1,len(X2)),\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/15:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(range(len(X2)),\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/16:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = len(X2))\n",
      "\n",
      "pca.fit(X2)\n",
      "194/17:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(range(len(X2)),\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/18:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = len(X2))\n",
      "\n",
      "pca.fit(X2)\n",
      "194/19:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = len(X2))\n",
      "\n",
      "pca.fit(X2)\n",
      "print(pca.explained_variance_ratio_)\n",
      "194/20:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = len(X2))\n",
      "\n",
      "pca.fit(X2)\n",
      "# print(pca.explained_variance_ratio_)\n",
      "194/21: from sklearn.model_selection import train_test_split\n",
      "194/22:\n",
      "X2,X3,y2,y3 = train_test_split(X,y,train_size = 2000, shuffle = True, random_state=440, stratify=y)\n",
      "\n",
      "# we won't need these\n",
      "del X3,y3\n",
      "194/23: len(X2)\n",
      "194/24:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = len(X2))\n",
      "\n",
      "pca.fit(X2)\n",
      "# print(pca.explained_variance_ratio_)\n",
      "194/25:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 200)\n",
      "\n",
      "pca.fit(X2)\n",
      "# print(pca.explained_variance_ratio_)\n",
      "194/26:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "# print(pca.explained_variance_ratio_)\n",
      "194/27:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "print(pca.explained_variance_ratio_)\n",
      "194/28:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 200)\n",
      "\n",
      "pca.fit(X2)\n",
      "# print(pca.explained_variance_ratio_)\n",
      "194/29:\n",
      "## Code here\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.plot(range(200),\n",
      "        np.cumsum(pca.explained_variance_ratio_))\n",
      "\n",
      "plt.xlabel(\"PCA Component\", fontsize=16)\n",
      "plt.ylabel(\"Cumulative Variance\", fontsize=16)\n",
      "\n",
      "plt.show()\n",
      "194/30:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 200)\n",
      "\n",
      "pca.fit(X2)\n",
      "194/31:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "194/32:\n",
      "## Code here\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/33:\n",
      "## Code here\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/34:\n",
      "## Code here\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/35:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/36:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/37:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "colorize = dict(c=X_pca[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/38:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "colorize = dict(c=X_pca[:, 0], cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/39:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "colorize = dict(c=X_pca[:, 0], cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/40:\n",
      "## Code here\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "\n",
      "pca.fit(X2)\n",
      "194/41: ## Code here\n",
      "194/42:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "colorize = dict(c=X_pca[:, 0], cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/43:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "colorize = dict(c=y2[:, 0], cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/44:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "colorize = dict(c=y2, cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/45:\n",
      "## Code here\n",
      "# Plot it in 2D\n",
      "\n",
      "X_pca = pca.fit_transform(X2)\n",
      "\n",
      "#colorize = dict(c=X_pca[:, 0], cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "colorize = dict(c=y2, cmap=plt.cm.get_cmap('rainbow', 10), s = 20)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/46:\n",
      "## tSNE first\n",
      "\n",
      "iso = Isomap(n_neighbors=160, n_components=2)\n",
      "X_iso = iso.fit_transform(X2)\n",
      "194/47:\n",
      "## tSNE first\n",
      "from sklearn.manifold import Isomap\n",
      "iso = Isomap(n_neighbors=160, n_components=2)\n",
      "X_iso = iso.fit_transform(X2)\n",
      "194/48:\n",
      "## Code here\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "194/49:\n",
      "## Code here\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "tsne = TSNE(n_components = 2, perplexity=50, random_state = 440)\n",
      "X_tsne = tsne.fit_transform(XS)\n",
      "194/50:\n",
      "## Code here\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "tsne = TSNE(n_components = 2, perplexity=50, random_state = 440)\n",
      "X_tsne = tsne.fit_transform(X2)\n",
      "194/51:\n",
      "## Code here\n",
      "\n",
      "plt.figure(figsize=(10,8))\n",
      "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
      "plt.axis('equal')\n",
      "\n",
      "plt.show()\n",
      "195/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "195/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "195/3:\n",
      "#clean up minors\n",
      "badlist=[i for i in range(0,len(minors.character)) if minors.character[i].find('[')>-1 or minors.character[i].find('!')>-1 or\n",
      "         minors.character[i].find('written')>-1 or  minors.character[i].find('transcribed')>-1 or  \n",
      "         minors.character[i].find('cut to')>-1 or minors.character[i].find('lisa kudrow')>-1 or minors.character[i].find('matthew perry')>-1 \n",
      "         or minors.character[i].find('{')>-1]\n",
      "\n",
      "minors=minors.drop(badlist)\n",
      "195/4:\n",
      "#cleaning up the scenes\n",
      "#note the index's are to account for when rachel is in a bedroom that is either at joeys or at monicas so if we chande the minors list these index's will change also\n",
      "temp=[str(s).replace(\":\",\" \").strip() for s in scenes.location]\n",
      "scenes['location']=temp\n",
      "central_perk=scenes.iloc[[i for i in range(0,len(scenes.location)) if scenes.location[i].find(\"central perk\")==0]]\n",
      "monicas_apartment=scenes.iloc[[i for i in range(0,len(scenes.location)) if i==1214 or scenes.location[i].find(\"monica's\")==0 or scenes.location[i].find(\"monica and rachel\")==0 or scenes.location[i].find(\"monica and chandler\")==0 or scenes.location[i].find(\"monica and phoebe\")==0 or  scenes.location[i].find(\"chandler and monica\")==0]]\n",
      "joey_apartment=scenes.iloc[[i for i in range(0,len(scenes.location)) if i==2822 or scenes.location[i].find(\"chandler and joey\")==0 or scenes.location[i].find(\"joey and rachel\")==0 or scenes.location[i].find(\"chandler and joey\")==0 or scenes.location[i].find(\"chandler's apartment\")==0 or scenes.location[i].find(\"chandler's bedroom\")==0]]\n",
      "195/5:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "\n",
      "#number of guest stars\n",
      "guest_stars=[0]*len(episode)\n",
      "for i in range(0, len(guests)):\n",
      "    guest_season=int(guests.iloc[i].unique_id3.split('-')[0])\n",
      "    guest_episode=int(guests.iloc[i].unique_id3.split('-')[1])\n",
      "    position_val=episode.loc[(episode.season==guest_season) & (episode.episode==guest_episode)]\n",
      "    if len(position_val)>0:\n",
      "        guest_stars[position_val.index[0]]=guests.iloc[i].num_guests\n",
      "    else:\n",
      "        guest_stars[episode.loc[(episode.season==guest_season) & (episode.episode==guest_episode-2)].index[0]]=guests.iloc[i].num_guests\n",
      "\n",
      "\n",
      "\n",
      "#combining with the wiki info\n",
      "writer=[]\n",
      "views=[]\n",
      "month=[]\n",
      "for i in range(0, len(wiki_info)):\n",
      "    writer.append(wiki_info.iloc[i].wri)\n",
      "    views.append(wiki_info.iloc[i].vie)\n",
      "    month.append(wiki_info.iloc[i].month)\n",
      "    #add the extra 0 for when we skip this episode and put in all the old stuff we have\n",
      "    if len(str(wiki_info.iloc[i].epi))>2:\n",
      "        writer.append(0)\n",
      "        views.append(0)\n",
      "        month.append(0)\n",
      "\n",
      "        \n",
      "#wiki info\n",
      "episode[\"writer\"]=writer\n",
      "episode[\"views\"]=views\n",
      "episode[\"month\"]=month\n",
      "\n",
      "#guest_star info\n",
      "episode[\"num_guest_stars\"]=guest_stars\n",
      "\n",
      "#number of \"scenes\"\n",
      "episode[\"central_perk_loc\"]=central_perk_count/scene_total\n",
      "episode[\"monicas_loc\"]=monica_rachel_count/scene_total\n",
      "episode[\"chandlers_loc\"]=joey_chandle_count/scene_total\n",
      "\n",
      "#percentage of lines said in script note NaN if we divide by zero cause its a part 1 and part 2\n",
      "episode[\"monicas_lines\"]=monicas_lines/line_totals\n",
      "episode[\"rachel_lines\"]=rachel_lines/line_totals\n",
      "episode[\"ross_lines\"]=ross_lines/line_totals\n",
      "episode[\"phoebe_lines\"]=phoebe_lines/line_totals\n",
      "episode[\"joey_lines\"]=joey_lines/line_totals\n",
      "episode[\"chandler_lines\"]=chandler_lines/line_totals\n",
      "episode[\"all_lines\"]=all_lines/line_totals\n",
      "195/6:\n",
      "#one standard deviation\n",
      "average=[np.mean(episode.Rating)-np.std(episode.Rating),np.mean(episode.Rating)+np.std(episode.Rating) ]\n",
      "above_average=[np.mean(episode.Rating)+np.std(episode.Rating),np.max(episode.Rating)]\n",
      "below_average=[np.min(episode.Rating),np.mean(episode.Rating)-np.std(episode.Rating) ]\n",
      "print(average)\n",
      "print(above_average)\n",
      "print(below_average)\n",
      "print(len([1 for i in episode.Rating if average[0]<=i<=average[1] ]))\n",
      "print(len([1 for i in episode.Rating if above_average[0]<=i<=above_average[1] ]))\n",
      "print(len([1 for i in episode.Rating if below_average[0]<=i<=below_average[1] ]))\n",
      "195/7:\n",
      "#looking at half a standard deviation above and below the mean review\n",
      "average=[np.mean(episode.Rating)-1/2*np.std(episode.Rating),np.mean(episode.Rating)+1/2*np.std(episode.Rating) ]\n",
      "above_average=[np.mean(episode.Rating)+1/2*np.std(episode.Rating),np.max(episode.Rating)]\n",
      "below_average=[np.min(episode.Rating),np.mean(episode.Rating)-1/2*np.std(episode.Rating) ]\n",
      "print(average)\n",
      "print(above_average)\n",
      "print(below_average)\n",
      "print(len([1 for i in episode.Rating if average[0]<=i<=average[1] ]))\n",
      "print(len([1 for i in episode.Rating if above_average[0]<=i<=above_average[1] ]))\n",
      "print(len([1 for i in episode.Rating if below_average[0]<=i<=below_average[1] ]))\n",
      "\n",
      "rats=[]\n",
      "for i in episode.Rating:\n",
      "    if average[0]<=i<=average[1]: rats.append(\"av\")\n",
      "    elif above_average[0]<=i<=above_average[1]: rats.append(\"above\")\n",
      "    elif below_average[0]<=i<=below_average[1]: rats.append(\"below\")\n",
      "\n",
      "        \n",
      "episode[\"rat_grouped\"]=rats\n",
      "195/8:\n",
      "#adding in the directors who directed 10 or more episodes\n",
      "#episode.groupby(episode.director).episode.count()\n",
      "#Kevin Bright (as Kevin S. Bright)\n",
      "directors_big=['Ben Weiss', 'David Schwimmer', 'Gail Mancuso', 'Gary Halvorson', 'James Burrows', 'Kevin Bright', 'Michael Lembeck', 'Peter Bonerz']\n",
      "\n",
      "dics=[]\n",
      "for i in episode.director:\n",
      "    found_one=False\n",
      "    counter=0\n",
      "    while not found_one and counter<len(directors_big):\n",
      "        if counter==5:\n",
      "            if str(i).find(directors_big[counter])==0 and str(i).find(directors_big[3])<0:\n",
      "                dics.append('Kevin Bright')\n",
      "                found_one=True\n",
      "        else:\n",
      "            if str(i).find(directors_big[counter])==0:\n",
      "                dics.append(directors_big[counter])\n",
      "                found_one=True\n",
      "        counter=counter+1\n",
      "    if not found_one:\n",
      "        dics.append('other')\n",
      "\n",
      "episode[\"common_director\"]=dics\n",
      "195/9:\n",
      "about=[]\n",
      "main=['rachel', 'ross', 'phoebe', 'chandler', 'monica', 'joey', 'the gang', 'christmas', 'thanksgiving']\n",
      "for title in episode.title:\n",
      "    newtitle=title.lower()\n",
      "    setall=[]\n",
      "    for person in main:\n",
      "        if newtitle.find(person)>-1:\n",
      "            setall.append(1)\n",
      "        else:\n",
      "            setall.append(0)\n",
      "    about.append(setall)\n",
      "195/10:\n",
      "unique=[]\n",
      "colors=[]\n",
      "for combo in about:\n",
      "    if combo not in unique:\n",
      "        unique.append(combo)\n",
      "    if combo=='[0, 0, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('red')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('black')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('blue')\n",
      "    elif combo=='[0, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 1, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('fuchsia')\n",
      "    elif combo=='[0, 0, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('orange')\n",
      "    elif combo=='[1, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('darkviolet')\n",
      "    elif combo=='[0, 1, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('grey')\n",
      "    elif combo=='[0, 1, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('cornflowerblue')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('darkolivegreen')\n",
      "    elif combo=='[0, 0, 0, 1, 1, 0, 0, 0, 0]':\n",
      "        colors.append('teal')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 1, 0]':\n",
      "        colors.append('lime')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 1]':\n",
      "        colors.append('dodgerblue')\n",
      "195/11:\n",
      "combo_characters={'[0, 0, 0, 0, 1, 0, 0, 0, 0]': \"Monica\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 0]': \"other\",\n",
      " '[1, 0, 0, 0, 0, 0, 0, 0, 0]': \"Rachel\",\n",
      " '[0, 1, 0, 0, 0, 0, 0, 0, 0]':\"Ross\",\n",
      " '[0, 0, 1, 0, 0, 0, 0, 0, 0]': \"Phoebe\",\n",
      " '[1, 1, 0, 0, 0, 0, 0, 0, 0]': \"Ross and Rachel\",\n",
      "'[0, 0, 0, 0, 0, 1, 0, 0, 0]': \"Joey\",\n",
      " '[0, 0, 0, 1, 0, 0, 0, 0, 0]': \"Chandler\",\n",
      " '[0, 1, 0, 1, 0, 0, 0, 0, 0]': \"Ross and Chandler\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 1]': \"Thanksgiving\",\n",
      " '[0, 1, 0, 0, 1, 0, 0, 0, 0]': \"Ross and Monica\",\n",
      " '[0, 0, 0, 1, 1, 0, 0, 0, 0]': \"Chandler and Monica\",\n",
      " '[1, 0, 0, 0, 0, 1, 0, 0, 0]': \"Rachel and Joey\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 1, 0]': \"Christmas\"}\n",
      "195/12:\n",
      "minidata=pd.get_dummies([combo_characters[str(bout)] for bout in about])\n",
      "minidata['ratings']=episode.Rating\n",
      "minidata['date']=episode.release_date\n",
      "minidata['index']=[i for i in range(0, len(episode))]\n",
      "195/13:\n",
      "plt.figure(figsize=(40,15))\n",
      "plt.plot(minidata.index, minidata.ratings, 'y--')\n",
      "plt.scatter(minidata.loc[minidata.Rachel == 1,'index'], minidata.loc[minidata.Rachel == 1,'ratings'], \n",
      "               c = 'red', alpha = .8, label=\"Rachel\")\n",
      "plt.scatter(minidata.loc[minidata.Chandler == 1,'index'], minidata.loc[minidata.Chandler == 1,'ratings'], \n",
      "               c = 'orange', alpha = .8, label=\"Chandler\")\n",
      "plt.scatter(minidata.loc[minidata.Monica == 1,'index'], minidata.loc[minidata.Monica == 1,'ratings'], \n",
      "               c = 'blue', alpha = .8, label=\"Monica\")\n",
      "plt.scatter(minidata.loc[minidata.Ross == 1,'index'], minidata.loc[minidata.Ross == 1,'ratings'], \n",
      "               c = 'purple', alpha = .8, label=\"Ross\")\n",
      "plt.scatter(minidata.loc[minidata.Phoebe == 1,'index'], minidata.loc[minidata.Phoebe == 1,'ratings'], \n",
      "               c = 'slategray', alpha = .8, label=\"Phoebe\")\n",
      "plt.scatter(minidata.loc[minidata.Joey == 1,'index'], minidata.loc[minidata.Joey == 1,'ratings'], \n",
      "               c = 'navy', alpha = .8, label=\"Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Rachel\"] == 1,'index'], minidata.loc[minidata[\"Ross and Rachel\"] == 1,'ratings'], \n",
      "               c = 'magenta', alpha = .8, label=\"Ross and Rachel\")\n",
      "\n",
      "\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Chandler\"] == 1,'index'], minidata.loc[minidata[\"Ross and Chandler\"] == 1,'ratings'], \n",
      "               c = 'steelblue', alpha = .8, label=\"Ross and Chandler\")\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Monica\"] == 1,'index'], minidata.loc[minidata[\"Ross and Monica\"] == 1,'ratings'], \n",
      "               c = 'yellowgreen', alpha = .8, label=\"Ross and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Chandler and Monica\"] == 1,'index'], minidata.loc[minidata[\"Chandler and Monica\"] == 1,'ratings'], \n",
      "               c = 'olive', alpha = .8, label=\"Chandler and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Rachel and Joey\"]  == 1,'index'], minidata.loc[minidata[\"Rachel and Joey\"] == 1,'ratings'], \n",
      "               c = 'blueviolet', alpha = .8, label=\"Rachel and Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata.Thanksgiving == 1,'index'], minidata.loc[minidata.Thanksgiving == 1,'ratings'], \n",
      "               c = 'peachpuff', alpha = .8, label=\"Thanksgiving\")\n",
      "plt.scatter(minidata.loc[minidata.Christmas == 1,'index'], minidata.loc[minidata.Christmas == 1,'ratings'], \n",
      "               c = 'forestgreen', alpha = .8, label=\"Christmas\")\n",
      "\n",
      "\n",
      "#plot a straight line at the season finales \n",
      "season_finales=[i for i in range(0, len(episode)) if i==len(episode)-1 or episode.iloc[i].episode>episode.iloc[i+1].episode]\n",
      "\n",
      "for i in range(0, len(season_finales)):\n",
      "    plt.vlines(season_finales[i],min(episode.Rating),max(episode.Rating))\n",
      "\n",
      "#plt.scatter(minidata.loc[minidata.other == 1,'index'], minidata.loc[minidata.other == 1,'ratings'], \n",
      "#               c = 'green', alpha = .8, label=\"other\")\n",
      "\n",
      "\n",
      "plt.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)\n",
      "plt.xlabel(\"Epsiode\")\n",
      "plt.ylabel(\"Rating\")\n",
      "plt.title(\"Episode vs Ratings with respect to Title\") \n",
      "plt.show()\n",
      "195/14: joey=pd.read_csv('../data/Joey.csv')\n",
      "195/15:\n",
      "counter=0\n",
      "pos=[]\n",
      "for line in joey.line:\n",
      "    if line.lower().find(\"how you doin\")>-1 :\n",
      "        pos.append(counter)\n",
      "    counter=counter+1\n",
      "\n",
      "print(len(joey.iloc[pos]))\n",
      "joey.iloc[pos]\n",
      "195/16:\n",
      "about=[]\n",
      "main=['rachel', 'ross', 'phoebe', 'chandler', 'monica', 'joey', 'the gang', 'christmas', 'thanksgiving']\n",
      "for title in episode.title:\n",
      "    newtitle=title.lower()\n",
      "    setall=[]\n",
      "    for person in main:\n",
      "        if newtitle.find(person)>-1:\n",
      "            setall.append(1)\n",
      "        else:\n",
      "            setall.append(0)\n",
      "    about.append(setall)\n",
      "195/17:\n",
      "unique=[]\n",
      "colors=[]\n",
      "for combo in about:\n",
      "    if combo not in unique:\n",
      "        unique.append(combo)\n",
      "    if combo=='[0, 0, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('red')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('black')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('blue')\n",
      "    elif combo=='[0, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 1, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('fuchsia')\n",
      "    elif combo=='[0, 0, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('orange')\n",
      "    elif combo=='[1, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('darkviolet')\n",
      "    elif combo=='[0, 1, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('grey')\n",
      "    elif combo=='[0, 1, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('cornflowerblue')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('darkolivegreen')\n",
      "    elif combo=='[0, 0, 0, 1, 1, 0, 0, 0, 0]':\n",
      "        colors.append('teal')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 1, 0]':\n",
      "        colors.append('lime')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 1]':\n",
      "        colors.append('dodgerblue')\n",
      "195/18:\n",
      "combo_characters={'[0, 0, 0, 0, 1, 0, 0, 0, 0]': \"Monica\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 0]': \"other\",\n",
      " '[1, 0, 0, 0, 0, 0, 0, 0, 0]': \"Rachel\",\n",
      " '[0, 1, 0, 0, 0, 0, 0, 0, 0]':\"Ross\",\n",
      " '[0, 0, 1, 0, 0, 0, 0, 0, 0]': \"Phoebe\",\n",
      " '[1, 1, 0, 0, 0, 0, 0, 0, 0]': \"Ross and Rachel\",\n",
      "'[0, 0, 0, 0, 0, 1, 0, 0, 0]': \"Joey\",\n",
      " '[0, 0, 0, 1, 0, 0, 0, 0, 0]': \"Chandler\",\n",
      " '[0, 1, 0, 1, 0, 0, 0, 0, 0]': \"Ross and Chandler\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 1]': \"Thanksgiving\",\n",
      " '[0, 1, 0, 0, 1, 0, 0, 0, 0]': \"Ross and Monica\",\n",
      " '[0, 0, 0, 1, 1, 0, 0, 0, 0]': \"Chandler and Monica\",\n",
      " '[1, 0, 0, 0, 0, 1, 0, 0, 0]': \"Rachel and Joey\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 1, 0]': \"Christmas\"}\n",
      "195/19:\n",
      "minidata=pd.get_dummies([combo_characters[str(bout)] for bout in about])\n",
      "minidata['ratings']=episode.Rating\n",
      "minidata['date']=episode.release_date\n",
      "minidata['index']=[i for i in range(0, len(episode))]\n",
      "195/20:\n",
      "plt.figure(figsize=(40,15))\n",
      "plt.plot(minidata.index, minidata.ratings, 'y--')\n",
      "plt.scatter(minidata.loc[minidata.Rachel == 1,'index'], minidata.loc[minidata.Rachel == 1,'ratings'], \n",
      "               c = 'red', alpha = .8, label=\"Rachel\")\n",
      "plt.scatter(minidata.loc[minidata.Chandler == 1,'index'], minidata.loc[minidata.Chandler == 1,'ratings'], \n",
      "               c = 'orange', alpha = .8, label=\"Chandler\")\n",
      "plt.scatter(minidata.loc[minidata.Monica == 1,'index'], minidata.loc[minidata.Monica == 1,'ratings'], \n",
      "               c = 'blue', alpha = .8, label=\"Monica\")\n",
      "plt.scatter(minidata.loc[minidata.Ross == 1,'index'], minidata.loc[minidata.Ross == 1,'ratings'], \n",
      "               c = 'purple', alpha = .8, label=\"Ross\")\n",
      "plt.scatter(minidata.loc[minidata.Phoebe == 1,'index'], minidata.loc[minidata.Phoebe == 1,'ratings'], \n",
      "               c = 'slategray', alpha = .8, label=\"Phoebe\")\n",
      "plt.scatter(minidata.loc[minidata.Joey == 1,'index'], minidata.loc[minidata.Joey == 1,'ratings'], \n",
      "               c = 'navy', alpha = .8, label=\"Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Rachel\"] == 1,'index'], minidata.loc[minidata[\"Ross and Rachel\"] == 1,'ratings'], \n",
      "               c = 'magenta', alpha = .8, label=\"Ross and Rachel\")\n",
      "\n",
      "\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Chandler\"] == 1,'index'], minidata.loc[minidata[\"Ross and Chandler\"] == 1,'ratings'], \n",
      "               c = 'steelblue', alpha = .8, label=\"Ross and Chandler\")\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Monica\"] == 1,'index'], minidata.loc[minidata[\"Ross and Monica\"] == 1,'ratings'], \n",
      "               c = 'yellowgreen', alpha = .8, label=\"Ross and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Chandler and Monica\"] == 1,'index'], minidata.loc[minidata[\"Chandler and Monica\"] == 1,'ratings'], \n",
      "               c = 'olive', alpha = .8, label=\"Chandler and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Rachel and Joey\"]  == 1,'index'], minidata.loc[minidata[\"Rachel and Joey\"] == 1,'ratings'], \n",
      "               c = 'blueviolet', alpha = .8, label=\"Rachel and Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata.Thanksgiving == 1,'index'], minidata.loc[minidata.Thanksgiving == 1,'ratings'], \n",
      "               c = 'peachpuff', alpha = .8, label=\"Thanksgiving\")\n",
      "plt.scatter(minidata.loc[minidata.Christmas == 1,'index'], minidata.loc[minidata.Christmas == 1,'ratings'], \n",
      "               c = 'forestgreen', alpha = .8, label=\"Christmas\")\n",
      "\n",
      "\n",
      "#plot a straight line at the season finales \n",
      "season_finales=[i for i in range(0, len(episode)) if i==len(episode)-1 or episode.iloc[i].episode>episode.iloc[i+1].episode]\n",
      "\n",
      "for i in range(0, len(season_finales)):\n",
      "    plt.vlines(season_finales[i],min(episode.Rating),max(episode.Rating))\n",
      "\n",
      "#plt.scatter(minidata.loc[minidata.other == 1,'index'], minidata.loc[minidata.other == 1,'ratings'], \n",
      "#               c = 'green', alpha = .8, label=\"other\")\n",
      "\n",
      "\n",
      "plt.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)\n",
      "plt.xlabel(\"Epsiode\")\n",
      "plt.ylabel(\"Rating\")\n",
      "plt.title(\"Episode vs Ratings with respect to Title\") \n",
      "plt.show()\n",
      "195/21:\n",
      "about=[]\n",
      "main=['rachel', 'ross', 'phoebe', 'chandler', 'monica', 'joey', 'the gang', 'christmas', 'thanksgiving']\n",
      "for title in episode.title:\n",
      "    newtitle=title.lower()\n",
      "    setall=[]\n",
      "    for person in main:\n",
      "        if newtitle.find(person)>-1:\n",
      "            setall.append(1)\n",
      "        else:\n",
      "            setall.append(0)\n",
      "    about.append(setall)\n",
      "195/22:\n",
      "unique=[]\n",
      "colors=[]\n",
      "for combo in about:\n",
      "    if combo not in unique:\n",
      "        unique.append(combo)\n",
      "    if combo=='[0, 0, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('red')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('black')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('blue')\n",
      "    elif combo=='[0, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 1, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('fuchsia')\n",
      "    elif combo=='[0, 0, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('green')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('orange')\n",
      "    elif combo=='[1, 1, 0, 0, 0, 0, 0, 0, 0]':\n",
      "        colors.append('darkviolet')\n",
      "    elif combo=='[0, 1, 0, 1, 0, 0, 0, 0, 0]':\n",
      "        colors.append('grey')\n",
      "    elif combo=='[0, 1, 0, 0, 1, 0, 0, 0, 0]':\n",
      "        colors.append('cornflowerblue')\n",
      "    elif combo=='[1, 0, 0, 0, 0, 1, 0, 0, 0]':\n",
      "        colors.append('darkolivegreen')\n",
      "    elif combo=='[0, 0, 0, 1, 1, 0, 0, 0, 0]':\n",
      "        colors.append('teal')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 1, 0]':\n",
      "        colors.append('lime')\n",
      "    elif combo=='[0, 0, 0, 0, 0, 0, 0, 0, 1]':\n",
      "        colors.append('dodgerblue')\n",
      "195/23:\n",
      "combo_characters={'[0, 0, 0, 0, 1, 0, 0, 0, 0]': \"Monica\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 0]': \"other\",\n",
      " '[1, 0, 0, 0, 0, 0, 0, 0, 0]': \"Rachel\",\n",
      " '[0, 1, 0, 0, 0, 0, 0, 0, 0]':\"Ross\",\n",
      " '[0, 0, 1, 0, 0, 0, 0, 0, 0]': \"Phoebe\",\n",
      " '[1, 1, 0, 0, 0, 0, 0, 0, 0]': \"Ross and Rachel\",\n",
      "'[0, 0, 0, 0, 0, 1, 0, 0, 0]': \"Joey\",\n",
      " '[0, 0, 0, 1, 0, 0, 0, 0, 0]': \"Chandler\",\n",
      " '[0, 1, 0, 1, 0, 0, 0, 0, 0]': \"Ross and Chandler\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 0, 1]': \"Thanksgiving\",\n",
      " '[0, 1, 0, 0, 1, 0, 0, 0, 0]': \"Ross and Monica\",\n",
      " '[0, 0, 0, 1, 1, 0, 0, 0, 0]': \"Chandler and Monica\",\n",
      " '[1, 0, 0, 0, 0, 1, 0, 0, 0]': \"Rachel and Joey\",\n",
      " '[0, 0, 0, 0, 0, 0, 0, 1, 0]': \"Christmas\"}\n",
      "195/24:\n",
      "minidata=pd.get_dummies([combo_characters[str(bout)] for bout in about])\n",
      "minidata['ratings']=episode.Rating\n",
      "minidata['date']=episode.release_date\n",
      "minidata['index']=[i for i in range(0, len(episode))]\n",
      "195/25:\n",
      "plt.figure(figsize=(40,15))\n",
      "plt.plot(minidata.index, minidata.ratings, 'y--')\n",
      "plt.scatter(minidata.loc[minidata.Rachel == 1,'index'], minidata.loc[minidata.Rachel == 1,'ratings'], \n",
      "               c = 'red', alpha = .8, label=\"Rachel\")\n",
      "plt.scatter(minidata.loc[minidata.Chandler == 1,'index'], minidata.loc[minidata.Chandler == 1,'ratings'], \n",
      "               c = 'orange', alpha = .8, label=\"Chandler\")\n",
      "plt.scatter(minidata.loc[minidata.Monica == 1,'index'], minidata.loc[minidata.Monica == 1,'ratings'], \n",
      "               c = 'blue', alpha = .8, label=\"Monica\")\n",
      "plt.scatter(minidata.loc[minidata.Ross == 1,'index'], minidata.loc[minidata.Ross == 1,'ratings'], \n",
      "               c = 'purple', alpha = .8, label=\"Ross\")\n",
      "plt.scatter(minidata.loc[minidata.Phoebe == 1,'index'], minidata.loc[minidata.Phoebe == 1,'ratings'], \n",
      "               c = 'slategray', alpha = .8, label=\"Phoebe\")\n",
      "plt.scatter(minidata.loc[minidata.Joey == 1,'index'], minidata.loc[minidata.Joey == 1,'ratings'], \n",
      "               c = 'navy', alpha = .8, label=\"Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Rachel\"] == 1,'index'], minidata.loc[minidata[\"Ross and Rachel\"] == 1,'ratings'], \n",
      "               c = 'magenta', alpha = .8, label=\"Ross and Rachel\")\n",
      "\n",
      "\n",
      "\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Chandler\"] == 1,'index'], minidata.loc[minidata[\"Ross and Chandler\"] == 1,'ratings'], \n",
      "               c = 'steelblue', alpha = .8, label=\"Ross and Chandler\")\n",
      "plt.scatter(minidata.loc[minidata[\"Ross and Monica\"] == 1,'index'], minidata.loc[minidata[\"Ross and Monica\"] == 1,'ratings'], \n",
      "               c = 'yellowgreen', alpha = .8, label=\"Ross and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Chandler and Monica\"] == 1,'index'], minidata.loc[minidata[\"Chandler and Monica\"] == 1,'ratings'], \n",
      "               c = 'olive', alpha = .8, label=\"Chandler and Monica\")\n",
      "plt.scatter(minidata.loc[minidata[\"Rachel and Joey\"]  == 1,'index'], minidata.loc[minidata[\"Rachel and Joey\"] == 1,'ratings'], \n",
      "               c = 'blueviolet', alpha = .8, label=\"Rachel and Joey\")\n",
      "\n",
      "plt.scatter(minidata.loc[minidata.Thanksgiving == 1,'index'], minidata.loc[minidata.Thanksgiving == 1,'ratings'], \n",
      "               c = 'peachpuff', alpha = .8, label=\"Thanksgiving\")\n",
      "plt.scatter(minidata.loc[minidata.Christmas == 1,'index'], minidata.loc[minidata.Christmas == 1,'ratings'], \n",
      "               c = 'forestgreen', alpha = .8, label=\"Christmas\")\n",
      "\n",
      "\n",
      "#plot a straight line at the season finales \n",
      "season_finales=[i for i in range(0, len(episode)) if i==len(episode)-1 or episode.iloc[i].episode>episode.iloc[i+1].episode]\n",
      "\n",
      "for i in range(0, len(season_finales)):\n",
      "    plt.vlines(season_finales[i],min(episode.Rating),max(episode.Rating))\n",
      "\n",
      "#plt.scatter(minidata.loc[minidata.other == 1,'index'], minidata.loc[minidata.other == 1,'ratings'], \n",
      "#               c = 'green', alpha = .8, label=\"other\")\n",
      "\n",
      "\n",
      "plt.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)\n",
      "plt.xlabel(\"Epsiode\")\n",
      "plt.ylabel(\"Rating\")\n",
      "plt.title(\"Episode vs Ratings with respect to Title\") \n",
      "plt.show()\n",
      "195/26:\n",
      "##adding in the common name to the title \n",
      "about=[]\n",
      "main=['rachel', 'ross', 'phoebe', 'chandler', 'monica', 'joey']\n",
      "for title in episode.title:\n",
      "    newtitle=title.lower()\n",
      "    setall=[]\n",
      "    for person in main:\n",
      "        if newtitle.find(person)>-1:\n",
      "            setall.append(person)\n",
      "    if len(setall)==0:\n",
      "        about.append('other')\n",
      "    else:\n",
      "        about.append(setall)\n",
      "about\n",
      "episode[\"title_about\"]=about\n",
      "episode.head()\n",
      "195/27:\n",
      "##adding in the common name to the title \n",
      "about=[]\n",
      "main=['rachel', 'ross', 'phoebe', 'chandler', 'monica', 'joey']\n",
      "for title in episode.title:\n",
      "    newtitle=title.lower()\n",
      "    setall=[]\n",
      "    for person in main:\n",
      "        if newtitle.find(person)>-1:\n",
      "            setall.append(person)\n",
      "    if len(setall)==0:\n",
      "        about.append('other')\n",
      "    else:\n",
      "        about.append(setall)\n",
      "about\n",
      "episode[\"title_about\"]=about\n",
      "episode.head(10)\n",
      "195/28: episode.to_csv(\"Combining_info.csv\")\n",
      "195/29: episode.head(0)\n",
      "195/30: episode.head(1)\n",
      "195/31: episode.head(1)[short_summary]\n",
      "195/32: episode(1)[short_summary]\n",
      "195/33: episode(1)[\"short_summary\"]\n",
      "195/34: episode[short_summary][0]\n",
      "195/35: episode[\"short_summary\"][0]\n",
      "195/36: episode[\"monicas_lines\"][0]\n",
      "195/37: monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "195/38: monica.head()\n",
      "195/39: monica.line()\n",
      "195/40: monica.line[]\n",
      "195/41: monica[\"line\"]\n",
      "195/42: monica[\"line\"][0]\n",
      "195/43: monica[\"line\"][1]\n",
      "195/44: monica[\"line\"][:2]\n",
      "195/45: !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "195/46: xcode-select --install\n",
      "195/47: !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "195/48: !unzip glove*.zip\n",
      "195/49:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "195/50:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(lines).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "195/51:\n",
      "monica[\"line\"][:2]\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "  sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/52:\n",
      "monica[\"line\"][:2]\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "# for s in monica[\"line\"]:\n",
      "#   sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/53:\n",
      "monica[\"line\"][:2]\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "# for s in monica[\"line\"]:\n",
      "#   sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/54:\n",
      "monica[\"line\"][:2]\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "  sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/55:\n",
      "monica[\"line\"][:2]\n",
      "\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/56:\n",
      "monica[\"line\"][:2]\n",
      "import nltk\n",
      "nltk.download('punkt') # one time execution\n",
      "import re\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/57:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/58:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(lines).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "195/59:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "195/60:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean sentences)\n",
      "195/61:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences)\n",
      "195/62: monica[\"line\"][:2]\n",
      "195/63: monica[\"line\"][:2][1]\n",
      "195/64: monica[\"line\"][:2][0]\n",
      "195/65: monica[\"line\"][:2][1]\n",
      "195/66: monica[\"line\"][:2][2]\n",
      "195/67: monica[\"line\"][:2][1]\n",
      "195/68: monica[\"line\"][:2][:]\n",
      "195/69: monica[\"line\"][:2,1]\n",
      "195/70: monica[\"line\"][:2]\n",
      "195/71: monica[\"line\"].text()[:2]\n",
      "195/72: monica[\"line\"]\n",
      "195/73: monica[\"line\"][0]\n",
      "195/74:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences)\n",
      "195/75:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "# print(sentences)\n",
      "195/76:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "195/77: !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "195/78:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[:5])\n",
      "195/79:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "195/80:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "195/81:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "195/82:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "195/83:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "195/84: len(word_embeddings)\n",
      "195/85: nltk.download('stopwords')\n",
      "195/86:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "195/87:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "195/88:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "195/89:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "195/90:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors.append(v)\n",
      "195/91:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "195/92: sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "195/93: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/94:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/95: sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "195/96: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/97:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/98:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/99: sentence_vectors[0].shape\n",
      "195/100: sentence_vectors[0].reshape(1,100)\n",
      "195/101: df = pd.read_csv(\"tennis_articles_v4.csv\")\n",
      "195/102:\n",
      "df = pd.read_csv(\"tennis_articles_v4.csv\")\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in df['article_text']:\n",
      "  sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "195/103:\n",
      "df = pd.read_csv(\"tennis_articles_v4.csv\")\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in df['article_text']:\n",
      "  sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "len(sentences)\n",
      "195/104:\n",
      "df = pd.read_csv(\"tennis_articles_v4.csv\")\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in df['article_text']:\n",
      "  sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "len(sentences)\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "195/105:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "195/106:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "195/107:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "195/108:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors.append(v)\n",
      "195/109:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors.append(v)\n",
      "# similarity matrix\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "195/110:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors.append(v)\n",
      "# similarity matrix\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "for i in range(len(sentences)):\n",
      "  for j in range(len(sentences)):\n",
      "    if i != j:\n",
      "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/111:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors.append(v)\n",
      "# similarity matrix\n",
      "print(len(sentences))\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "for i in range(len(sentences)):\n",
      "  for j in range(len(sentences)):\n",
      "    if i != j:\n",
      "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/112:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/113:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "195/114:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "195/115:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "195/116:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "                   \n",
      "# End of preprocessing sentences\n",
      "195/117:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "195/118:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "195/119:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "195/120:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "                   \n",
      "# End of preprocessing sentences\n",
      "195/121:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "                   \n",
      "# End of preprocessing sentences\n",
      "195/122:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "195/123:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "195/124:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "195/125:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "                   \n",
      "# End of preprocessing sentences\n",
      "195/126:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "195/127:\n",
      "# remove stopwords from the sentences\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences\n",
      "195/128:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "195/129:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "195/130:\n",
      "# let’s create vectors for our sentences. \n",
      "# We will first fetch vectors (each of size 100 elements) for \n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "195/131:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "195/132:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "195/133:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/134: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/135:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/136: sentence_vectors[0].reshape(1,100)\n",
      "195/137: # sentence_vectors[0].reshape(1,100)\n",
      "195/138:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/139: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/140:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/141:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((1000,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "195/142:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((1000,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((1000,))\n",
      "        sentence_vectors.append(v)\n",
      "195/143:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/144:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "195/145:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/146: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/147: # sentence_vectors[0].reshape(1,100)\n",
      "195/148:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/149: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/150:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/151:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/152:\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))\n",
      "195/153:\n",
      "cosine_similarity(sentence_vectors[0].reshape(1,100),\n",
      "                                              sentence_vectors[0].reshape(1,100))[0,0]\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/154:\n",
      "cosine_similarity(sentence_vectors[0].reshape(1,100),\n",
      "                                              sentence_vectors[0].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/155:\n",
      "# # remove punctuations, numbers and special characters\n",
      "# clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# # make alphabets lowercase\n",
      "# clean_sentences = [s.lower() for s in clean_sentences]\n",
      "# clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "\n",
      "# sentence_vectors = []\n",
      "# for i in clean_sentences:\n",
      "#   if len(i) != 0:\n",
      "#     v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "#   else:\n",
      "#     v = np.zeros((100,))\n",
      "#   sentence_vectors.append(v)\n",
      "# # similarity matrix\n",
      "# print(len(sentences))\n",
      "\n",
      "# sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "# for i in range(len(sentences)):\n",
      "#   for j in range(len(sentences)):\n",
      "#     if i != j:\n",
      "#       sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/156:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[0].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/157:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/158:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentence_vectors)\n",
      "195/159:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentence_vectors[0])\n",
      "195/160:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentence[0])\n",
      "print(sentence_vectors[0])\n",
      "195/161:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/162:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/163:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[2])\n",
      "print(sentence_vectors[0])\n",
      "195/164:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[2])\n",
      "print(clean_sentences[2])\n",
      "print(sentence_vectors[0])\n",
      "195/165:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/166:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[1])\n",
      "print(sentence_vectors[0])\n",
      "195/167:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[1])\n",
      "print(clean_sentences[1])\n",
      "print(sentence_vectors[0])\n",
      "195/168:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stopwords)\n",
      "195/169:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "195/170:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[1])\n",
      "print(clean_sentences[1])\n",
      "print(sentence_vectors[0])\n",
      "195/171:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[1])\n",
      "print(clean_sentences[1])\n",
      "print(sentence_vectors[0])\n",
      "195/172:\n",
      "# let’s create vectors for our sentences. \n",
      "# We will first fetch vectors (each of size 100 elements) for \n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "195/173:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/174:\n",
      "# let’s create vectors for our sentences. \n",
      "# We will first fetch vectors (each of size 100 elements) for \n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "195/175:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/176:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:100]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/177:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/178: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/179: # sentence_vectors[0].reshape(1,100)\n",
      "195/180:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/181:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/182:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/183: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/184: # sentence_vectors[0].reshape(1,100)\n",
      "195/185:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/186:\n",
      "df = pd.read_csv(\"tennis_articles_v4.csv\")\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences1 = []\n",
      "for s in df['article_text']:\n",
      "  sentences1.append(sent_tokenize(s))\n",
      "\n",
      "sentences1 = [y for x in sentences1 for y in x] # flatten list\n",
      "len(sentences1)\n",
      "195/187:\n",
      "# # remove punctuations, numbers and special characters\n",
      "# clean_sentences1 = pd.Series(sentences1).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# # make alphabets lowercase\n",
      "# clean_sentences1 = [s.lower() for s in clean_sentences1]\n",
      "# clean_sentences1 = [remove_stopwords(r.split()) for r in clean_sentences1]\n",
      "\n",
      "# sentence_vectors1 = []\n",
      "# for i in clean_sentences1:\n",
      "#   if len(i) != 0:\n",
      "#     v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "#   else:\n",
      "#     v = np.zeros((100,))\n",
      "#   sentence_vectors1.append(v)\n",
      "# # similarity matrix\n",
      "# print(len(sentences1))\n",
      "\n",
      "# sim_mat = np.zeros([len(sentences1), len(sentences1)])\n",
      "# for i in range(len(sentences1)):\n",
      "#   for j in range(len(sentences1)):\n",
      "#     if i != j:\n",
      "#       sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "195/188:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences1 = pd.Series(sentences1).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences1 = [s.lower() for s in clean_sentences1]\n",
      "clean_sentences1 = [remove_stopwords(r.split()) for r in clean_sentences1]\n",
      "\n",
      "sentence_vectors1 = []\n",
      "for i in clean_sentences1:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors1.append(v)\n",
      "# similarity matrix\n",
      "print(len(sentences))\n",
      "\n",
      "sim_mat = np.zeros([len(sentences1), len(sentences1)])\n",
      "for i in range(len(sentences1)):\n",
      "  for j in range(len(sentences1)):\n",
      "    if i != j:\n",
      "      sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "195/189:\n",
      "# remove punctuations, numbers and special characters\n",
      "clean_sentences1 = pd.Series(sentences1).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "# make alphabets lowercase\n",
      "clean_sentences1 = [s.lower() for s in clean_sentences1]\n",
      "clean_sentences1 = [remove_stopwords(r.split()) for r in clean_sentences1]\n",
      "\n",
      "sentence_vectors1 = []\n",
      "for i in clean_sentences1:\n",
      "  if len(i) != 0:\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "  else:\n",
      "    v = np.zeros((100,))\n",
      "  sentence_vectors1.append(v)\n",
      "# similarity matrix\n",
      "print(len(sentences1))\n",
      "\n",
      "sim_mat = np.zeros([len(sentences1), len(sentences1)])\n",
      "for i in range(len(sentences1)):\n",
      "  for j in range(len(sentences1)):\n",
      "    if i != j:\n",
      "      sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "195/190: sentence_vectors1[0]\n",
      "195/191:\n",
      "#sentence_vectors1[0]\n",
      "sentence_vectors1[1]\n",
      "195/192:\n",
      "#sentence_vectors1[0]\n",
      "sentence_vectors[1]\n",
      "195/193:\n",
      "sentence_vectors1[0]\n",
      "sentence_vectors[1]\n",
      "195/194:\n",
      "print(sentence_vectors1[0])\n",
      "sentence_vectors[1]\n",
      "195/195:\n",
      "# print(sentence_vectors1[0])\n",
      "# sentence_vectors[1]\n",
      "195/196:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:119]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/197:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/198: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/199: # sentence_vectors[0].reshape(1,100)\n",
      "195/200:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/201:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:119]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/202:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/203:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/204:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:119]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/205:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/206: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/207: # sentence_vectors[0].reshape(1,100)\n",
      "195/208:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/209:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),\n",
      "                                              sentence_vectors[100].reshape(1,100))[0,0])\n",
      "#sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/210:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "#sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/211:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))\n",
      "#sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/212:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "sentence_vectors.shape\n",
      "195/213:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "len(sentence_vectors)\n",
      "195/214:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:119]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "        sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/215:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clearn_sentences))\n",
      "len(sentence_vectors)\n",
      "195/216:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "195/217:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences[0:119]:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/218:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "195/219:\n",
      "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "print(len(sentences))\n",
      "195/220: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/221:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "195/222:\n",
      "cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "#sim_mat[i][j] = cosine_similarity(sentence_vectors1[i].reshape(1,100), sentence_vectors1[j].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "#         if i != j:\n",
      "#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "#                                               sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/223:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "for i in range(len(sentences)):\n",
      "    for j in range(len(sentences)):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/224:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "for i in range(100):\n",
      "    for j in range(100):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/225:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "195/226:\n",
      "# sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "\n",
      "sim_mat = np.zeros([100, 100])\n",
      "print(len(sentences))\n",
      "195/227: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/228:\n",
      "# sim_mat = np.zeros([len(sentences), len(sentences)])\n",
      "\n",
      "sim_mat = np.zeros([100, 100])\n",
      "# print(len(sentences))\n",
      "195/229: from sklearn.metrics.pairwise import cosine_similarity\n",
      "195/230:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "for i in range(100):\n",
      "    for j in range(100):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/231:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "195/232: ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "195/233: ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "195/234:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:100])), reverse=True)\n",
      "195/235:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/236: sentences[0:100]\n",
      "195/237:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=200\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(100):\n",
      "    for j in range(100):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/238:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "195/239:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "195/240:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/241:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=200\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/242:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "195/243:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "195/244:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/245:\n",
      "for i in range(2):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/246: from gensim.summarization.summarizer import summarize\n",
      "195/247:\n",
      "import gensim\n",
      "from gensim.summarization.summarizer import summarize\n",
      "195/248:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=500\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "195/249:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "195/250:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "195/251:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "195/252:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/253:\n",
      "for i in range(20):\n",
      "  print(ranked_sentences[i][1])\n",
      "195/254: sentences[0:200]\n",
      "195/255: sentences[0]\n",
      "195/256: sentences[0:20]\n",
      "200/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "200/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "200/3:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in phoebe[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "200/4:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "200/5: len(word_embeddings)\n",
      "200/6:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "200/7:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "200/8:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "200/9:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "200/10:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "200/11:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "200/12:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "200/13:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "200/14:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "200/15:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "200/16:\n",
      "# let’s create vectors for our sentences. \n",
      "# We will first fetch vectors (each of size 100 elements) for \n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "200/17:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "print(sentences[0])\n",
      "print(clean_sentences[0])\n",
      "print(sentence_vectors[0])\n",
      "200/18: from sklearn.metrics.pairwise import cosine_similarity\n",
      "200/19: sentences[0:20]\n",
      "200/20:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=500\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "200/21:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "200/22:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "200/23:\n",
      "for i in range(20):\n",
      "  print(ranked_sentences[i][1])\n",
      "200/24:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "202/1:\n",
      "#Import general libraries (needed for functions)\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython import display\n",
      "\n",
      "#Import the RB Functions\n",
      "import qiskit.ignis.verification.randomized_benchmarking as rb\n",
      "\n",
      "#Import Qiskit classes \n",
      "import qiskit\n",
      "from qiskit.providers.aer.noise import NoiseModel\n",
      "from qiskit.providers.aer.noise.errors.standard_errors import depolarizing_error, thermal_relaxation_error\n",
      "202/2:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/3:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/4:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/5:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 1\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/6:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/7:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1, 2]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/8:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/9:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/10:\n",
      "#Import general libraries (needed for functions)\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython import display\n",
      "\n",
      "#Import the RB Functions\n",
      "import qiskit.ignis.verification.randomized_benchmarking as rb\n",
      "\n",
      "#Import Qiskit classes \n",
      "import qiskit\n",
      "from qiskit.providers.aer.noise import NoiseModel\n",
      "from qiskit.providers.aer.noise.errors.standard_errors import depolarizing_error, thermal_relaxation_error\n",
      "202/11:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1, 2]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/12:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/13:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/14:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/15:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [4]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/16:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/17:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/18:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/19:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [0]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/20:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/21:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/22:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/23:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 1\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/24:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/25:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 10\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/26:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/27:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/28:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/29:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 2\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/30:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/31:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 3\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/32:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/33:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/34:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/35:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/36:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/37:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [4]\n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/38:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/39:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/40:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/41:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/42:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/43:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][6].draw()\n",
      "202/44:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][4].draw()\n",
      "202/45:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/46:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][4].draw()\n",
      "202/47:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][3].draw()\n",
      "202/48:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][2].draw()\n",
      "202/49:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/50:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][2].draw()\n",
      "202/51:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][1].draw()\n",
      "202/52:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "202/53:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[2][0].draw()\n",
      "202/54:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[5][0].draw()\n",
      "202/55:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "202/56:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/57:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/58:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][1].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/59:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][3].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/60:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][2].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/61:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/62:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=3 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/63:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/64:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=3 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 1,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/65:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=3 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/66:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/67:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=3 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "rb_opts['rb_pattern'] = [[0, 2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/68:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/69:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=3 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "#rb_opts['rb_pattern'] = [[0, 2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/70:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/71:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "#rb_opts['rb_pattern'] = [[0, 2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/72:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[4][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/73:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/74:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/75:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/76:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/77:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/78:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/79:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1,2,3]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/80:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/81:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/82:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/83:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[0,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/84:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[0,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/85:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/86:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[1,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/87:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/88:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[0,2]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/89:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/90:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[0,1]]\n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/91:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[1,2]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/92:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 5\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[1,2]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/93:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/94:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=4 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 2\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[1,2]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/95:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/96:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 2\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 3],[1,2]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/97:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/98:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [2,3]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 2\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/99:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/100:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1,2]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 2\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/101:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/102:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 1\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/103:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/104:\n",
      "#Generate RB circuits (2Q RB)\n",
      "\n",
      "#number of qubits\n",
      "nQ=2 \n",
      "rb_opts = {}\n",
      "#Number of Cliffords in the sequence\n",
      "# rb_opts['length_vector'] = [1, 10, 20, 50, 75, 100, 125, 150, 175, 200]\n",
      "rb_opts['length_vector'] = [1]\n",
      "# My interpretation: that's definitely m \n",
      "\n",
      "#Number of seeds (random sequences)\n",
      "# Is this Km , what about the pattern say [[0,1]] for 2\n",
      "\n",
      "rb_opts['nseeds'] = 1\n",
      "#Default pattern\n",
      "\n",
      "rb_opts['rb_pattern'] = [[0, 1]]\n",
      "\n",
      "## For the look of it the second one is for the ancilla in [0,3]\n",
      "## \n",
      "\n",
      "#????????#\n",
      "\n",
      "\n",
      "\n",
      "rb_circs, xdata = rb.randomized_benchmarking_seq(**rb_opts)\n",
      "202/105:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/106:\n",
      "# Create a new circuit without the measurement\n",
      "qregs = rb_circs[0][-1].qregs\n",
      "cregs = rb_circs[0][-1].cregs\n",
      "qc = qiskit.QuantumCircuit(*qregs, *cregs)\n",
      "for i in rb_circs[0][-1][0:-nQ]:\n",
      "    qc.data.append(i)\n",
      "203/1:\n",
      "# Cell 1\n",
      "import numpy as np\n",
      "\n",
      "from qiskit import Aer, QuantumCircuit, execute\n",
      "from qiskit.visualization import plot_histogram\n",
      "from IPython.display import display, Math, Latex\n",
      "\n",
      "from may4_challenge import plot_state_qsphere\n",
      "from may4_challenge.ex1 import minicomposer\n",
      "from may4_challenge.ex1 import check1, check2, check3, check4, check5, check6, check7, check8\n",
      "from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "202/107:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "state = statevec(rb_circs[0][0]) # determine final state after running the circuit\n",
      "display(Math(vec_in_braket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/108: from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "202/109: from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "203/2:\n",
      "# Cell 1\n",
      "import numpy as np\n",
      "\n",
      "from qiskit import Aer, QuantumCircuit, execute\n",
      "from qiskit.visualization import plot_histogram\n",
      "from IPython.display import display, Math, Latex\n",
      "\n",
      "from may4_challenge import plot_state_qsphere\n",
      "from may4_challenge.ex1 import minicomposer\n",
      "from may4_challenge.ex1 import check1, check2, check3, check4, check5, check6, check7, check8\n",
      "from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "203/3:\n",
      "# Cell 1\n",
      "import numpy as np\n",
      "\n",
      "from qiskit import Aer, QuantumCircuit, execute\n",
      "from qiskit.visualization import plot_histogram\n",
      "from IPython.display import display, Math, Latex\n",
      "\n",
      "from may4_challenge import plot_state_qsphere\n",
      "from may4_challenge.ex1 import minicomposer\n",
      "from may4_challenge.ex1 import check1, check2, check3, check4, check5, check6, check7, check8\n",
      "from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "202/110: # from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "202/111:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "state = statevec(rb_circs[0][0]) # determine final state after running the circuit\n",
      "display(Math(vec_in_braket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "203/4:\n",
      "# Cell 1\n",
      "import numpy as np\n",
      "\n",
      "from qiskit import Aer, QuantumCircuit, execute\n",
      "from qiskit.visualization import plot_histogram\n",
      "from IPython.display import display, Math, Latex\n",
      "\n",
      "from may4_challenge import plot_state_qsphere\n",
      "from may4_challenge.ex1 import minicomposer\n",
      "from may4_challenge.ex1 import check1, check2, check3, check4, check5, check6, check7, check8\n",
      "from may4_challenge.ex1 import return_state, vec_in_braket, statevec\n",
      "202/112:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "print(xdats)\n",
      "\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/113:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "print(xdata)\n",
      "\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/114:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "rb_circs[0][0].qubits()\n",
      "\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/115:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "rb_circs[0][0].qubits\n",
      "\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/116:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "rb_circs[0][0].qubits\n",
      "\n",
      "print(rb_circs[0][0])\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/117:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw()\n",
      "\n",
      "rb_circs[0][0].qubits\n",
      "\n",
      "# print(rb_circs[0][0])\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/118:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw(\"latex\")\n",
      "\n",
      "rb_circs[0][0].qubits\n",
      "\n",
      "# print(rb_circs[0][0])\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/119:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw(\"latex\")\n",
      "\n",
      "# rb_circs[0][0].qubits\n",
      "\n",
      "# print(rb_circs[0][0])\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "202/120:\n",
      "# If you experience a BrokenProcessPool error in a later cell,\n",
      "# delete this cell and re-run the notebook\n",
      "rb_circs[0][0].draw(\"text\")\n",
      "\n",
      "# rb_circs[0][0].qubits\n",
      "\n",
      "# print(rb_circs[0][0])\n",
      "# state = statevec(rb_circs[0][0]) \n",
      "# determine final state after running the circuit\n",
      "# display(Math(vec_in_bra   ket(state.data)))\n",
      "\n",
      "#rb_circs[sample(0,5)][?? what's this for , list index]\n",
      "204/1:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "204/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "204/3:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "204/4:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "204/5:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "204/6:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "204/7:\n",
      "#looking at half a standard deviation above and below the mean review\n",
      "average=[np.mean(episode.Rating)-1/2*np.std(episode.Rating),np.mean(episode.Rating)+1/2*np.std(episode.Rating) ]\n",
      "above_average=[np.mean(episode.Rating)+1/2*np.std(episode.Rating),np.max(episode.Rating)]\n",
      "below_average=[np.min(episode.Rating),np.mean(episode.Rating)-1/2*np.std(episode.Rating) ]\n",
      "print(average)\n",
      "print(above_average)\n",
      "print(below_average)\n",
      "print(len([1 for i in episode.Rating if average[0]<=i<=average[1] ]))\n",
      "print(len([1 for i in episode.Rating if above_average[0]<=i<=above_average[1] ]))\n",
      "print(len([1 for i in episode.Rating if below_average[0]<=i<=below_average[1] ]))\n",
      "\n",
      "rats=[]\n",
      "for i in episode.Rating:\n",
      "    if average[0]<=i<=average[1]: rats.append(\"av\")\n",
      "    elif above_average[0]<=i<=above_average[1]: rats.append(\"above\")\n",
      "    elif below_average[0]<=i<=below_average[1]: rats.append(\"below\")\n",
      "\n",
      "        \n",
      "episode[\"rat_grouped\"]=rats\n",
      "204/8:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "204/9:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "204/10:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "204/11: episode[\"monicas_lines\"][0]\n",
      "204/12:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "204/13:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "204/14: episode[\"monicas_lines\"][0]\n",
      "204/15:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "204/16:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "204/17:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "204/18:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "206/1:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "206/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "206/3:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "206/4:\n",
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "\n",
      "#counting the number of scenes\n",
      "scene_total=np.array([len(scenes.loc[(scenes.season==episode.iloc[x].season) & (scenes.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "central_perk_count=np.array([len(central_perk.loc[(central_perk.season==episode.iloc[x].season) & (central_perk.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "monica_rachel_count=np.array([len(monicas_apartment.loc[(monicas_apartment.season==episode.iloc[x].season) & (monicas_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_chandle_count=np.array([len(joey_apartment.loc[(joey_apartment.season==episode.iloc[x].season) & (joey_apartment.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "episode.to_csv(\"Combining_info.csv\")\n",
      "206/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#lines said in script\n",
      "monicas_lines=np.array([len(monica.loc[(monica.season==episode.iloc[x].season) & (monica.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "rachel_lines=np.array([len(rachel.loc[(rachel.season==episode.iloc[x].season) & (rachel.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "ross_lines=np.array([len(ross.loc[(ross.season==episode.iloc[x].season) & (ross.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "phoebe_lines=np.array([len(phoebe.loc[(phoebe.season==episode.iloc[x].season) & (phoebe.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "joey_lines=np.array([len(joey.loc[(joey.season==episode.iloc[x].season) & (joey.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "chandler_lines=np.array([len(chandler.loc[(chandler.season==episode.iloc[x].season) & (chandler.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "all_lines=np.array([len(alls.loc[(alls.season==episode.iloc[x].season) & (alls.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "minors_lines=np.array([len(minors.loc[(minors.season==episode.iloc[x].season) & (minors.episode==episode.iloc[x].episode)]) for x in range(0,len(episode))])\n",
      "\n",
      "line_totals= monicas_lines+rachel_lines+ross_lines+phoebe_lines+joey_lines+chandler_lines+minors_lines+all_lines\n",
      "206/6: monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "206/7:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "206/8:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "206/9:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in phoebe[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "206/10: # Word Embeddings (Using Glove)\n",
      "206/11: # Word Embeddings (Using Glove)\n",
      "206/12: # Word Embeddings (Using Glove)\n",
      "206/13:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "206/14: monica.head()\n",
      "206/15: #monica.head()\n",
      "206/16:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica.head()\n",
      "206/17: monica[\"line\"][0]\n",
      "206/18:\n",
      "monica[\"line\"][:2]\n",
      "import nltk\n",
      "nltk.download('punkt') # one time execution\n",
      "import re\n",
      "206/19:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "206/20: !unzip glove*.zip\n",
      "206/21:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "206/22:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "206/23:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "206/24:\n",
      "len(word_embeddings)\n",
      "# This is the length of the vectors we embedd our data into\n",
      "206/25:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "206/26:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "206/27:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "206/28:\n",
      "# let’s create vectors for our sentences. \n",
      "# We will first fetch vectors (each of size 100 elements) for \n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "206/29:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "# print(sentences[0])\n",
      "# print(clean_sentences[0])\n",
      "# print(sentence_vectors[0])\n",
      "206/30:\n",
      "# let’s create vectors for our sentences. \n",
      "\n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "206/31:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "# print(sentences[0])\n",
      "# print(clean_sentences[0])\n",
      "# print(sentence_vectors[0])\n",
      "206/32: from sklearn.metrics.pairwise import cosine_similarity\n",
      "206/33:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "206/34:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=500\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "206/35:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "206/36:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "206/37:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "206/38:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "206/39: #sentences[0:100]\n",
      "206/40:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "206/41:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "206/42:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "206/43:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "206/44:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "206/45:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "206/46:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "206/47:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "206/48:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "206/49:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "206/50:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in phoebe[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "206/51:\n",
      "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "!unzip glove*.zip\n",
      "206/52:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "206/53:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "206/54:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "206/55:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "206/56:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica.head()\n",
      "206/57:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "206/58:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "206/59:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "207/1:\n",
      "monica[\"line\"][:2]\n",
      "import nltk\n",
      "#Natural Language toolkit\n",
      "\n",
      "nltk.download('punkt') # one time execution\n",
      "import re\n",
      "207/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "207/3:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "207/4:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "207/5:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "207/6:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in phoebe[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "#print(sentences[0:5])\n",
      "208/1:\n",
      "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "208/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "208/3:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "207/7: pwd\n",
      "208/4: pwd\n",
      "208/5: # pwd\n",
      "208/6:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "208/8:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "207/8:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/9:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/10:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/11:\n",
      "# pwd\n",
      "songs_ph\n",
      "208/12:\n",
      "# pwd\n",
      "songs_ph\n",
      "208/13:\n",
      "# pwd\n",
      "songs_ph[0]\n",
      "208/14:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/15:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/16:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/17:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/18:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/19:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/20:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/21:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/22:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "208/23:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/24:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/25:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "207/9:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/26:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/27:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/28:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/29:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "208/30:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "208/31:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/32:\n",
      "# pwd\n",
      "songs_ph[\"line\"]\n",
      "208/33:\n",
      "# pwd\n",
      "# songs_ph[\"line\"]\n",
      "208/34:\n",
      "# pwd\n",
      "songs_ph\n",
      "208/35:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(''../data/songs_phoebe.csv')\n",
      "208/36:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "208/37:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/38:\n",
      "# pwd\n",
      "songs_ph\n",
      "208/39:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/40:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"]\n",
      "208/41:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica\n",
      "208/42:\n",
      "# pwd\n",
      "songs_ph['Line']\n",
      "208/43:\n",
      "# pwd\n",
      "songs_ph['Line'][1]\n",
      "208/44:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "208/45:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/46:\n",
      "# pwd\n",
      "songs_ph['Line'][1]\n",
      "208/47:\n",
      "# pwd\n",
      "songs_ph['Line'][0]\n",
      "208/48:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "208/49:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "208/50:\n",
      "# pwd\n",
      "songs_ph['Line'][0]\n",
      "208/51:\n",
      "# pwd\n",
      "songs_ph['line'][0]\n",
      "208/52:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "208/53:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "208/54:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "209/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "209/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/3:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "209/4:\n",
      "# pwd\n",
      "songs_ph['line'][0]\n",
      "209/5:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "209/6:\n",
      "# pwd\n",
      "songs_ph['line'][0]\n",
      "209/7:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "209/8:\n",
      "# pwd\n",
      "songs_ph['line'][2]\n",
      "209/9:\n",
      "# pwd\n",
      "songs_ph['line'][0]\n",
      "207/10:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "207/11:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "207/12:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "207/13: pwd\n",
      "207/14:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "207/15:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "207/16:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "207/17: pwd\n",
      "209/10:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "209/11:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/12:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "209/13:\n",
      "# pwd\n",
      "songs_ph['line'][0]\n",
      "209/14:\n",
      "# pwd\n",
      "songs_ph[\"line\"][0]\n",
      "209/15:\n",
      "# pwd\n",
      "songs_ph[\"line\"][0]\n",
      "209/16:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica\n",
      "monica[\"line\"][:2]\n",
      "209/17:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica\n",
      "#monica[\"line\"][:2]\n",
      "209/18:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/19:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica\n",
      "#monica[\"line\"][:2]\n",
      "209/20:\n",
      "# pwd\n",
      "songs_ph[\"line\"][0]\n",
      "209/21:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "209/22:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/23:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica\n",
      "#monica[\"line\"][:2]\n",
      "209/24:\n",
      "# pwd\n",
      "songs_ph[\"line\"][0]\n",
      "209/25:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "monica[\"line\"][:2]\n",
      "209/26:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "209/27:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "209/28:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/Phoebe_songs.numbers.csv')\n",
      "209/29:\n",
      "# pwd\n",
      "songs_ph[\"line\"][0]\n",
      "209/30:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "207/18:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "207/19:\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "monica[\"line\"][:2]\n",
      "207/20: pwd\n",
      "207/21:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "209/31:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/Phoebe_songs.numbers.csv')\n",
      "209/32:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/sonngs_phoebe.csv')\n",
      "209/33:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "209/34:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/sonngs_phoebe.csv')\n",
      "209/35:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/36:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "209/37:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/38:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv('../data/songs_phoebe.csv')\n",
      "209/39:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "209/40:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "209/41:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "210/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/3:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/4:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/5:\n",
      "cwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/6:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "# songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/7:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "207/22: pwd\n",
      "210/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "210/9:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "# songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/10:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "210/11:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/12:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "210/13:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "210/14:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "# songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/15:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "210/16:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "# songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/17:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "210/18:\n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "210/19:\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/20:\n",
      "import os\n",
      "\n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/21:\n",
      "import os\n",
      "os.getcwd()  \n",
      "pwd\n",
      "# songs_ph[\"line\"][0]\n",
      "210/22:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "210/23:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "# songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/24:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/25:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/26:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "210/27:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "210/28:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "210/29:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "210/30:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "210/31:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "210/32: monica[\"line\"][0]\n",
      "210/33:\n",
      "monica[\"line\"][:2]\n",
      "import nltk\n",
      "nltk.download('punkt') # one time execution\n",
      "import re\n",
      "210/34:\n",
      "# remove punctuations, numbers and special characters\n",
      "print(sentences[0:5])\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "print(clean_sentences[0:5])\n",
      "210/35:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "210/36:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "210/37:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "210/38:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "210/39:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "210/40:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "210/41:\n",
      "# remove stopwords from the sentences\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "210/42:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "210/43:\n",
      "# let’s create vectors for our sentences. \n",
      "\n",
      "#the constituent words in a sentence and then take mean/average \n",
      "#of those vectors to arrive at a consolidated vector for the sentence\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        print(w)\n",
      "210/44:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "# print(sentences[0])\n",
      "# print(clean_sentences[0])\n",
      "# print(sentence_vectors[0])\n",
      "210/45: from sklearn.metrics.pairwise import cosine_similarity\n",
      "210/46:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "210/47:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=500\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "210/48:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=230\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "210/49:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "210/50:\n",
      "# ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
      "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "210/51:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "210/52:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/53: from sklearn.manifold import TSNE\n",
      "210/54:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "210/55:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "210/56:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "210/57:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/58:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/59:\n",
      "# Tokenize the string into words\n",
      "tokens = word_tokenize(big_title_string)\n",
      "\n",
      "# Remove non-alphabetic tokens, such as punctuation\n",
      "words = [word.lower() for word in tokens if word.isalpha()]\n",
      "\n",
      "\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/60:\n",
      "# Tokenize the string into words\n",
      "tokens = word_tokenize(clean_sentences)\n",
      "\n",
      "# Remove non-alphabetic tokens, such as punctuation\n",
      "words = [word.lower() for word in tokens if word.isalpha()]\n",
      "\n",
      "\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/61:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "tokens = word_tokenize(clean_sentences)\n",
      "\n",
      "# Remove non-alphabetic tokens, such as punctuation\n",
      "words = [word.lower() for word in tokens if word.isalpha()]\n",
      "\n",
      "\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/62:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "tokens = word_tokenize(clean_sentences)\n",
      "\n",
      "# Remove non-alphabetic tokens, such as punctuation\n",
      "words = [word.lower() for word in tokens if word.isalpha()]\n",
      "\n",
      "'''\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "\n",
      "'''\n",
      "210/63:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "words_filtered=[]\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        words_filtered.append(w)\n",
      "        #print(w)\n",
      "        \n",
      "        \n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/64:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "#print(stop_words)\n",
      "210/65:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "words_filtered=[]\n",
      "vector_list=[]\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        words_filtered.append(w)\n",
      "        \n",
      "        vector_list.append(word_embeddings.get(w, np.zeros((100,)))  )\n",
      "        #print(w)\n",
      "        \n",
      "        \n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/66:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "210/67:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/68:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/69:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/70:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import xgboost as xgb\n",
      "import gensim\n",
      "import seaborn as sns\n",
      "210/71:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import xgboost as xgb\n",
      "import gensim\n",
      "import seaborn as sns\n",
      "210/72:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "import gensim\n",
      "import seaborn as sns\n",
      "210/73:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "import gensim\n",
      "210/74:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "210/75:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "210/76:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "# print(sentences[0])\n",
      "# print(clean_sentences[0])\n",
      "# print(sentence_vectors[0])\n",
      "210/77:\n",
      "# sentence_vectors[0].reshape(1,100)\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "210/78:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=230\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "210/79:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "words_filtered=[]\n",
      "vector_list=[]\n",
      "for i in clean_sentences[0:5]:\n",
      "    for w in i.split():\n",
      "        words_filtered.append(w)\n",
      "        \n",
      "        vector_list.append(word_embeddings.get(w, np.zeros((100,)))  )\n",
      "        #print(w)\n",
      "        \n",
      "        \n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/80:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/81:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/82:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/83:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/84:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/85:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/86:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 5))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/87:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 30, 5))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/88:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 30, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/89:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 20, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/90:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 16, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/91:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 15, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/92:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 15, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/93:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/94:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/95:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:200])\n",
      "sns.set()\n",
      "210/96:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/97:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 10)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:200])\n",
      "sns.set()\n",
      "210/98:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 2))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/99:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 14, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/100:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/101:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 20)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:200])\n",
      "sns.set()\n",
      "210/102:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/103:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:200])\n",
      "sns.set()\n",
      "210/104:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/105:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:300])\n",
      "sns.set()\n",
      "210/106:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/107:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:300])\n",
      "sns.set()\n",
      "210/108:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/109:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:300])\n",
      "sns.set()\n",
      "210/110:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/111:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/112:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/113: len(words_filtered)\n",
      "210/114:\n",
      "from nltk.tokenize import word_tokenize\n",
      "# Tokenize the string into words\n",
      "words_filtered=[]\n",
      "vector_list=[]\n",
      "for i in clean_sentences:\n",
      "    for w in i.split():\n",
      "        words_filtered.append(w)\n",
      "        \n",
      "        vector_list.append(word_embeddings.get(w, np.zeros((100,)))  )\n",
      "        #print(w)\n",
      "        \n",
      "        \n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "210/115: len(words_filtered)\n",
      "210/116:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 50)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/117:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 13, 1))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/118:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/119:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/120:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 200, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/121:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/122:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 20)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/123:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/124:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/125:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/126:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 20)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:700])\n",
      "sns.set()\n",
      "210/127:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/128:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 30)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:300])\n",
      "sns.set()\n",
      "210/129:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/130:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/131:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 40)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:300])\n",
      "sns.set()\n",
      "210/132:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/133:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 40)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/134:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/135:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 3, init = 'random', random_state = 10, perplexity = 20)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/136:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 20)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "sns.set()\n",
      "210/137:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 20))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/138:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    #print(word)\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "210/139:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils\n",
      "210/140: clean_sentences\n",
      "210/141: sentences\n",
      "210/142: sentences.lower()\n",
      "210/143: sentences.lower()\n",
      "210/144:\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "210/145:\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "210/146:\n",
      "X = []\n",
      " Y = []\n",
      "length = len(text)\n",
      "seq_length = 100\n",
      "  for i in range(0, length-seq_length, 1):\n",
      "     sequence = text[i:i + seq_length]\n",
      "     label =text[i + seq_length]\n",
      "     X.append([char_to_n[char] for char in sequence])\n",
      "     Y.append(char_to_n[label])\n",
      "210/147:\n",
      "X = []\n",
      " Y = []\n",
      "length = len(text)\n",
      "seq_length = 100\n",
      "  for i in range(0, length-seq_length, 1):\n",
      "     sequence = text[i:i + seq_length]\n",
      "     label =text[i + seq_length]\n",
      "     X.append([char_to_n[char] for char in sequence])\n",
      "     Y.append(char_to_n[label])\n",
      "210/148:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "seq_length = 100\n",
      "  for i in range(0, length-seq_length, 1):\n",
      "     sequence = text[i:i + seq_length]\n",
      "     label =text[i + seq_length]\n",
      "     X.append([char_to_n[char] for char in sequence])\n",
      "     Y.append(char_to_n[label])\n",
      "210/149:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "seq_length = 100\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/150:\n",
      "text=sentences\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "210/151:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "seq_length = 100\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/152: #sentences.lower()\n",
      "210/153:\n",
      "model = Sequential()\n",
      "model.add(LSTM(100, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(50))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "210/154:\n",
      "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "X_modified = X_modified / float(len(characters))\n",
      "Y_modified = np_utils.to_categorical(Y)\n",
      "210/155:\n",
      "model = Sequential()\n",
      "model.add(LSTM(100, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(50))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "210/156:\n",
      "model = Sequential()\n",
      "model.add(LSTM(100, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(50))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "210/157:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/158:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('cwd')\n",
      "210/159:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/160: ls\n",
      "210/161:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "210/162: rm cwd\n",
      "210/163: ls\n",
      "210/164:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/165:\n",
      "ls\n",
      "model.load_weights('baseline.h5')\n",
      "210/166: model.load_weights('baseline.h5')\n",
      "210/167:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/168:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 100\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/169:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 10\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/170:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/171:\n",
      "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "X_modified = X_modified / float(len(characters))\n",
      "Y_modified = np_utils.to_categorical(Y)\n",
      "210/172:\n",
      "model = Sequential()\n",
      "model.add(LSTM(100, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(50))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "210/173:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "210/174: model.load_weights('baseline.h5')\n",
      "210/175:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/176:\n",
      "string_mapped = X[99]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/177:\n",
      "string_mapped = X[50]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/178:\n",
      "string_mapped = X[51]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/179:\n",
      "string_mapped = X[100]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/180: X\n",
      "210/181: X.shape\n",
      "210/182: len(X)\n",
      "210/183:\n",
      "string_mapped = X[100]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/184:\n",
      "string_mapped = X[98]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/185:\n",
      "string_mapped = X[98]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/186:\n",
      "string_mapped = X[50]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/187:\n",
      "string_mapped = X[49]\n",
      "# generating characters\n",
      "for i in range(seq_length):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/188: len(X)\n",
      "210/189:\n",
      "txt=\"\"\n",
      "for char in full_string:\n",
      "    txt = txt+char\n",
      "txt\n",
      "210/190:\n",
      "string_mapped = X[49]\n",
      "# generating characters\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/191:\n",
      "string_mapped = X[50]\n",
      "# generating characters\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/192:\n",
      "string_mapped = X[4]\n",
      "# generating characters\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/193:\n",
      "\n",
      "string_mapped = X[99]\n",
      "full_string = [n_to_char[value] for value in string_mapped]\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/194:\n",
      "string_mapped = X[99]\n",
      "full_string = [n_to_char[value] for value in string_mapped]\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/195:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/196:\n",
      "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "X_modified = X_modified / float(len(characters))\n",
      "Y_modified = np_utils.to_categorical(Y)\n",
      "210/197:\n",
      "model = Sequential()\n",
      "model.add(LSTM(100, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(50))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
      "210/198:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "210/199:\n",
      "string_mapped = X[49]\n",
      "full_string = [n_to_char[value] for value in string_mapped]\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/200:\n",
      "txt=\"\"\n",
      "for char in full_string:\n",
      "    txt = txt+char\n",
      "txt\n",
      "210/201:\n",
      "string_mapped = X[5]\n",
      "full_string = [n_to_char[value] for value in string_mapped]\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "210/202:\n",
      "txt=\"\"\n",
      "for char in full_string:\n",
      "    txt = txt+char\n",
      "txt\n",
      "210/203:\n",
      "txt=\"\"\n",
      "for char in full_string:\n",
      "    print(char)\n",
      "    txt = txt+char\n",
      "txt\n",
      "210/204: X\n",
      "210/205:\n",
      "text=sentences\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "210/206:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "210/207: X\n",
      "212/1:\n",
      "text=sentences\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "212/3:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "212/4:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "212/5:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "212/6:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "212/7: sentences\n",
      "212/8:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "212/9:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "212/10:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "212/11:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "212/12: clean_sentences\n",
      "212/13: sentences\n",
      "212/14:\n",
      "text=sentences\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/15:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/16: X\n",
      "212/17: text\n",
      "212/18: characters\n",
      "212/19: sentences\n",
      "212/20: clean_sentences\n",
      "212/21:\n",
      "text=clean_sentences\n",
      "characters = sorted(list(set(sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/22: characters\n",
      "212/23:\n",
      "text=clean_sentences\n",
      "characters = sorted(list(set(clean_sentences)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/24: characters\n",
      "212/25:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50|\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/26:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/27: X\n",
      "212/28:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/29:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/30:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "\n",
      "for i in range(0, 50, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/31:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "\n",
      "for i in range(0, 2, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/32:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "\n",
      "for i in range(0, 2, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/33:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/34:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print(label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/35:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/36:\n",
      "text=songs_ph\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/37: characters\n",
      "212/38:\n",
      "text=songs_ph\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/39: characters\n",
      "212/40:\n",
      "text=songs_ph\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/41: characters\n",
      "212/42:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/43:\n",
      "text=songs_ph\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/44: characters\n",
      "212/45:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/46: songs_ph\n",
      "212/47:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/48:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/49: characters\n",
      "212/50:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "# for i in range(0, length-seq_length, 1):\n",
      "for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/51:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted(list(set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/52: characters\n",
      "212/53:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "#for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/54:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted((set(text)))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/55: characters\n",
      "212/56:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "#for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/57:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted((text))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/58:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted((text))\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/59: characters\n",
      "212/60:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "#for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/61:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = clean_sentences\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/62: characters\n",
      "212/63:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "#for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/64:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted(clean_sentences)\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/65: characters\n",
      "212/66:\n",
      "X = []\n",
      "Y = []\n",
      "length = len(text)\n",
      "print(length)\n",
      "seq_length = 50\n",
      "#PLAY WITH seq length\n",
      "for i in range(0, length-seq_length, 1):\n",
      "#for i in range(0, 1, 1):\n",
      "    sequence = text[i:i + seq_length]\n",
      "    print(sequence)\n",
      "    label =text[i + seq_length]\n",
      "    print('label',label)\n",
      "    print([char_to_n[char] for char in sequence])\n",
      "    X.append([char_to_n[char] for char in sequence])\n",
      "    Y.append(char_to_n[label])\n",
      "212/67: But since this is a small dataset (with 17,670 words), and the number of unique words (4,605 in number) constitute around one-fourth of the data, it would not be a wise decision to train on such a mapping. This is because if we assume that all unique words occurred equally in number (which is not true), we would have a word occurring roughly four times in the entire training dataset, which is just not sufficient to build a text generator.\n",
      "212/68:\n",
      "text=songs_ph[\"line\"]\n",
      "characters = sorted(clean_sentences)\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/69:\n",
      "text=songs_ph[\"line\"]\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      "characters = sorted(clean_sentences)\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/70:\n",
      "raw_text=songs_ph[\"line\"]\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      "\n",
      "characters = sorted(clean_sentences)\n",
      "n_to_char = {n:char for n, char in enumerate(characters)}\n",
      "char_to_n = {char:n for n, char in enumerate(characters)}\n",
      "212/71:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc('rhyme.txt')\n",
      "print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/72:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc('\"../data/songs_phoebe.csv\"')\n",
      "print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/73:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\"')\n",
      "print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/74:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/75:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/76:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "212/77:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "212/78:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "212/79: ##LOAD DATA\n",
      "212/80:\n",
      "##LOAD DATA\n",
      "# load doc into memory\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      "\n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "ad doc into memory\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "212/81:\n",
      "\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "ad doc into memory\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "212/82:\n",
      "\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "212/83:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "212/84:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "212/85:\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "212/86:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "212/87:\n",
      "\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "212/88:\n",
      "\n",
      "sequences = array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "212/89:\n",
      "sequences = array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "212/90:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "212/91: X\n",
      "212/92:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "212/93:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "212/94:\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils\n",
      "212/95:\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "212/96:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "212/97:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "212/98:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "213/1:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "213/2:\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "213/3:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "213/4:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "213/5:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "213/6:\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "213/7:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "213/8:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "213/9:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "213/10:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "213/11:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "213/12:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "213/13:\n",
      "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "X_modified = X_modified / float(len(characters))\n",
      "Y_modified = np_utils.to_categorical(Y)\n",
      "213/14:\n",
      "# X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "# X_modified = X_modified / float(len(characters))\n",
      "# Y_modified = np_utils.to_categorical(Y)\n",
      "213/15:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=100, verbose=2)\n",
      "213/16:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1?, verbose=2)\n",
      "213/17:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "213/18:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "213/19:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "213/20:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "215/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "215/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "215/3:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "215/4:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "215/5:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "215/6:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "215/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "215/8:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "215/9:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "215/10:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "215/11:\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "215/12:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "215/13:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "215/14:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "X.shape\n",
      "215/15:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "215/16:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "215/17:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "X.shape\n",
      "216/1: raw_text\n",
      "216/2:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "216/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "216/4:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "216/5:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "216/6:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "216/7: raw_text\n",
      "216/8:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "216/9:\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "216/10:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "216/11:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/12:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/13:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/14:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "216/15:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/16:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/17:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/18:\n",
      "sequences = np.array(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/19:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/20:\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "216/21:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "216/22:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/23:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/24:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "216/25:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "216/26:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/27:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/28:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "216/29:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "216/30:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "216/31:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "216/32:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "216/33:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "216/34:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "217/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "217/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "217/3:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "217/4:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "217/5:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "217/6:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "217/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "217/8:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "217/9:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "217/10:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "217/11:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "217/12:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "217/13:\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "217/14:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "218/1:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "218/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "218/3:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "218/4:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "218/5:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "218/6:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "218/7:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "218/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "218/9:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "218/10:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "218/11: raw_text\n",
      "218/12:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "218/13:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "218/14:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "218/15:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "218/16:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "218/17: model.load_weights('baseline.h5')\n",
      "218/18:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "218/19: from pickle import dump\n",
      "218/20:\n",
      "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "218/21:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "218/22:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "218/23: model.load_weights('baseline.h5')\n",
      "218/24:\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "encoded = [mapping[char] for char in in_text]\n",
      "218/25: model.load_weights('baseline.h5')\n",
      "218/26:\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "encoded = [mapping[char] for char in in_text]\n",
      "218/27: model = load_model('model.h5')\n",
      "218/28:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('baseline.h5')\n",
      "218/29:\n",
      "from keras.models import load_model()\n",
      "model = load_model('model.h5')\n",
      "218/30:\n",
      "from keras.models import load_model\n",
      "model = load_model('model.h5')\n",
      "218/31:\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/32:\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/33:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/34:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/35:\n",
      "# dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "# from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/36:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/37:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/38:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/39:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('baseline.h5')\n",
      "218/40:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "from keras.models import load_model\n",
      "model = load_model('model.h5')\n",
      "218/41:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "218/42:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/43:\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "\n",
      "model = load_model('model.h5')\n",
      "218/44: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/45:\n",
      "model = load_model('model.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "encoded = [mapping[char] for char in in_text]\n",
      "encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/46:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "218/47: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/48:\n",
      "model = load_model('model.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "encoded = [mapping[char] for char in in_text]\n",
      "encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/49:\n",
      "model = load_model('model.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "encoded = [mapping[char] for char in in_text]\n",
      "encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/50: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/51:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/52:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# load the model\n",
      "model = load_model('model.h5')\n",
      "# load the mapping\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/53:\n",
      "model = load_model('model.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/54:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "218/55:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/56:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/57:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/58:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/59:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "# model.load_weights('model_weights.h5')\n",
      "218/60: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/61:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/62:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/63:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/64:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/65:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "    model.save_weights(\"model_weights.h5\")\n",
      "218/66:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/67:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/68:\n",
      "model.fit(X, y, epochs=1, batch_size=100)\n",
      "\n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "\n",
      "model.save_weights('model.h5')\n",
      "218/69: model.fit(X, y, epochs=1, batch_size=100)\n",
      "218/70:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "# model.load_weights('model_weights.h5')\n",
      "218/71: model.fit(X, y, epochs=1, batch_size=100)\n",
      "218/72: model.fit(X, y, epochs=1, batch_size=100)\n",
      "218/73:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "218/74:\n",
      "import json\n",
      "\n",
      "# lets assume `model` is main model \n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/75:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "# model.load_weights('model_weights.h5')\n",
      "218/76:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "\n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/77: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/78:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# load the model\n",
      "model = load_model('model.h5')\n",
      "# load the mapping\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/79:\n",
      "model = load_model('model_weights.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/80:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# load the model\n",
      "model = load_model('model_weights.h5')\n",
      "# load the mapping\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/81:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "    \n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/82: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/83:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# load the model\n",
      "model = load_model('model_weights.h5')\n",
      "# load the mapping\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/84:\n",
      "import json\n",
      "\n",
      "# lets assume `model` is main model \n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/85:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "    \n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/86: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/87:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# load the model\n",
      "model = load_model('model_weights.h5')\n",
      "# load the mapping\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/88:\n",
      "# generate a sequence of characters with a language model\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/89:\n",
      "model = load_model('model_weights.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/90: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/91:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "218/92:\n",
      "import json\n",
      "\n",
      "# lets assume `model` is main model \n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/93:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "    \n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/94: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/95:\n",
      "import json\n",
      "\n",
      "# lets assume `model` is main model \n",
      "model_json = model.to_json()\n",
      "with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "    json.dump(model_json, json_file)\n",
      "\n",
      "model.save_weights(\"model_weights.h5\")\n",
      "218/96:\n",
      "from keras.models import load_model\n",
      "from keras.models import model_from_json\n",
      "import json\n",
      "\n",
      "with open('model_in_json.json','r') as f:\n",
      "    model_json = json.load(f)\n",
      "    \n",
      "model = model_from_json(model_json)\n",
      "model.load_weights('model_weights.h5')\n",
      "218/97: dump(mapping, open('mapping.pkl', 'wb'))\n",
      "218/98:\n",
      "# generate a sequence of characters with a language model\n",
      "\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "218/99:\n",
      "model = load_model('model_weights.h5')\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "218/100:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "model.save('model.h5')\n",
      "\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "219/2:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "219/3:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "219/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "219/5:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "219/6:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "219/7:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "219/8:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "219/9:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "219/10:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "219/11:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "219/12:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "219/13: raw_text\n",
      "219/14:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "219/15:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "219/16:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "219/17:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "219/18:\n",
      "# X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "# X_modified = X_modified / float(len(characters))\n",
      "# Y_modified = np_utils.to_categorical(Y)\n",
      "219/19:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "model.save('model.h5')\n",
      "\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/20:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import dump\n",
      "\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "219/21:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "219/22:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "219/23:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "219/24:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "219/25:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "219/26:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "219/27:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "219/28:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "219/29: raw_text\n",
      "219/30:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "219/31:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "219/32:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "219/33:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "219/34:\n",
      "# X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "# X_modified = X_modified / float(len(characters))\n",
      "# Y_modified = np_utils.to_categorical(Y)\n",
      "219/35:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "model.save('model.h5')\n",
      "\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/36: model = load_model('model.h5')\n",
      "219/37:\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/38:\n",
      "# generate a sequence of characters with a language model\n",
      "\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "219/39:\n",
      "# model = load_model('model_weights.h5')\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "219/40:\n",
      "string_mapped = X[5]\n",
      "full_string = [n_to_char[value] for value in string_mapped]\n",
      "# generating characters\n",
      "for i in range(400):\n",
      "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
      "    x = x / float(len(characters))\n",
      "\n",
      "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
      "    seq = [n_to_char[value] for value in string_mapped]\n",
      "    full_string.append(n_to_char[pred_index])\n",
      "\n",
      "    string_mapped.append(pred_index)\n",
      "    string_mapped = string_mapped[1:len(string_mapped)]\n",
      "219/41:\n",
      "print(generate_seq(model, mapping, 10, 'Love is', 20))\n",
      "# test mid-line\n",
      "219/42:\n",
      "print(generate_seq(model, mapping, 10, 'Love is', 20))\n",
      "# test mid-line\n",
      "219/43:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "219/44:\n",
      "print(generate_seq(model, mapping, 10, 'Love is', 20))\n",
      "# test mid-line\n",
      "219/45:\n",
      "print(generate_seq(model, mapping, 'Love is', 20))\n",
      "# test mid-line\n",
      "219/46:\n",
      "print(generate_seq(model, mapping, 10, 'Love is', 20))\n",
      "# test mid-line\n",
      "219/47:\n",
      "print(generate_seq(model, mapping, 10, 'Love is', 640))\n",
      "# test mid-line\n",
      "219/48:\n",
      "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
      "# test mid-line\n",
      "219/49:\n",
      "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
      "# test mid-line\n",
      "219/50:\n",
      "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
      "# test mid-line\n",
      "219/51:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/52:\n",
      "print(generate_seq(model, mapping, 64, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/53:\n",
      "print(generate_seq(model, mapping, 64, 'l', 20))\n",
      "# test mid-line\n",
      "219/54:\n",
      "print(generate_seq(model, mapping, 64, 'l', 20))\n",
      "# test mid-line\n",
      "219/55:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 10))\n",
      "# test mid-line\n",
      "219/56:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/57:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/58:\n",
      "# generate a sequence of characters with a language model\n",
      "\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "219/59:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/60:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/61:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import dump\n",
      "\n",
      "#import xgboost as xgb\n",
      "#import gensim\n",
      "219/62:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "219/63:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "219/64:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "219/65:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "219/66:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "219/67:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from pickle import load\n",
      "import json\n",
      "from keras.models import load_model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "from keras.layers import Dropout\n",
      "from keras.layers import LSTM\n",
      "from keras.utils import np_utils, to_categorical\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "219/68:\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# save tokens to file, one dialog per line\n",
      "def save_doc(lines, filename):\n",
      "    data = '\\n'.join(lines)\n",
      "    file = open(filename, 'w')\n",
      "    file.write(data)\n",
      "    file.close()\n",
      " \n",
      "# load text\n",
      "raw_text = load_doc(\"../data/songs_phoebe.csv\")\n",
      "#print(raw_text)\n",
      " \n",
      "# clean\n",
      "tokens = raw_text.split()\n",
      "raw_text = ' '.join(tokens)\n",
      " \n",
      "# organize into sequences of characters\n",
      "length = 10\n",
      "sequences = list()\n",
      "for i in range(length, len(raw_text)):\n",
      "    # select sequence of tokens\n",
      "    seq = raw_text[i-length:i+1]\n",
      "    # store\n",
      "    sequences.append(seq)\n",
      "print('Total Sequences: %d' % len(sequences))\n",
      " \n",
      "# save sequences to file\n",
      "out_filename = 'char_sequences.txt'\n",
      "save_doc(sequences, out_filename)\n",
      "219/69:\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "def load_doc(filename):\n",
      "    # open the file as read only\n",
      "    file = open(filename, 'r')\n",
      "    # read all text\n",
      "    text = file.read()\n",
      "    # close the file\n",
      "    file.close()\n",
      "    return text\n",
      " \n",
      "# load\n",
      "in_filename = 'char_sequences.txt'\n",
      "raw_text = load_doc(in_filename)\n",
      "lines = raw_text.split('\\n')\n",
      "219/70: raw_text\n",
      "219/71:\n",
      "chars = sorted(list(set(raw_text)))\n",
      "mapping = dict((c, i) for i, c in enumerate(chars))\n",
      "\n",
      "sequences = list()\n",
      "for line in lines:\n",
      "    # integer encode line\n",
      "    encoded_seq = [mapping[char] for char in line]\n",
      "    # store\n",
      "    sequences.append(encoded_seq)\n",
      "\n",
      "# sequences = list()\n",
      "# for line in lines:\n",
      "#     # integer encode line\n",
      "#     encoded_seq = [mapping[char] for char in line]\n",
      "#     # store\n",
      "#     sequences.append(encoded_seq)\n",
      "219/72:\n",
      "# vocabulary size\n",
      "vocab_size = len(mapping)\n",
      "print('Vocabulary Size: %d' % vocab_size)\n",
      "219/73:\n",
      "sequences = np.array(sequences)\n",
      "print(sequences)\n",
      "X, y = sequences[:,:-1], sequences[:,-1]\n",
      "print(X.shape)\n",
      "print(X)\n",
      "print(y.shape)\n",
      "219/74:\n",
      "# need to one hot encode each character\n",
      "\n",
      "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
      "X = np.array(sequences)\n",
      "y = to_categorical(y, num_classes=vocab_size)\n",
      "\n",
      "\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "219/75:\n",
      "# X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
      "# X_modified = X_modified / float(len(characters))\n",
      "# Y_modified = np_utils.to_categorical(Y)\n",
      "219/76:\n",
      "model = Sequential()\n",
      "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
      "model.add(Dense(vocab_size, activation='softmax'))\n",
      "print(model.summary())\n",
      "\n",
      "# compile model\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "# fit model\n",
      "model.fit(X, y, epochs=1, verbose=2)\n",
      "\n",
      "\n",
      "model.save('model.h5')\n",
      "\n",
      "dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/77:\n",
      "# import json\n",
      "\n",
      "# # lets assume `model` is main model \n",
      "# model_json = model.to_json()\n",
      "# with open(\"model_in_json.json\", \"w\") as json_file:\n",
      "#     json.dump(model_json, json_file)\n",
      "\n",
      "# model.save_weights(\"model_weights.h5\")\n",
      "\n",
      "\n",
      "# from keras.models import load_model\n",
      "# from keras.models import model_from_json\n",
      "# import json\n",
      "\n",
      "# with open('model_in_json.json','r') as f:\n",
      "#     model_json = json.load(f)\n",
      "# model = model_from_json(model_json)\n",
      "# model.load_weights('model_weights.h5')\n",
      "219/78: model = load_model('model.h5')\n",
      "219/79:\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# dump(mapping, open('mapping.pkl', 'wb'))\n",
      "219/80:\n",
      "# generate a sequence of characters with a language model\n",
      "\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "219/81:\n",
      "# model = load_model('model_weights.h5')\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "\n",
      "# encoded = [mapping[char] for char in in_text]\n",
      "# encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "# yhat = model.predict_classes(encoded, verbose=0)\n",
      "\n",
      "# print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "219/82:\n",
      "print(generate_seq(model, mapping, 10, 'line Love', 20))\n",
      "# test mid-line\n",
      "219/83:\n",
      "# generate a sequence of characters with a language model\n",
      "\n",
      "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
      "    in_text = seed_text\n",
      "    # generate a fixed number of characters\n",
      "    for _ in range(n_chars):\n",
      "        # encode the characters as integers\n",
      "        encoded = [mapping[char] for char in in_text]\n",
      "        # truncate sequences to a fixed length\n",
      "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
      "        # one hot encode\n",
      "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
      "        encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
      "        # predict character\n",
      "        yhat = model.predict_classes(encoded, verbose=0)\n",
      "        # reverse map integer to character\n",
      "        out_char = ''\n",
      "        for char, index in mapping.items():\n",
      "            if index == yhat:\n",
      "                out_char = char\n",
      "                break\n",
      "        # append to input\n",
      "        in_text += char\n",
      "        return in_text\n",
      "\n",
      "# # load the model\n",
      "# model = load_model('model_weights.h5')\n",
      "# # load the mapping\n",
      "# mapping = load(open('mapping.pkl', 'rb'))\n",
      "219/84: model = load_model('model.h5')\n",
      "219/85:\n",
      "mapping = load(open('mapping.pkl', 'rb'))\n",
      "# dump(mapping, open('mapping.pkl', 'wb'))\n",
      "211/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "211/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "220/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "220/2:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "220/3:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "220/4:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "221/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/2:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/4:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/5:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "221/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/7:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/9:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/10:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "221/11:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/12:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/13:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/14:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/15:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "220/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "220/6:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "220/7:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "221/16:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "221/17:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "221/18:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "211/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "211/4:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "211/5:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "222/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "222/2:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "222/3:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "222/4:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "222/5:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "222/6:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "222/7:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "222/8:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/9:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "222/10:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "222/11:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "222/12:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "222/13:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/14:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "222/15:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/16:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "222/17:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "222/18:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "222/19:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "222/20:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "222/21:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "222/22:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "222/23:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "222/24:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "222/25:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "222/26:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "222/27:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/28:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "222/29:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "222/30:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "222/31:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "222/32:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "222/33:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "222/34:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continuous.columns)\n",
      "222/35:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "222/36:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/37: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "222/38:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/39: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/40: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/41:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/42:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/43:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/44: X_train,X_test,y_train,y_test= train_test_split(X2,y_all,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/45:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='relu'))\n",
      "model.add(Dense(127, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/46: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/47: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/48:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/49:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "222/50:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/51:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/52:\n",
      "#categorical and continuous\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "clf = Perceptron(tol=1e-1, random_state=random)\n",
      "clf.fit(X_train, y_train)\n",
      "Perceptron()\n",
      "clf.score(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/53:\n",
      "#forcing everything to be categorical the problem here is that it just forces it on all the data... which like... we can do... it just won't be good\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y1,test_size = size_t, random_state = random, shuffle = True)\n",
      "clf = Perceptron(tol=1e-1, random_state=random)\n",
      "clf.fit(X_train, y_train)\n",
      "Perceptron()\n",
      "clf.score(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/54:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "222/55:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/56:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/57:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/58:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/59:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "222/60:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "222/61:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "222/62:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "222/63:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "222/64:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "222/65:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "222/66:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/67:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "222/68:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "222/69:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "222/70:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continuous.columns)\n",
      "222/71:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "222/72:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/73: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "222/74:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/75: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/76: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/77:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/78:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/79: X_train,X_test,y_train,y_test= train_test_split(X2,y_all,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/80:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='relu'))\n",
      "model.add(Dense(127, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/81: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/82: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/83:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/84:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "222/85:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/86: y.shape\n",
      "222/87: y_train.shape\n",
      "222/88: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "222/89:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/90: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/91: y_train.shape\n",
      "222/92:\n",
      "x_train.shape\n",
      "y_train.shape\n",
      "222/93:\n",
      "X_train.shape\n",
      "y_train.shape\n",
      "222/94: print(X_train.shape, y_train.shape)\n",
      "222/95:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/96: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/97: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/98: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/99: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/100:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/101:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/102: X_train,X_test,y_train,y_test= train_test_split(X2,y_all,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/103:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/104: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/105: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/106:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/107:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(500, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/108: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/109: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/110:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/111:\n",
      "print(X_train.shape, y_train.shape)\n",
      "size_t=.3\n",
      "222/112: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/113:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/114:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/115: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/116:\n",
      "print(X_train.shape, y_train.shape)\n",
      "size_t=.3\n",
      "222/117: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/118:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/119:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/120: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "222/121:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/122: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/123:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "size_t=.3\n",
      "222/124: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "222/125:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/126: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/127: print(X_train.shape, y_train.shape)\n",
      "222/128: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/129:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/130:\n",
      "size_t1=.2\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/131:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/132:\n",
      "size_t1=.2\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/133:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/134: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/135: print(X_train.shape, y_train.shape)\n",
      "222/136: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/137:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/138:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='sigmoid'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/139: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/140: print(X_train.shape, y_train.shape)\n",
      "222/141: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/142:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/143:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/144:\n",
      "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
      "    initial_learning_rate=1e-3,\n",
      "    decay_steps=10000,\n",
      "    decay_rate=0.9)\n",
      "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
      "222/145: opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
      "222/146: opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
      "222/147: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/148: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/149: print(X_train.shape, y_train.shape)\n",
      "222/150: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "222/151:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/152: history = model.fit(X_train, y_train, epochs=10000, batch_size=len(X_train[0]))\n",
      "222/153: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/154:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/155:\n",
      "size_t1=.1\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/156:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='sigmoid'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/157: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/158: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/159: print(X_train.shape, y_train.shape)\n",
      "222/160: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/161:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/162:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/163:\n",
      "size_t1=.3\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/164:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='sigmoid'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/165: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/166: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/167: print(X_train.shape, y_train.shape)\n",
      "222/168: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/169:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/170:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/171:\n",
      "size_t1=.3\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/172:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(50, input_dim=54, activation='sigmoid'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/173: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/174: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/175: print(X_train.shape, y_train.shape)\n",
      "222/176: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/177:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/178:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(10, input_dim=54, activation='sigmoid'))\n",
      "#model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/179: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/180: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/181: print(X_train.shape, y_train.shape)\n",
      "222/182: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/183:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/184:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/185:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(500, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(2, activation='softmax'))\n",
      "222/186: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/187: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/188:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/189:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "y_all.shape\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/190:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/191:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/192:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all) , axis=1)\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/193:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all) , axis=0)\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/194:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all,axis=1) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/195:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all,axis=1, return_index=True) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/196:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all,axis=0, return_index=True) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/197:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "print(np.unique(y_all,axis=0, return_counts=True) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "220/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "220/9:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "220/10:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "220/11:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "220/12:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "220/13:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "220/14:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "220/15:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "220/16:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "220/17:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "220/18:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "220/19:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "220/20:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "220/21:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "220/22:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "220/23:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "220/24:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "220/25:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "220/26:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "220/27:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "220/28:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "220/29:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "220/30:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "220/31:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "220/32:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continous.columns)\n",
      "220/33:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "220/34:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "220/35: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "220/36:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "220/37:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "220/38:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "220/39:\n",
      "#all categorical data and guesing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(127,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "220/40:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "220/41:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continuous.columns)\n",
      "220/42:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "220/43:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "220/44: X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t, random_state = random,shuffle = True)\n",
      "220/45:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(27, input_dim=54, activation='relu'))\n",
      "model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "220/46: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "220/47: history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train[0]))\n",
      "220/48:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "220/49:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "220/50: X_train,X_test,y_train,y_test= train_test_split(X2,y_all,test_size = size_t, random_state = random, shuffle = True)\n",
      "220/51:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='relu'))\n",
      "model.add(Dense(127, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "220/52: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "220/53: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "220/54:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "220/55:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "220/56:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "220/57:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "220/58: X_train,X_test,y_train,y_test= train_test_split(X2,y_all,test_size = size_t, random_state = random, shuffle = True)\n",
      "220/59:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(127, input_dim=253, activation='relu'))\n",
      "model.add(Dense(127, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "220/60: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "220/61: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/198:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "222/199:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "222/200:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "222/201:\n",
      "#above or below the mean\n",
      "y1=[0]*len(episode_info)\n",
      "for counter in range(0, len(episode_info)):\n",
      "    if episode_info.iloc[counter].Rating>np.mean(episode_info.Rating):\n",
      "        y1[counter]=1\n",
      "222/202:\n",
      "#the results we want\n",
      "y_all = pd.get_dummies(episode_info.rat_grouped).to_numpy()\n",
      "222/203:\n",
      "#comparing these different values\n",
      "for i in range(0, len(y1)):\n",
      "    print([episode_info.iloc[i].Rating,y1[i],episode_info.iloc[i].rat_grouped])\n",
      "222/204:\n",
      "#take only the columns i care about\n",
      "episode=episode_info[allofthem[12:len(allofthem)-1]]\n",
      "allofthem=[col for col in episode]\n",
      "222/205:\n",
      "#make the continous and categorical dataframes\n",
      "continuous=episode[allofthem[0:11]]\n",
      "categorical=episode[allofthem[11:16]]\n",
      "categorical_all=episode[allofthem[11:16]]\n",
      "binary=episode[allofthem[16:len(allofthem)]]\n",
      "222/206:\n",
      "#rank the continuous data\n",
      "ranked_locations=[0]*len(episode)\n",
      "ranked_lines=[0]*len(episode)\n",
      "counter=0\n",
      "for counter in range(0, len(episode)):\n",
      "    line=episode.iloc[counter]\n",
      "    #rank the locations\n",
      "    original=[('central_perk',line.central_perk_loc) , ('monica',line.monicas_loc ), ('chandler', line.chandlers_loc),('ross',line.ross_loc)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_locations[counter]=[p[0] for p in original]\n",
      "    #rank the number of words\n",
      "    original=[('rachel',line.rachel_lines) , ('monica',line.monicas_lines ), ('chandler', line.chandler_lines),('ross',line.ross_lines),('phoebe',line.phoebe_lines),('joey',line.joey_lines)]\n",
      "    original.sort(key=lambda x: x[1])\n",
      "    ranked_lines[counter]=[p[0] for p in original]\n",
      "222/207:\n",
      "categorical_all['ranked_lines']=ranked_lines\n",
      "categorical_all['ranked_locations']=ranked_locations\n",
      "222/208:\n",
      "# Function to get one hot encoded only the catergorical values it has been given\n",
      "def prepare(df,cols):\n",
      "    df[cols] = df[cols].astype(str)\n",
      "    \n",
      "    dummies = pd.get_dummies(df[cols])\n",
      "    \n",
      "\n",
      "    return dummies\n",
      "\n",
      "# One hot encode the categorical data\n",
      "categorical_prime = prepare(categorical,categorical.columns)\n",
      "categorical_prime2 = prepare(categorical_all,categorical_all.columns)\n",
      "222/209:\n",
      "#scale the continuous data\n",
      "scaler = StandardScaler()\n",
      "df_scaled = pd.DataFrame(scaler.fit_transform(continuous),columns = continuous.columns)\n",
      "222/210:\n",
      "#combine the dataframes\n",
      "X=pd.concat([df_scaled,categorical_prime, binary],axis=1).to_numpy()\n",
      "X2=pd.concat([categorical_prime2, binary],axis=1).to_numpy()\n",
      "222/211:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/212:\n",
      "size_t1=.3\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y_all,test_size = size_t1, random_state = random,shuffle = True)\n",
      "222/213:\n",
      "#x has 54 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(10, input_dim=54, activation='sigmoid'))\n",
      "#model.add(Dense(27, activation='sigmoid'))\n",
      "model.add(Dense(3, activation='softmax'))\n",
      "222/214: opt = keras.optimizers.Adam(lr=1e-3)\n",
      "222/215: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
      "222/216: print(X_train.shape, y_train.shape)\n",
      "222/217: history = model.fit(X_train, y_train, epochs=1000, batch_size=len(X_train[0]))\n",
      "222/218:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/219:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/220:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "\n",
      "print(np.unique(y_all,axis=0, return_counts=True) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/221:\n",
      "#x has 253 categories of data\n",
      "model = Sequential()\n",
      "model.add(Dense(500, input_dim=253, activation='tanh', init=\"uniform\"))\n",
      "model.add(Dense(50, activation='sigmoid'))\n",
      "model.add(Dense(2, activation='softmax'))\n",
      "222/222: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "222/223: history = model.fit(X_train, y_train, epochs=80, batch_size=len(X_train[0]))\n",
      "222/224:\n",
      "# evaluate the keras model\n",
      "_, accuracy = model.evaluate(X_test, y_test)\n",
      "print('Accuracy: %.2f' % (accuracy*100))\n",
      "222/225:\n",
      "#for some categorical and some continuous and guessing above or below\n",
      "X_train,X_test,y_train,y_test= train_test_split(X,y1,test_size = size_t, random_state = random,shuffle = True)\n",
      "mlp = MLPClassifier(hidden_layer_sizes=(27,),max_iter=10000)\n",
      "mlp.fit(X_train,y_train)\n",
      "222/226:\n",
      "y_pred = mlp.predict(X_test)\n",
      "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
      "222/227:\n",
      "import keras\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "222/228:\n",
      "y_all_1=y_all[:,[0,2]]\n",
      "\n",
      "print(np.unique(y_all,axis=0, return_counts=True) )\n",
      "print(y_all.shape)\n",
      "X_train,X_test,y_train,y_test= train_test_split(X2,y_all_1,test_size = size_t, random_state = random, shuffle = True)\n",
      "222/229:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "222/230:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "222/231:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "223/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "223/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "223/3:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "222/232:\n",
      "#make a list of all the column headers\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "223/4: episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "223/5:\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "223/6: title_col=allofthem[\"title_about\"]\n",
      "223/7: title_col=allofthem[\"title_about\"]\n",
      "223/8: title_col=allofthem[\"title_about\"]\n",
      "223/9: title_col=episode_info[\"title_about\"]\n",
      "223/10: title\n",
      "223/11: title_col\n",
      "223/12: title_col=episode_info[\"short_summary\"]\n",
      "223/13: title_col\n",
      "223/14: title_col[0]\n",
      "223/15: title_col[0:5]\n",
      "227/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "227/2:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "227/3:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "227/4:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "227/5:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "227/6:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "227/7:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "226/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "226/2:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "226/3:\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "226/4: sum_table=episode_info[\"short_summary\"]\n",
      "226/5: sum_table=episode_info[\"short_summary\"]\n",
      "226/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "226/7:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "226/8:\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "226/9: sum_table=episode_info[\"short_summary\"]\n",
      "226/10:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "226/11:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/12:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table[\"\"short_summary\"\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/13:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table[\"short_summary]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/14:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table[\"short_summary\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/15:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/16:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:2])\n",
      "226/17:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/18: clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "226/19:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "226/20:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "226/21:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "226/22:\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "226/23:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "226/24: from sklearn.metrics.pairwise import cosine_similarity\n",
      "226/25:\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "226/26:\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/27:\n",
      "t_len=230\n",
      "\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/28:\n",
      "t_len=490\n",
      "\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/29:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/30: ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/31:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "226/32:\n",
      "# for i in range(10):\n",
      "#   print(ranked_sentences[i][1])\n",
      "226/33: len(ranked_sentences)\n",
      "227/8:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "227/9:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "227/10:\n",
      "# Initialize figure\n",
      "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
      "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
      "\n",
      "# Import adjustText, initialize list of texts\n",
      "from adjustText import adjust_text\n",
      "texts = []\n",
      "words_to_plot = list(np.arange(0, 400, 10))\n",
      "\n",
      "# Append words to list\n",
      "for word in words_to_plot:\n",
      "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
      "    \n",
      "# Plot text using adjust_text (because overlapping text is hard to read)\n",
      "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
      "            expand_points = (2,1), expand_text = (1,2),\n",
      "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
      "\n",
      "plt.show()\n",
      "227/11:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "227/12:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "227/13:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "227/14:\n",
      "episode=pd.read_csv('../data/episode_info.csv')\n",
      "wiki_info=pd.read_csv(\"../data/creators.csv\")\n",
      "rachel=pd.read_csv(\"../data/Rachel.csv\")\n",
      "monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "phoebe=pd.read_csv(\"../data/Phoebe.csv\")\n",
      "ross=pd.read_csv(\"../data/Ross.csv\")\n",
      "joey=pd.read_csv(\"../data/Joey.csv\")\n",
      "chandler=pd.read_csv(\"../data/Chandler.csv\")\n",
      "alls=pd.read_csv(\"../data/All.csv\")\n",
      "guests=pd.read_csv(\"../data/guest_stars.csv\")\n",
      "scenes=pd.read_csv(\"../data/scenes.csv\")\n",
      "minors=pd.read_csv(\"../data/minors.csv\")\n",
      "songs_ph=pd.read_csv(\"../data/songs_phoebe.csv\")\n",
      "227/15:\n",
      "# monica=pd.read_csv(\"../data/Monica.csv\")\n",
      "#monica\n",
      "songs_ph\n",
      "# monica[\"line\"][:2]\n",
      "227/16:\n",
      "import os\n",
      "os.getcwd()  \n",
      "\n",
      "# songs_ph[\"line\"][0]\n",
      "227/17:\n",
      "\n",
      "import nltk\n",
      "nltk.download('punkt') # one time \n",
      "import re\n",
      "227/18:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in songs_ph[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "227/19:\n",
      "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
      "#!unzip glove*.zip\n",
      "#Uncomment if not downloaded already ? I will try to do so in Google Collab\n",
      "227/20:\n",
      "#Cleaning Sentences\n",
      "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "227/21:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "227/22: monica[\"line\"][0]\n",
      "227/23:\n",
      "monica[\"line\"][:2]\n",
      "import nltk\n",
      "nltk.download('punkt') # one time execution\n",
      "import re\n",
      "227/24:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "226/34: np.argsort(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/35: np.argsort((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/36: np.argsort((((scores[i],s) for i,s in enumerate(sentences[0:t_len])))\n",
      "226/37: np.argsort(((scores[i],s) for i,s in enumerate(sentences[0:t_len])))\n",
      "226/38: (((scores[i],s) for i,s in enumerate(sentences[0:t_len]))\n",
      "226/39: ((scores[i],s) for i,s in enumerate(sentences[0:t_len]))\n",
      "226/40:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/41: ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/42: ((scores[i],s) for i,s in enumerate(sentences[0:t_len]))\n",
      "226/43: sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/44: scores[0]\n",
      "226/45: scores[1]\n",
      "226/46:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][0])\n",
      "226/47: #Let's go brute force for the moment\n",
      "226/48:\n",
      "#Let's go brute force for the moment\n",
      "ranked_sentences[2]\n",
      "226/49:\n",
      "#Let's go brute force for the moment\n",
      "ranked_sentences[2][2]\n",
      "226/50: #Let's go brute force for the moment\n",
      "226/51: for el in ranked_sentences[:][1]\n",
      "226/52:\n",
      "for el in ranked_sentences[:][1]:\n",
      "    print el\n",
      "226/53:\n",
      "for el in ranked_sentences[:][1]:\n",
      "    print(el)\n",
      "226/54:\n",
      "for el in ranked_sentences[][1]:\n",
      "    print(el)\n",
      "226/55:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "226/56:\n",
      "for el in ranked_sentences[0:len(ranked_sentences)][1]:\n",
      "    print(el)\n",
      "226/57:\n",
      "for el in range[0:len(ranked_sentences)]:\n",
      "    print(el)\n",
      "226/58:\n",
      "for el in range(0:len(ranked_sentences)):\n",
      "    print(el)\n",
      "226/59:\n",
      "for el in range(0,len(ranked_sentences)):\n",
      "    print(el)\n",
      "226/60:\n",
      "for el in range(0,len(ranked_sentences)):\n",
      "    print(ranked_sentences[el][0])\n",
      "226/61:\n",
      "for el in range(0,len(ranked_sentences)):\n",
      "    print(ranked_sentences[el][1])\n",
      "226/62:\n",
      "#Let's go brute force for the moment\n",
      "scores\n",
      "226/63:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores)\n",
      "226/64:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[1])\n",
      "226/65:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[:][1])\n",
      "226/66:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i] for i in range(0,len(senteces)))\n",
      "226/67:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i] for i in range(0,len(sentences)))\n",
      "226/68:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i] for i in enumerate(sentences[0:t_len]))\n",
      "226/69:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i] for i in enumerate(sentences[0:t_len]))\n",
      "226/70:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i] for i,s in enumerate(sentences[0:t_len]))\n",
      "226/71:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(s for i,s in enumerate(sentences[0:t_len]))\n",
      "226/72:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores[i],s for i,s in enumerate(sentences[0:t_len]))\n",
      "226/73:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(list(scores))\n",
      "226/74: scores[3]\n",
      "226/75: scores[2]\n",
      "226/76: np.argsort(((scores[i],s) for i,s in enumerate(sentences[0:t_len])))\n",
      "226/77:\n",
      "scores[2]\n",
      "len(scores)\n",
      "226/78:\n",
      "scores[2]\n",
      "print(scores)\n",
      "226/79:\n",
      "scores[2]\n",
      "print(scores[2])\n",
      "226/80:\n",
      "scores[2]\n",
      "print(scores[5])\n",
      "226/81:\n",
      "scores_list=[]\n",
      "for i in range(len(scores)):\n",
      "    score_list.append(scores[i])\n",
      "# print(scores[5])\n",
      "226/82:\n",
      "scores_list=[]\n",
      "for i in range(len(scores)):\n",
      "    score_list.append(scores[i])\n",
      "# print(scores[5])\n",
      "226/83:\n",
      "scores_list=[]\n",
      "for i in range(len(scores)):\n",
      "    scores_list.append(scores[i])\n",
      "# print(scores[5])\n",
      "226/84:\n",
      "#Let's go brute force for the moment\n",
      "np.argsort(scores_list)\n",
      "226/85:\n",
      "for el in range(len(scores)):\n",
      "    print(sort_list[el])\n",
      "226/86: ((scores[i],s) for i,s in enumerate(sentences[0:t_len]))\n",
      "226/87:\n",
      "#Let's go brute force for the moment\n",
      "sort_list=np.argsort(scores_list)\n",
      "226/88:\n",
      "for el in range(len(scores)):\n",
      "    print(sort_list[el])\n",
      "226/89:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_tables))\n",
      "226/90:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "226/91: sum_table\n",
      "226/92:\n",
      "sum_table\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(s)\n",
      "226/93:\n",
      "sum_table\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(s)\n",
      "print(senteces)\n",
      "226/94:\n",
      "sum_table\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(s)\n",
      "print(sentences)\n",
      "226/95:\n",
      "sum_table\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(s)\n",
      "print(len(sentences))\n",
      "226/96: clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "226/97: clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "226/98:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "226/99:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "226/100:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "226/101:\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "226/102:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "226/103: from sklearn.metrics.pairwise import cosine_similarity\n",
      "226/104:\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "226/105:\n",
      "t_len=490\n",
      "\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/106:\n",
      "t_len=len(sentence_vectors)\n",
      "\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/107:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/108: ranked_sentences = sorted(( (scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/109:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "226/110: # len(ranked_sentences)\n",
      "226/111: # np.argsort(((scores[i],s) for i,s in enumerate(sentences[0:t_len])))\n",
      "226/112: ((scores[i],s) for i,s in enumerate(sentences[0:t_len]))\n",
      "226/113: sorted(((scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/114: scores[0]\n",
      "226/115:\n",
      "scores_list=[]\n",
      "for i in range(len(scores)):\n",
      "    scores_list.append(scores[i])\n",
      "# print(scores[5])\n",
      "226/116:\n",
      "#Let's go brute force for the moment\n",
      "sort_list=np.argsort(scores_list)\n",
      "226/117:\n",
      "for el in range(len(scores)):\n",
      "#     print(sort_list[el])\n",
      "226/118:\n",
      "for el in range(len(scores)):\n",
      "     print(sort_list[el])\n",
      "226/119:\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "print(len(allofthem))\n",
      "226/120:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "print(episode_info[\"short_summary\"][0])\n",
      "226/121:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "print(episode_info[\"rat_grouped\"])\n",
      "226/122:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "print(episode_info['Rating'])\n",
      "print(episode_info[\"rat_grouped\"])\n",
      "226/123:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "# print(episode_info['Rating'])\n",
      "# print(episode_info[\"rat_grouped\"])\n",
      "226/124:\n",
      "for el in range(len(scores)):\n",
      "    print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "#      print(sort_list[el])\n",
      "226/125:\n",
      "for el in range(len(scores)):\n",
      "    print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el])\n",
      "\n",
      "    #      print(sort_list[el])\n",
      "226/126:\n",
      "rat_group_predicted=[]:\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    else if sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "        \n",
      "for el in range(len(scores)):\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_grouped_predicted[el])\n",
      "226/127:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    else if sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "        \n",
      "for el in range(len(scores)):\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_grouped_predicted[el])\n",
      "226/128:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    if 77<sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "        \n",
      "for el in range(len(scores)):\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_grouped_predicted[el])\n",
      "226/129:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    if 77<sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "        \n",
      "for el in range(len(scores)):\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_group_predicted[el])\n",
      "226/130:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    if 77<sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "ctr=[]  \n",
      "for el in range(len(scores)):\n",
      "    if episode_info[\"rat_grouped\"][el]==rat_group_predicted[el]:\n",
      "        ctr+=1\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_group_predicted[el]) \n",
      "\n",
      "print(ctr)\n",
      "226/131:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<77:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    if 77<sort_list[el]<144:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "ctr=0\n",
      "for el in range(len(scores)):\n",
      "    if episode_info[\"rat_grouped\"][el]==rat_group_predicted[el]:\n",
      "        ctr+=1\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_group_predicted[el]) \n",
      "\n",
      "print(ctr)\n",
      "226/132: 79/227\n",
      "226/133:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<85:\n",
      "        rat_group_predicted.append('above')\n",
      "        \n",
      "    if 85<sort_list[el]<169:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('below')\n",
      "\n",
      "ctr=0\n",
      "for el in range(len(scores)):\n",
      "    if episode_info[\"rat_grouped\"][el]==rat_group_predicted[el]:\n",
      "        ctr+=1\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_group_predicted[el]) \n",
      "\n",
      "print(ctr)\n",
      "226/134:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Perceptron\n",
      "226/135:\n",
      "episode_info=pd.read_csv('../data/New_Characters/with_rows_removed_addedrows.csv')\n",
      "\n",
      "#random state for the entire file as well as the train test split percentage\n",
      "random=856\n",
      "size_t=.1\n",
      "226/136:\n",
      "allofthem=[col for col in episode_info]\n",
      "print(allofthem)\n",
      "\n",
      "# print(len(allofthem))\n",
      "226/137:\n",
      "sum_table=episode_info[\"short_summary\"]\n",
      "print(len(sum_table))\n",
      "# print(episode_info['Rating'])\n",
      "# print(episode_info[\"rat_grouped\"])\n",
      "226/138:\n",
      "sum_table\n",
      "sentences = []\n",
      "for s in sum_table:\n",
      "    sentences.append(s)\n",
      "print(len(sentences))\n",
      "226/139:\n",
      "\n",
      "# import nltk\n",
      "# nltk.download('punkt') # one time \n",
      "# import re\n",
      "226/140:\n",
      "# from nltk.tokenize import sent_tokenize\n",
      "# sentences = []\n",
      "# for s in sum_table:\n",
      "#     sentences.append(sent_tokenize(s))\n",
      "\n",
      "# sentences = [y for x in sentences for y in x] # flatten list\n",
      "# print(sentences[0:5])\n",
      "226/141: clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "226/142:\n",
      "# Extract word vectors\n",
      "word_embeddings = {}\n",
      "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
      "for line in f:\n",
      "    values = line.split()\n",
      "    word = values[0]\n",
      "    coefs = np.asarray(values[1:], dtype='float32')\n",
      "    word_embeddings[word] = coefs\n",
      "f.close()\n",
      "226/143:\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('english')\n",
      "print(stop_words)\n",
      "226/144:\n",
      "def remove_stopwords(sen):\n",
      "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
      "    return sen_new\n",
      "226/145:\n",
      "\n",
      "clean_sentences = [s.lower() for s in clean_sentences]\n",
      "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
      "226/146:\n",
      "sentence_vectors = []\n",
      "for i in clean_sentences:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "226/147: from sklearn.metrics.pairwise import cosine_similarity\n",
      "226/148:\n",
      "print(len(clean_sentences))\n",
      "len(sentence_vectors)\n",
      "226/149:\n",
      "t_len=len(sentence_vectors)\n",
      "\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/150:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/151: ranked_sentences = sorted(( (scores[i],s) for i,s in enumerate(sentences[0:t_len])), reverse=True)\n",
      "226/152:\n",
      "for i in range(10):\n",
      "  print(ranked_sentences[i][1])\n",
      "226/153: # len(ranked_sentences)\n",
      "226/154:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Rachel dates Ross\"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "226/155:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Rachel dates Ross\"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "clean_sentence_query = [s.lower() for s in clean_sentence_query]\n",
      "clean_sentence_query = [remove_stopwords(r.split()) for r in clean_sentence_query]\n",
      "226/156:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "if len(i) != 0:\n",
      "    #print('I am here')\n",
      "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "else:\n",
      "    v = np.zeros((100,))\n",
      "sentence_vectors.append(v)\n",
      "226/157:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "sentence_vectors.append(v)\n",
      "226/158:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vectors.append(v)\n",
      "226/159:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vector_query.append(v)\n",
      "227/25:\n",
      "#cosine_similarity(sentence_vectors[100].reshape(1,100),sentence_vectors[100].reshape(1,100))[0,0]\n",
      "# for i in range(len(sentences)):\n",
      "#     for j in range(len(sentences)):\n",
      "\n",
      "t_len=230\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "227/26:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/160:\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/161:\n",
      "t_len=len(sentences)\n",
      "sim_mat = np.zeros([t_len, t_len])\n",
      "\n",
      "for i in range(t_len):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/162:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vector_query.reshape(1,100),\n",
      "                                              sentence_vector_query.reshape(1,100))[0,0]\n",
      "226/163:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vector_query.reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/164:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vector_query.reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/165:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vector_query.reshape(1,100),\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/166:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/167:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/168:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat1 = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat1[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/169:\n",
      "import networkx as nx\n",
      "\n",
      "nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "scores = nx.pagerank(nx_graph)\n",
      "226/170:\n",
      "# import networkx as nx\n",
      "\n",
      "# nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "# scores = nx.pagerank(nx_graph)\n",
      "226/171: sim_mat1\n",
      "226/172: sort_list=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[0])))\n",
      "226/173: sim_mat1.shape\n",
      "226/174: sort_list=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/175: sort_l=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/176: sort_l\n",
      "226/177:\n",
      "#Let's go brute force for the moment\n",
      "sort_list=np.argsort(scores_list, reverse==True)\n",
      "226/178:\n",
      "#Let's go brute force for the moment\n",
      "sort_list=np.argsort(scores_list)\n",
      "226/179:\n",
      "rat_group_predicted=[]\n",
      "\n",
      "for el in range(len(scores)):\n",
      "    if sort_list[el]<58:\n",
      "        rat_group_predicted.append('below')\n",
      "        \n",
      "    if 85<sort_list[el]<142:\n",
      "        rat_group_predicted.append('av')\n",
      "    \n",
      "    else: \n",
      "        rat_group_predicted.append('above')\n",
      "\n",
      "ctr=0\n",
      "for el in range(len(scores)):\n",
      "    if episode_info[\"rat_grouped\"][el]==rat_group_predicted[el]:\n",
      "        ctr+=1\n",
      "    #print(episode_info[\"rat_grouped\"][sort_list[el]])\n",
      "    print(episode_info[\"rat_grouped\"][el], rat_group_predicted[el]) \n",
      "\n",
      "print(ctr)\n",
      "226/180: # sort_l\n",
      "226/181: # sim_mat1\n",
      "226/182: sim_list=list(sim_mat1.reshape(sim_mat1.shape[1]))\n",
      "226/183: max_pos=sim_list.index(max(sim_list))\n",
      "226/184: print(max_pos)\n",
      "226/185: sum_table=episode_info[\"short_summary\"]\n",
      "226/186: sum_table[max_pos]\n",
      "226/187: sum_table[sort_l[-1]]\n",
      "226/188: sum_table[sort_l[:-2]]\n",
      "226/189: sum_table[sort_l[-2]]\n",
      "226/190:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Ross dates \"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "clean_sentence_query = [s.lower() for s in clean_sentence_query]\n",
      "clean_sentence_query = [remove_stopwords(r.split()) for r in clean_sentence_query]\n",
      "226/191:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vector_query.append(v)\n",
      "226/192:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat1 = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat1[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/193:\n",
      "# import networkx as nx\n",
      "\n",
      "# nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "# scores = nx.pagerank(nx_graph)\n",
      "226/194: # sim_mat1\n",
      "226/195: sort_l=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/196: sim_mat1.shape\n",
      "226/197: # sort_l\n",
      "226/198: sim_list=list(sim_mat1.reshape(sim_mat1.shape[1]))\n",
      "226/199: max_pos=sim_list.index(max(sim_list))\n",
      "226/200: print(max_pos)\n",
      "226/201: sum_table=episode_info[\"short_summary\"]\n",
      "226/202: sum_table[max_pos]\n",
      "226/203: sum_table[sort_l[-1]]\n",
      "226/204: sum_table[sort_l[-2]]\n",
      "226/205: sum_table[sort_l[-3]]\n",
      "226/206:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Monica dates \"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "clean_sentence_query = [s.lower() for s in clean_sentence_query]\n",
      "clean_sentence_query = [remove_stopwords(r.split()) for r in clean_sentence_query]\n",
      "226/207:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vector_query.append(v)\n",
      "226/208:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat1 = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat1[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/209:\n",
      "# import networkx as nx\n",
      "\n",
      "# nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "# scores = nx.pagerank(nx_graph)\n",
      "226/210: # sim_mat1\n",
      "226/211: sort_l=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/212: sim_mat1.shape\n",
      "226/213: # sort_l\n",
      "226/214: sim_list=list(sim_mat1.reshape(sim_mat1.shape[1]))\n",
      "226/215: max_pos=sim_list.index(max(sim_list))\n",
      "226/216: print(max_pos)\n",
      "226/217: sum_table=episode_info[\"short_summary\"]\n",
      "226/218: sum_table[max_pos]\n",
      "226/219: sum_table[sort_l[-1]]\n",
      "226/220: sum_table[sort_l[-2]]\n",
      "226/221: sum_table[sort_l[-3]]\n",
      "226/222: sum_table[sort_l[-3]]\n",
      "226/223:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Monica cleans \"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "clean_sentence_query = [s.lower() for s in clean_sentence_query]\n",
      "clean_sentence_query = [remove_stopwords(r.split()) for r in clean_sentence_query]\n",
      "226/224:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vector_query.append(v)\n",
      "226/225:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat1 = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat1[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/226:\n",
      "# import networkx as nx\n",
      "\n",
      "# nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "# scores = nx.pagerank(nx_graph)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/227: # sim_mat1\n",
      "226/228: sort_l=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/229: sim_mat1.shape\n",
      "226/230: # sort_l\n",
      "226/231: sim_list=list(sim_mat1.reshape(sim_mat1.shape[1]))\n",
      "226/232: max_pos=sim_list.index(max(sim_list))\n",
      "226/233: print(max_pos)\n",
      "226/234: sum_table=episode_info[\"short_summary\"]\n",
      "226/235: sum_table[max_pos]\n",
      "226/236: sum_table[sort_l[-1]]\n",
      "226/237: sum_table[sort_l[-2]]\n",
      "226/238: sum_table[sort_l[-3]]\n",
      "227/27:\n",
      "from nltk.tokenize import sent_tokenize\n",
      "sentences = []\n",
      "for s in monica[\"line\"]:\n",
      "    sentences.append(sent_tokenize(s))\n",
      "\n",
      "sentences = [y for x in sentences for y in x] # flatten list\n",
      "print(sentences[0:5])\n",
      "229/1: pwd\n",
      "226/239:\n",
      "#\"Input your sentece\"\n",
      "sentence_query=[\"Phoebe sings \"]\n",
      "clean_sentence_query = pd.Series(sentence_query).str.replace(\"[^a-zA-Z]\", \" \")\n",
      "\n",
      "clean_sentence_query = [s.lower() for s in clean_sentence_query]\n",
      "clean_sentence_query = [remove_stopwords(r.split()) for r in clean_sentence_query]\n",
      "226/240:\n",
      "sentence_vector_query = []\n",
      "\n",
      "for i in clean_sentence_query:\n",
      "    if len(i) != 0:\n",
      "        #print('I am here')\n",
      "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
      "    else:\n",
      "        v = np.zeros((100,))\n",
      "    sentence_vector_query.append(v)\n",
      "226/241:\n",
      "t_len=len(sentences)\n",
      "\n",
      "sim_mat1 = np.zeros([1, t_len])\n",
      "\n",
      "for i in range(1):\n",
      "    for j in range(t_len):\n",
      "        if i != j:\n",
      "            sim_mat1[i][j] = cosine_similarity(sentence_vector_query,\n",
      "                                              sentence_vectors[j].reshape(1,100))[0,0]\n",
      "226/242:\n",
      "# import networkx as nx\n",
      "\n",
      "# nx_graph = nx.from_numpy_array(sim_mat1)\n",
      "# scores = nx.pagerank(nx_graph)\n",
      "226/243: # sim_mat1\n",
      "226/244: sort_l=np.argsort(list(sim_mat1.reshape(sim_mat1.shape[1])))\n",
      "226/245: sim_mat1.shape\n",
      "226/246: # sort_l\n",
      "226/247: sim_list=list(sim_mat1.reshape(sim_mat1.shape[1]))\n",
      "226/248: max_pos=sim_list.index(max(sim_list))\n",
      "226/249: print(max_pos)\n",
      "226/250: sum_table=episode_info[\"short_summary\"]\n",
      "226/251: sum_table[max_pos]\n",
      "226/252: sum_table[sort_l[-1]]\n",
      "226/253: sum_table[sort_l[-2]]\n",
      "226/254: sum_table[sort_l[-3]]\n",
      "226/255: ## Recommends a different one\n",
      "226/256: sum_table[sort_l[:3]]\n",
      "226/257: sum_table[sort_l[-4]]\n",
      "226/258:\n",
      "# After answering questions like: favorite character, \n",
      "# favorite side character, favorite quote, etc, gives a reccomendation on which episode \n",
      "# to watch using the ratings and the factors\n",
      "226/259:\n",
      "## A netflix add on that recommends based on a few things \n",
      "## when someone wants to rewatch, \n",
      "\n",
      "## Synopsis are anyways available\n",
      "226/260:\n",
      "#Instead of show recommendation , work on episode recommendation\n",
      "# based on questionare such as .......\n",
      "# ..\n",
      "231/1:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "   1:\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "#Initialize t-SNE\n",
      "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
      "\n",
      "# Use only 400 rows to shorten processing time\n",
      "tsne_df = tsne.fit_transform(df[:400])\n",
      "   2:\n",
      "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
      "vector_list = [model[word] for word in words if word in model.vocab]\n",
      "\n",
      "# Create a list of the words corresponding to these vectors\n",
      "words_filtered = [word for word in words if word in model.vocab]\n",
      "\n",
      "# Zip the words together with their vector representations\n",
      "word_vec_zip = zip(words_filtered, vector_list)\n",
      "\n",
      "# Cast to a dict so we can turn it into a DataFrame\n",
      "word_vec_dict = dict(word_vec_zip)\n",
      "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
      "df.head(3)\n",
      "   3: sns.set()\n",
      "   4:\n",
      "import pandas as pd\n",
      "import gensim\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import xgboost as xgb\n",
      "   5: .ipynb_checkpoints/\n",
      "   6: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g\n",
    "#awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenize the string into words\n",
    "words_filtered=[]\n",
    "vector_list=[]\n",
    "for i in clean_sentences:\n",
    "    for w in i.split():\n",
    "        words_filtered.append(w)\n",
    "        \n",
    "        vector_list.append(word_embeddings.get(w, np.zeros((100,)))  )\n",
    "        #print(w)\n",
    "                \n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 40)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# Import adjustText, initialize list of texts\n",
    "from adjustText import adjust_text\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, 13, 1))\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
